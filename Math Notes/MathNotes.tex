% Created Nov 10, 2020
\documentclass[12pt, english]{book}
\usepackage[letterpaper, portrait, margin=2.5cm]{geometry} %Set paper type, format, margin
\usepackage[svgnames]{xcolor}
\usepackage{%abstract, 
	amsmath, amssymb, array, babel, booktabs, caption, censor, fancyhdr, flafter, float, framed, gensymb, hyperref, inputenc, indentfirst, mathtools, MnSymbol, multicol, multirow, longtable, lscape, ltablex, polynom, pgfplots, physics, rotating, scalerel, setspace, stackengine, tabularx, threeparttable, titlesec, titling, wrapfig}
\usepackage{cleveref}
\usetikzlibrary{calc,angles,positioning,intersections,quotes,decorations.markings,backgrounds,patterns, arrows.meta}
\usepgfplotslibrary{groupplots}
\usepackage{amsthm} %Do not use amsthm with ntheorem unlees ntheorem is using the amsthm option
\usepackage{thmtools}
\pgfplotsset{compat=1.8}
\usepgfplotslibrary{external}
\tikzexternalize[prefix=tikz/] 

\setlength{\parindent}{0em} %Set indent on paragraph
\setlength{\parskip}{0.5em} %Set spaces between paragraphs

\newcommand\showdiv[1]{\overline{\smash{\hstretch{.5}{)}\mkern-3.2mu\hstretch{.5}{)}}#1}}
\newcommand\ph[1]{\textcolor{white}{#1}}
\setstackgap{S}{1.5pt}

% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the 
% starred version does not.


\declaretheoremstyle[
	spaceabove=6pt, 		
	spacebelow=6pt,
	%	headfont=\normalfont\bfseries,
	notefont=\bfseries, notebraces={}{},
	bodyfont=\slshape,
	postheadspace=1em,
	headpunct=,
	headformat={\NAME~\NUMBER:\NOTE \hfill \smallskip\linebreak},%
	]{axiomstyle}

\declaretheoremstyle[
	spaceabove=6pt, 		
	spacebelow=6pt,
	headfont=\bfseries, %\color{blue},
	notefont=\bfseries, notebraces={}{},
	bodyfont=\slshape \color{DarkBlue},
	postheadspace=1em,
	headpunct=,
	headformat={\NAME~\NUMBER:\NOTE \hfill \smallskip\linebreak},%
	]{theoremstyle}
	
\declaretheoremstyle[
	spaceabove=6pt, 		
	spacebelow=6pt,
	headfont=\bfseries, %\color{Blue}%
	notefont=\bfseries,%
	notebraces={}{},%
	headpunct=,%
	bodyfont=\itshape\color{DarkGreen},%
	headformat={\NAME~\NUMBER:\NOTE \hfill \smallskip\linebreak},%
	%	preheadhook=\begin{leftbar},%
	%	postfoothook=\end{leftbar},%
	]{minorstyle}
	
\declaretheoremstyle[
	spaceabove=6pt, 		
	spacebelow=6pt,
	headfont=\bfseries,%\color{Blue},%
	notefont=\bfseries,%
	notebraces={}{},%
	headpunct=,%
	bodyfont=\slshape \color{OrangeRed},%
	headformat={\NAME~\NUMBER:\NOTE \hfill \smallskip\linebreak},%
%	preheadhook=\begin{leftbar},%
%	postfoothook=\end{leftbar},%
	]{defstyle}
	
\declaretheoremstyle[
	spaceabove=6pt, 		
	spacebelow=6pt,
	%	headfont=\normalfont\bfseries,
	bodyfont=\slshape,
%	postheadspace=1em,
	headpunct=,
%	headformat={\NAME~\NUMBER:\NOTE \hfill \smallskip\linebreak},%
	]{examplestyle}

\declaretheorem{axiom}[
	numberwithin=chapter, style=axiomstyle, parent=chapter,
	]
	
\declaretheorem{theorem, conjecture}[
	numberwithin=section, style=theoremstyle, parent=section,
	]
	
\declaretheorem{lemma, proposition, corollary}[
	style=minorstyle,
	parent=theorem,
%	numberwithin=section
	]

\declaretheorem{definition}[parent=section, style=defstyle]

\declaretheorem{remark, observation, question}[numbered=no]

\declaretheorem{example}[numberwithin=section, style=examplestyle]


% Indenting the Proof enviroment
\makeatletter
\renewenvironment{proof}[1][\proofname]{\par
	\pushQED{\qed}%
	\normalfont \topsep6\p@\@plus6\p@\relax
	\list{}{%
		\settowidth{\leftmargin}{\itshape\proofname:\hskip\labelsep}%
		\setlength{\labelwidth}{0pt}%
		\setlength{\itemindent}{-\leftmargin}%
	}%
	\item[\hskip\labelsep\itshape#1\@addpunct{:}]\ignorespaces
}{%
	\popQED\endlist\@endpefalse
}
\makeatother

% Letters after part
\makeatletter
\renewcommand{\@endpart}{\vfil\newpage}
\makeatother
\newenvironment{partintro}
{\vspace*{\fill}
	\section*{\centering Resources used in part \thepart}
	\begin{quotation}}
	{\end{quotation}\vspace*{\fill}\newpage}
\newcommand{\nopartintro}{%
	\vspace*{\fill}
	\thispagestyle{empty}
	\newpage
}

\begin{document}
	\title{The Book of Math (Notes)}
	\author{Kevin Kuo}
	\date{November 10, 2020}
	
	\pagestyle{fancy}
	\fancyhead{} % clear all header fields
	\fancyhead[LO]{  }
	\fancyhead[CO]{  }
	\fancyhead[RO]{  }
	\renewcommand{\headrulewidth}{0pt}
	
	\fancyfoot{} % clear all footer fields
	\fancyfoot[LO]{}
	\fancyfoot[CO]{\thepage}
	\fancyfoot[RO]{}
	\renewcommand{\footrulewidth}{0pt}
	\vspace{0cm}	
	
	\frontmatter
	
	\maketitle
	
	\newpage
	\section*{Forward and Disclaimer}
	These are math notes made by a student (with a physics major and math minor) based off text books. It may contain misconceptions and misinterpretations, thus should not be viewed in the same light of a text book. Use at your own risk and mental sanity.
	
	\section*{Symbols}
	\begin{tabularx}{\textwidth}{ l c p{0.6\linewidth}}
		\multicolumn{3}{l}{\textbf{{\large Logic}}} \\ [10pt]
		\hline
		\textbf{Name} & \textbf{Symbol} & \textbf{Comment} \\
		\hline
		Exists 					& $\exists$ 		& There exists at least one\\
		For all 				& $\forall$ 		& \\
		Not exists 				& $\nexists$ 		& There does not exist\\ 
		Exists one				& $\exists!$ 		& There only exists one and only one \\
		And 					& $\land$			& \\
		Or						& $\lor$			& Inclusive or \\
		Not 					& $\neg$			& \\
		Logically implies 		& $\implies$ 		& If \\
		Logically implied by 	& $\Longleftarrow$ 	& Only if \\  
		Logically equivalent 	& $\iff$ 			& If and only if \\
		Implies 				& $\longrightarrow$	& \\
		Implied by 				& $\longleftarrow$ 	& \\  
		Double Implication 		& $\longleftrightarrow$	& \\
		\hline	
		
		& & \\
		\multicolumn{3}{l}{\textbf{{\large Set Notation}}} \\ [10pt]
		\hline
		\textbf{Name} & \textbf{Symbol} & \textbf{Comment} \\
		\hline
 		Empty Set 				& $\emptyset$ 		& The set that is empty \\
 		Natural Numbers 		& $\mathbb{N}$		& Set of natural numbers not containing 0, equivalent to the set of positive integers \\
 		Integers 				& $\mathbb{Z}$		& Set of integers \\
 		Rational Numbers 		& $\mathbb{Q}$		& \\
 		Algebraic Numbers		& $\mathbb{A}$		& \\
 		Real Numbers 			& $\mathbb{R}$		& \\
 		Complex Numbers 		& $\mathbb{C}$		& \\
 		
 		In 						& $\in$ 			& \\
 		Not in 					& $\nin$			& \\
 		Owns 					& $\ni$				& Has an element \\
 		
 		Proper Subset 			& $\subset$			& Subset that is not itself \\
 		Subset 					& $\subseteq$		& \\
 		Superset 				& $\supset$ 		& Superset that is not itself\\
 		Proper Superset 		& $\supseteq$		& \\
 		Power set				& $\wp$				& \\
 		Union 					& $\cup$			& \\
 		Intersection			& $\cap$			& \\
 		Difference				& $\setminus$		& \\
 		\hline
 		
 		& & \\
 		\multicolumn{3}{l}{\textbf{{\large Relationships}}} \\ [10pt]
 		\hline
 		\textbf{Name} & \textbf{Symbol} & \textbf{Comment} \\
 		\hline
 		Defined 				& $\doteq$ 			& \\
 		Approximate 			& $\approx$			& \\
 		Equivalent				& $\equiv$	 		& Isomorphic (Group Theory) \\
 		Congruent 				& $\cong$			& Homomorphic (Group Theory) \\
 		Proportional 			& $\propto$			& \\
 		\hline
 		
 		& & \\
 		\multicolumn{3}{l}{\textbf{{\large Operators}}} \\ [10pt]
 		\hline
 		\textbf{Name} & \textbf{Symbol} & \textbf{Comment} \\
 		\hline
 		& $\oplus$ & \\
 		& $\otimes$ & \\
 		& $\odot$ & \\
 		& $\circ$ & Convolution \\
 		Dagger& $\dagger$ & Complex conjugate transpose of a matrix \\
 		\hline
 		
 		& & \\
 		\multicolumn{3}{l}{\textbf{{\large Arrows}}} \\ [10pt]
 		\hline
 		\textbf{Name} & \textbf{Symbol} & \textbf{Comment} \\
 		\hline
 		Maps to 				& $\mapsto$			& \\
 		\hline
 		
 		& & \\
 		\multicolumn{3}{l}{\textbf{{\large Hebrew}}} \\ [10pt]
 		\hline
 		\textbf{Name} & \textbf{Symbol} & \textbf{Comment} \\
 		\hline
 		Aleph					& $\aleph$			& Carnality of infinite sets that can be well ordered \\
 		\hline
 		
 		& & \\
 		\multicolumn{3}{l}{\textbf{{\large Other}}} \\ [10pt]
 		\hline
 		\textbf{Name} & \textbf{Symbol} & \textbf{Comment} \\
 		\hline
 		Real part 				& $\Re$				& Real part of a number \\
 		Imaginary part 			& $\Im$				& Imaginary part of a number \\
 		\hline
	\end{tabularx}

	\newpage
	\section*{Book Constitution}
	\subsection*{Intents and Purpose}
	The goal of this book is to organize mathematical knowledge of topics related to the study of physics or the author's interest. It is meant to be used as a source of for future reference, not as a textbook for students new to the topics. It is a notebook of a student, thus should be treated as one and not as a textbook. At most, it could be used as a study guide along side a textbook. Definitely not as the main source for acquiring knowledge. 
	
	\subsection*{Layout and Organization}
	The book is split into parts each containing a field of study mathematics, or a topic large enough to justify giving it its own part. Each part contains chapters that focuses on a particular topic required to understand the field, with sections dedicated to describing a particular knowledge required for the topic. 
	
	As axioms, definitions, theorems, corollary, and proofs are integral and abundant to the study of mathematics, each will have a unique style. Each environment and its styles are displayed as follows: 
	
	\begin{axiom}[Axiom name]{Example Axiom}
		Axioms are the ``ground rules" of the set.
	\end{axiom}

	\begin{theorem}[Theorem name or citation]{Example Theorem}
		An important logical result from the axioms, with proof.
	\end{theorem}

	\begin{conjecture}[Name of conjecture or citation]{Example Conjecture}
		A hypothesis, without proof.
	\end{conjecture}
	
	\begin{corollary}{Example Corollary}
		An implication as a result of a theorem.
	\end{corollary}

	\begin{lemma}{Example Lemma}
		Small theorems that build up to a larger theorem. 
	\end{lemma}
	
	\begin{proposition}{Example Proposition}
		Example proposition.
	\end{proposition}
	
	\begin{proof}
		Logical deductions that results in a theorem.
		\textcolor{Grey}{Proofs I've written will be in grey, which may or may not be correct.}
	\end{proof}

%	\theoremstyle{break}
	\begin{definition}[Word]{Example Definition}
		The definition of a word.
	\end{definition}	

	\begin{example}
		An example.
	\end{example}

	\begin{remark}{Remark}
		A comment by the author in the textbooks used.
	\end{remark}
	
	\begin{observation}{Example Observation}
		A remark by me.
	\end{observation}

	\begin{question}{Example Question}
		A question from me for a mystery to be answered later. 
	\end{question}

	
	\section*{}
	\tableofcontents
	
	\mainmatter
	\part{Logic} \label{Logic Part}
	
	\chapter{Proofs}
	
	
	\part{Numbers} \label{Numbers Part}
	\begin{partintro}
		content...
	\end{partintro}
	
	\chapter{Natural $\mathbb{N}$} \label{Natural Chapter - Numbers}
	
	\chapter{Integers $\mathbb{Z}$} \label{Integers Chapter - Numbers}
	
	\chapter{Rationals $\mathbb{Q}$} \label{Rationals Chapter - Numebers}
	
	\chapter{Constructible} \label{Constructible Chapter - Numbers}
	
	\chapter{Algebraic $\mathbb{A}$} \label{Algebraic Chapter - Numbers}
	
	\chapter{Reals $\mathbb{R}$} \label{Reals Chapter - Numbers}
	
	\chapter{Complex $\mathbb{C}$} \label{Complex Chapter - Numbers}
	

	
	\part{Real Analysis} \label{Real Analysis Part}
	\begin{partintro}
		\begin{itemize}
			\item[1.] Kenneth A. Ross - Elementary Analysis (2nd Ed.) \cite{Ross.K-Elementary-Analysis-2013}
		\end{itemize}
	\end{partintro}

	\chapter{Sequences} \label{Sequences Chapter - Real Analysis}
	
	\section{Limits} \label{Limits Section - Real Analysis}
	
	\subsection{Limit Theorems} \label{Limit Theorems Subsection - Real Analysis}
	
	\section{Monotone and Cauchy Sequences} \label{Monotone and Cauchy Sequences Section - Real Analysis}
	
	\section{Subsequences} \label{Subsequences Section - Real Analysis}
	
	\section{$\lim \operatorname{sup}$ and $\lim \operatorname{inf}$} \label{lim sup and lim inf Section - Real Analysis}
	
	\section{Series} \label{Series Section - Real Analysis}
	
	\section{Alternating Series and Integral Tests} \label{Alternating Series and Integral Tests Section - Real Analysis}
	
	\chapter{Continuity} \label{Continuity Chapter - Real Analysis}
	
	\section{Continuous Functions} \label{Continuous Functions Section - Real Analysis}
	
	\subsection{Properties} \label{Properties of Continuous Functions Subsection - Real Analysis}
	
	\section{Uniform Continuity} \label{Uniform Continuity Section - Real Analysis}
	
	\section{Limits of Functions} \label{Limits of Functions Section - Real Analysis}

	\chapter{Metric Spaces}	
	
	\part{Complex Analysis} \label{Complex Analysis Part}
	\begin{partintro}
	\noindent Primary:
		\begin{itemize}
			\item[1.] Brown and Churchill - Complex Variables and Applications \cite{Brown.J;Churchill.R-Complex-Variables-2014}
		\end{itemize}
		Supplement: 
		\begin{itemize}
			\item[1.] A. David Wunsch - Complex Variables with Applications \cite{Wunsh.A-Complex-Variables-2005}
		\end{itemize}
	\end{partintro}
	
	\chapter{Basics} \label{Basics Chapter - Complex}
	\section{Complex Numbers} \label{Complex Numbers Section - Complex}
	$$\mathbb{C} = \{x + iy \mid x, y \in \mathbb{R}, i = \sqrt{-1}\}$$
	Complex numbers are elements of the complex field $(\mathbb{C})$, therefore, they obey all the properties of a field. 
	
	We will denote complex numbers by $z = x + iy$ with $x, y \in \mathbb{R}$, and refer the real part as $\Re(z) = \operatorname{Re}(z) = x$ and imaginary part as $\Im(z) = \operatorname{Im}(z) = y$. Complex numbers can also be defined as an ordered pair $z = (x, y)$ which is interpreted as points in the complex plane. $(x, 0)$ are points on the real axis while $(0 , y)$ are points in the imaginary axis. This expression is often called a Couple, and was presented in 1833 by mathematician William Rowan Hamilton (1805 - 1865).
	
	\begin{center}
		\begin{tikzpicture}
		\begin{axis}[
			small,
			%			title={},
			xlabel={$\Re$},
			ylabel={$\Im$},
			xmin=0, xmax=10,
			ymin=0, ymax=10,
			xtick={100},
			ytick={100},
			axis lines = left,
%			legend pos=north west,
%			ymajorgrids=true,
%			grid style=dashed,
			]
			
			\addplot[
			only marks,
			nodes near coords,
			point meta = explicit symbolic, 
			color=Black,
			mark=*,
			]
			coordinates {
				(7, 5) [$z = x + iy$]
			};
%			\legend{$z = x + iy$}
		\end{axis}
	\end{tikzpicture}
	\end{center}
	Like numbers in $\mathbb{R}$, numbers in $\mathbb{C}$ obey the commutative, distributive, and associative laws. We add and multiply complex numbers in the usual way: 
	\begin{align*}
		z_1 + z_2 &= (x_1 + iy_1) + (x_2 + iy_2) & z_1 z_2 &= (x_1 + iy_1) (x_2 + iy_2) \\
			&= (x_1 + x_2) + i(y_1 + y_2) & &=(x_1 x_2 - y_1 y_2) + i(x_1 y_2 + x_2 y_1)
	\end{align*}
	$\forall z \in \mathbb{C}$, there is an unique additive inverse $(-z)$ and $\forall z \in \mathbb{C}\setminus\{0\}$, there is an unique multiplicative inverse $(z^{-1})$ such that 
	\begin{align*}
		&z + (-z) = 0  & &zz^{-1} = 1 \\
		&\implies -z = -x - iy & &\implies (x_1 x_2 - y_1 y_2) = 1 \land (x_1 y_2 + x_2 y_1) = 0 \\
		& & &\implies z^{-1} = \frac{x_1}{x_1^2 + y_1^2} - i \frac{y_1}{x_1^2 + y_1^2}
	\end{align*}
	The existence and uniqueness of the inverses can be easily proven. 
	
	The addition of complex numbers may also be interpreted as akin to vector addition. 
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				small,
				%			title={},
				xlabel={$\Re$},
				ylabel={$\Im$},
				xmin=0, xmax=10,
				ymin=0, ymax=10,
				xtick={100},
				ytick={100},
				axis lines = left,
				%			legend pos=north west,
				%			ymajorgrids=true,
				%			grid style=dashed,
				]
				
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(7, 5) [$z_1$]
					(1, 4) [$z_2$]
					(8, 9) [$z_1 + z_2$]
				};
				%			\legend{$z = x + iy$}
				\draw[line width=1pt,Grey,-stealth](0,0)--(70,50);
				\draw[line width=1pt,Grey,-stealth](0,0)--(10,40);
				\draw[line width=1pt,Grey,-stealth](70,50)--(80,90);
				\draw[line width=1pt,Grey,-stealth](10,40)--(80,90);
				\draw[line width=1pt,black,-stealth](0,0)--(80,90);
			\end{axis}
	\end{tikzpicture}
	\end{center}
	Note: As a group with addition, \(\mathbb{R}^2 \cong \mathbb{C}\), however this is not the case for rings. \(\mathbb{C}\) is a field, but \(\mathbb{R}^2\) is not. \(\mathbb{R}^2\) have non-zero divisors (ie. Take any \(a,b \in \mathbb{R}\), \((a, 0) \cdot (0, b) = 0\)).
	
	\section{Triangle Inequality} \label{Triangle Inequality Section - Complex}
	It is not analysis without a section dedicated to the triangle inequality. For any given number $z_1, z_2 \in \mathbb{C}$ it makes no sense to write an inequality $z_1 = a_1 + ib_1 <  a_2 + ib_2 = z_2$. Thus, we need have a different notion of size. 
	
	\begin{definition}[Modulus]
		The modulus of a complex number is a function $\mathbb{C} \rightarrow \mathbb{R}_{>0}$:
		$$\abs{z} = \sqrt{x^2 + y^2} = \sqrt{z \bar{z}}$$
	\end{definition}
	It is obvious why the definition is not $\abs{z} = \sqrt{x^2 + (iy)^2}$ as problems arise when $x = y$. The modulus is the distance of $z$ from $(0, 0)$. $\bar{z}$ is the complex conjugate of $z$, which is explored in \cref{Complex Conjugate Section - Complex}
	
	\begin{theorem}[Triangle Inequality]
		$\forall z_1, z_2 \in \mathbb{C} [\abs{z_1 + z_2} \leq \abs{z_1} + \abs{z_2}]$
		\label{Triange Inequality - Complex}
	\end{theorem}
	
	From the theorem, we can derive a similar inequality: 
	\begin{align*}
		\abs{z_1} = \abs{z_1 + z_2 - z_2} \leq \abs{z_1 + z_2} + \abs{-z_2}
		 &\implies \abs{z_1} - \abs{z_2} \leq \abs{z_1 + z_2}
	\end{align*}
	
	An important property of polynomials is observed when \cref{Triange Inequality - Complex} is applied to polynomials.
	
	\begin{corollary}
		Consider the polynomial $P(z)$ where $a_n \in \mathbb{C}$, $n \in \mathbb{N}$, $a_0 \neq 0$, and $z \in \mathbb{C}$.
		$$P(z) = a_0 + a_1 z + a_2 z^2 + \ldots + a_n z^n$$
		Then $\forall z, \exists R \in \mathbb{R}_{>0}, \abs{z} < R$ such that
		$$\abs{\frac{1}{P(z)}} < \frac{2}{\abs{a_n} R^n}$$
		\label{Complex Poly Reciprocal Bounded Corollary - Complex}
	\end{corollary}
	\begin{proof}
		Consider 
		\begin{align*}
			w &= \frac{P(z)}{z_n} - a_n = \frac{a_0}{z^n} + \frac{a_1}{z^{n-1}} + \ldots + \frac{a_{n-1}}{z} 
				& z \neq 0 \\
			&\implies wz^n = a_0 + a_1 z + \ldots + a_{n-1}z^{n-1} \\
			&\implies \abs{w}\abs{z}^n \leq \abs{a_0} + \abs{a_1}\abs{z} + \ldots + \abs{a_{n-1}}\abs{z}^{n-1} \\
			&\implies \abs{w} \leq \frac{\abs{a_0}}{\abs{z}^n} + \frac{\abs{a_1}}{\abs{z}^{n-1}} + \ldots + \frac{\abs{a_{n-1}}}{\abs{z}} 
				& \\
			&\implies \abs{w} < n\frac{\abs{a_n}}{2n} = \frac{\abs{a_n}}{2} 
				& \exists \text{ sufficiently large } R < \abs{z}  \text{ s.t.}\\
			& 	& \forall m, \ 0 \leq m \leq n-1, \ \frac{\abs{a_m}}{\abs{z}^{n-m}} < \frac{\abs{a_n}}{2n} \\
			&\implies \abs{a_n + w} \geq \abs{\abs{a_n} - \abs{w}} > \frac{\abs{a_n}}{2}
				& R < \abs{z} \\
			&\implies \abs{P_n(z)} = \abs{a_n + w} \abs{z}^n > \frac{\abs{a_n}}{2}\abs{z}^n > \frac{\abs{a_n}}{2} R^n 
				& R < \abs{z} \\
			&\implies \abs{\frac{1}{P(z)}} < \frac{2}{\abs{a_n} R^n}
		\end{align*}
	\end{proof}
	This tells us that if $z$ is a solution to a polynomial $P(z)$, then the reciprocal of the polynomial $1/P(z)$ is bounded above by $R = \abs{z}$. (i.e. It is bounded by a circle of radius $\abs{z}$.)
	
	\section{Polar and Exponential Form} \label{Polar and Exponential Form Section - Complex}
	\begin{definition}[Argument of $z$]
		Consider any $z \in \mathbb{C}$ where $z \neq 0$.
		Let $\theta$ be the angle in radians between $z$ and the real axis .
		Then $\forall n \in \mathbb{N}$, $-\pi < \theta \leq \pi$, the argument of $z$:
		$$ \operatorname{arg}(z) = \theta + 2 n \pi$$
		\label{Argument - Complex}
	\end{definition}
	
	We know $\forall n \in \mathbb{N}$, $\theta + 2 \pi n = \theta$. This leads us to the definition of the principal argument of $z$.
	\begin{definition}[Principal Argument of $z$]
		Consider any $z \in \mathbb{C}$ where $z \neq 0$.
		Let $\theta$ be the angle in radians between $z$ and the real axis.
		Then for $-pi < \theta \leq \pi$, the principal argument of $z$:
		$$\operatorname{Arg}(z) = \theta$$
		\label{Principal Argument - Complex}
	\end{definition}
	It is clear that $\operatorname{arg}(z) = \operatorname{Arg}(z) + 2n \pi$. It is common for the principal argument to be defined $-\pi < \theta \leq \pi$, although other definitions use $0 \leq \theta < 2\pi$.
	
	\begin{definition}[Polar Form of $z$]
		Consider $z \in \mathbb{C}$. Let $r = \abs{z}$, and $\theta = \operatorname{arg}(z)$. 
		Then $\forall z \in \mathbb{C}, z \neq 0$:
		$$z = x + iy = r(\cos(\theta) + i \sin(\theta))$$
		\label{Polar Form of z - Complex}
	\end{definition}

	Notice that all three definitions require that $z \neq 0$ as $\theta$ is undefined at $z = 0$.
	
	\begin{theorem}[Euler's Formula]
		$$e^{i \theta} = \cos(\theta) + i \sin(\theta)$$
		\label{Euler's Formula - Complex}
	\end{theorem}
	Combining \cref{Polar Form of z - Complex} with \cref{Euler's Formula - Complex}, we obtain the Exponential Form of $z$: 
	
	\begin{definition}[Exponential Form of $z$]
	Consider any $z \in \mathbb{C}$, and let $r = \abs{z}$ and $\theta = \operatorname{Arg}(z)$. Then the exponential form of $z$:
		$$z = r e^{i \theta}$$
		\label{Exponential Form of z - Complex}
	\end{definition}
	Note: $\theta = \tan^{-1}(y/x)$ and $r = \sqrt{x^2 + y^2}$.
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				small,
				%			title={},
				xlabel={$\Re$},
				ylabel={$\Im$},
				xmin=0, xmax=10,
				ymin=0, ymax=10,
				xtick={100},
				ytick={100},
				axis lines = left,
				%			legend pos=north west,
				%			ymajorgrids=true,
				%			grid style=dashed,
				]
				
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(7, 5) [$z = x + iy = re^{i\theta}$]
				};
				%			\legend{$z = x + iy$}
				\draw[thick]  (0,0) to ["$r=\abs{z}$"] (70,50);
				% angle
				\draw[draw=blue] (0,0) ++(35:50) arc (35:0:50)
				node[midway,above right,inner sep=2pt,font={\footnotesize}]{$\theta$};
			\end{axis}
		\end{tikzpicture}
	\end{center}
	
	\subsection{Properties of Polar and Exponential Form} \label{Properties of Polar and Exponential Form Subsection - Complex}
	It would be easier to work with the exponential form of $z$ then convert it to the polar form later. The exponential form of a complex number is part of the exponential family of functions, thus possess all the properties of the family. Consider any complex number $z_1 = r_1 e^{i\theta_1}$ and $z_2 = r_2 e^{i\theta_2}$.
	\begin{align*}
		z_1  z_2 &= r_1 r_2 e^{i(\theta_1 + \theta_2)} 
			& z^n &= r^n e^{i n \theta} \qquad \forall n \in \mathbb{Z}
	\end{align*}
	A special case arrives for integer exponential of $z$ on the unit circle.
	\begin{theorem}[de Moivre's Formula]
		Consider any $z = e^{i \theta} \in \mathbb{C}$ on the unit circle, and let $n \in \mathbb{Z}$.
		\begin{align*}
			\forall z\in \mathbb{C} \ \forall n \in \mathbb{Z}
			\left[\abs{z} = 1 \implies (\cos(\theta) + i\sin(\theta))^n = \cos(n \theta) + i\sin(n \theta)\right]
		\end{align*}
		\label{de Moivre's Formula Theorem - Complex}
	\end{theorem}
	\begin{proof}
		Consider $z = e^{i \theta}$ and let $n \in \mathbb{Z}$. 
		\begin{align*}
			z^n = (e^{i \theta})^n = e^{in\theta} = \cos(n\theta) + i\sin(n\theta)
		\end{align*}
	\end{proof}

	The proof hints that \cref{de Moivre's Formula Theorem - Complex} can be generalized to $\forall n \in \mathbb{R}$, which we will see shortly in \cref{Roots of z Section - Complex}. Using \cref{de Moivre's Formula Theorem - Complex}, we can obtain the double angle identities.
	
	\begin{corollary}[Double Angle Identities]
		\begin{align*}
			\cos(2 \theta) &= \cos^2(\theta) - \sin^2(\theta) 
				& \sin(2\theta) &= 2\sin(\theta)\cos(\theta)
		\end{align*}
	\end{corollary}
	\begin{proof}
		Consider any $z$ on the unit circle, that is $z=e^{i\theta}$.
		\begin{align*}
			&(\cos(\theta) + i \sin(\theta))^2 = \cos(2\theta) + i \sin(2\theta)
				&\text{\Cref{de Moivre's Formula Theorem - Complex}} \\
			&\implies \cos^2(\theta) - \sin^2(\theta) + i2\sin(\theta)\cos(\theta) = cos(2\theta) + i\sin(2\theta) 
		\end{align*}
		Equating the real and imaginary parts yield the desired results. 
	\end{proof}

	\subsection{Properties of Arguments} \label{Properties of Argumetns Subsection - Complex}
	Recall from \cref{Properties of Polar and Exponential Form Subsection - Complex}: 
	\begin{align*}
		z_1  z_2 &= r_1 r_2 e^{i(\theta_1 + \theta_2)} 
		& z^n &= r^n e^{i n \theta} \qquad \forall n \in \mathbb{Z}
	\end{align*}
	The arguments for the arguments of products of any $z_1, z_2 \in \mathbb{C}$ follows immediately from the properties of the exponential.
	\begin{corollary}[Arguments of Products]
		\begin{align*}
			\arg(z_1 z_2) &= \arg(z_1) + \arg(z_2)
				& \operatorname{Arg}(z_1 z_2) = \operatorname{Arg}(z_1) + \operatorname{Arg}(z_2) \\
			\arg(z^n) &= n\arg(z)  
				& \operatorname{Arg}(z^n) = n\operatorname{Arg}(z)
		\end{align*}
		\label{Arguments of Products Corollary - Complex}
	\end{corollary}
	\begin{proof}
		\begin{align*}
			z_1  z_2 &= r_1 r_2 e^{i(\theta_1 + \theta_2)} & \\
			&\implies \arg(z_1 z_2) = \arg(z_1) + 2n_1 \pi + \arg(z_2) + 2n_2\pi
				& n_1, n_2 \in \mathbb{Z} \\
			&\implies \arg(z_1 z_2) = \arg(z_1) + \arg(z_2) & \\
			&\implies \operatorname{Arg}(z_1 z_2) = \operatorname{Arg}(z_1) = \operatorname{Arg}(z_2)  \\
			\\
			z^n &= r^n e^{i n\theta} & \\
			&\implies \arg(z^n) = n\arg(z) + 2n\pi & n \in \mathbb{Z} \\
			&\implies \arg(z^n) = n\arg(z)  \\
			&\implies \operatorname{z^n} = n\operatorname{Arg}(z)
		\end{align*}
	\end{proof}
	It is clear that: 
	\begin{align*}
		\arg\left(\frac{z_1}{z_2}\right) &= \arg(z_1) - \arg(z_2)
			& \operatorname{Arg}\left(\frac{z_1}{z_2}\right) &= \operatorname{Arg}(z_1) - \operatorname{Arg}(z_2)
	\end{align*}

	\section{Roots of $z$} \label{Roots of z Sections - Complex}
	In \cref{Exponential Form of z - Complex}, you might be wondering why $z^n = r^n e^{i n \theta}$ is not for $n \in \mathbb{R}$. That is because there is more things to consider, which we will explore in this section. Recall that $z = re^(i \theta) = re^{i (\theta + 2n \pi)}$ for $n \in \mathbb{Z}$. 
	
	\begin{definition}[Exponential of $z$]
		Consider any $z \in \mathbb{C}$ and any $x \in \mathbb{R}$
		$$z^x = \left(r e^{i(\theta + 2 n \pi)} \right)^x = r^x e^{i x(\theta + 2n \pi)}$$
		\label{Exponential of z Definiion - Complex}
	\end{definition}

	For $x \nin \mathbb{Z}$, it is clear that $z^x =  r^x e^{ix(\theta + 2 n \pi)} \neq r^x e^{ix\theta}$, since $2nx\pi = 0 \iff nx \in \mathbb{Z}$. In order to define the roots of $z$ we must need a more general and proper definition of $z$.
	
	\begin{definition}[Roots of $z_0$]
		\label{Roots of z Definition - Complex}
		Consider any $z_0 \in \mathbb{C}$ and any $m \in \mathbb{N}$.
		$$z_0^{\frac{1}{m}} = r_0^\frac{1}{m} e^{i\left(\frac{\theta_0 + 2n \pi}{m}\right)} = r_0^\frac{1}{m} e^{i \left(\frac{\theta_0}{m} + \frac{2n \pi}{m}\right)}$$
	\end{definition}  
	
	Taking the $m$-th root of $z_0 \in C$ scales $\theta_0$ by $1/m$, and provides solutions at equally spaced by $2\pi / m$ on a circle of radius $r^{1/m}$. That is, the roots lie on the vertices of a regular n-sided polygon inscribed in a circle of radius $\abs{z}^{1/m}$. 
	
	\begin{example}
		Consider $z_0 = 32 e^{i(5/6)\pi}$, then $z_0^{(1/5)} = 3e^{i(\pi/6) + i(2/5) n \pi}$ for $n \in \mathbb{Z}$. The radius went from $35$ to $35^{(1/5)} = 2$, and five roots appear equally spaced with distance of $(2/5)\pi$ on a circle with radius $2$. Before and after graphs are as follows, note graph on right is zoomed in: 
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
				\begin{axis}[
					width=8.9cm,
					unit vector ratio=1 1 1,
					xlabel={$\Re$},
					ylabel={$\Im$},
					xmin=-30, xmax=30,
					ymin=-30, ymax=30,
%					xtick={-20,-10,10,20},
%					ytick={-20,-10,10,20},
					xtick=100,
					ytick=100,
					axis lines = middle,
					]
					
					\addplot[
					only marks,
					nodes near coords,
					point meta = explicit symbolic, 
					color=Black,
					mark=*,
					]
					coordinates {
						(-27.7128, 16) [$z_0 = 32e^{i(5/6)\pi}$]
					};
				\end{axis}
%				\path (current bounding box.north east) -- 
%				(current bounding box.south east) coordinate[midway] (2BL);
			\end{tikzpicture}
			{$\Large \xrightarrow{\mathmakebox[2cm] {f(z) = z^{1/5}}}$}
%			\hspace*{2.8cm}
			\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
				\begin{axis}[
%					small,
					width=8.9cm,
					unit vector ratio=1 1 1,
					xlabel={$\Re$},
					ylabel={$\Im$},
					xmin=-3.1, xmax=3.1,
					ymin=-3.1, ymax=3.1,
%					xtick distance=2,
%					ytick distance=2,
					xtick=10,
					ytick=10,
					axis lines = middle,
					axis line style = Black,
					]
					\addplot[
					only marks,
					nodes near coords,
					point meta = explicit symbolic, 
					color=Black,
					mark=*,
					]
					coordinates {
						(1.73205, 1) [$2e^{i\pi/6}$]
						(-0.4158, 1.9563) [$2e^{i17\pi/30}$]
						(-1.9890, 0.20906) [$2e^{i29\pi/30}$]
						(-0.8135, -1.8271) [$2e^{-i19\pi/30}$]
						(1.4863, -1.3383) [$2e^{-i7\pi/30}$]
					};
					\draw[line width=1pt,LightGrey](axis cs: 0, 0) circle [radius=200];
					\draw[line width=1pt,LightGrey](axis cs: 0,0)--(axis cs: 1.73205, 1);
					\draw[line width=1pt,LightGrey](axis cs:0,0)--(axis cs:-0.4158, 1.9563);
					\draw[line width=1pt,LightGrey](axis cs:0,0)--(axis cs:-1.9890, 0.20906);
					\draw[line width=1pt,LightGrey](axis cs:0,0)--(axis cs:-0.8135, -1.8271);
					\draw[line width=1pt,LightGrey](axis cs:0,0)--(axis cs:1.4863, -1.3383);
				\end{axis}
%				\path (current bounding box.north west) -- 
%				(current bounding box.south west) coordinate[midway] (2BR);
			\end{tikzpicture}
%			\tikz[overlay,remember picture]{\draw[->, very thick] 
%			($(2BL)+(0.5, 0)$) -- ($(2BR)+(-0.5,0)$)
%			node[midway,above,text width=2cm]{$f(z) = z^\frac{1}{5}$};} 
		\end{figure}
	\end{example} 
	
	We can see that the roots of $z_0$ form a set:
	\begin{definition}[Set of roots of $z_0$]
		\label{Set of roots of z - Complex}
		Consider the $m$-th root of any $z_0 \in \mathbb{C}$. Let: 
		\begin{align*}
			z_0 &= r_0 e^{i\theta_0}
			&c_0 &= r_0^{1/m} e^{i\theta_0/m} 
			&\omega_n &= e^{\frac{i2\pi}{m}} & \ m \in \mathbb{N}
		\end{align*}
		Then the set of roots of $z_0$:
		\begin{align*}
			z_0^{1/m} = \left\{c_k = c_0 \omega_m^k \mid k\in \mathbb{N}, \ 0 \leq k < m\right\}
		\end{align*}
	\end{definition}
	$c_0$ is the principal root. The root corresponding to the principal argument of $z$.
	\begin{definition}[Principal Root]
	Consider the $m$-th root of any $z_0 \in \mathbb{C}$. The principal root of $z_0$ is defined as:
		$$c_0 = r_0^{\frac{1}{m}} e^{i\frac{\theta_0}{m}}$$
	\end{definition}
	
	\begin{example}
		Recall from the previous example: $z_0 = 32 e^{i(5/6)\pi}$. This gives us
		\begin{align*}
			c_0 &= 32^{1/5} e^{i\pi/6}= 2 e^{i\pi/6}
				&\omega_5 &= e^{i2\pi/5}
		\end{align*}
		Then
		\begin{align*}
			c_0 &= c_0 \omega_5^0 = 2 e^{i\pi/6} \\
			c_1 &= c_0 \omega_5^1 = 2 e^{i\pi/6} e^{i2\pi/5} = 2 e^{i17\pi/30} \\
			c_2 &= c_0 \omega_5^1 = 2 e^{i\pi/6} e^{i4\pi/5} = 2 e^{i29\pi/30} \\
			c_3 &= c_0 \omega_5^1 = 2 e^{i\pi/6} e^{i6\pi/5} = 2 e^{i41\pi/30} = 2 e^{-i19\pi/30}\\
			c_4 &= c_0 \omega_5^1 = 2 e^{i\pi/6} e^{i8\pi/5} = 2 e^{i53\pi/30} = 2 e^{-i7\pi/30}\\
		\end{align*}
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[remember picture]
				\begin{axis}[
					%					small,
					width=10cm,
					unit vector ratio=1 1 1,
					xlabel={$\Re$},
					ylabel={$\Im$},
					xmin=-3.1, xmax=3.1,
					ymin=-3.1, ymax=3.1,
					%					xtick distance=2,
					%					ytick distance=2,
					xtick=10,
					ytick=10,
					axis lines = middle,
					axis line style = Black,
					]
					\addplot[
					only marks,
					nodes near coords,
					point meta = explicit symbolic, 
					color=Black,
					mark=*,
					]
					coordinates {
						(1.73205, 1) [$c_0 = 2e^{i\pi/6}$]
						(-0.4158, 1.9563) [$c_1 = 2e^{i17\pi/30}$]
						(-1.9890, 0.20906) [$c_2 = 2e^{i29\pi/30}$]
						(-0.8135, -1.8271) [$c_3 = 2e^{-i19\pi/30}$]
						(1.4863, -1.3383) [$c_4 = 2e^{-i7\pi/30}$]
					};
					\draw[line width=1pt,LightGrey](axis cs: 0, 0) circle [radius=200];
					\draw[line width=1pt,LightGrey](axis cs: 0,0)--(axis cs: 1.73205, 1);
					\draw[line width=1pt,LightGrey](axis cs:0,0)--(axis cs:-0.4158, 1.9563);
					\draw[line width=1pt,LightGrey](axis cs:0,0)--(axis cs:-1.9890, 0.20906);
					\draw[line width=1pt,LightGrey](axis cs:0,0)--(axis cs:-0.8135, -1.8271);
					\draw[line width=1pt,LightGrey](axis cs:0,0)--(axis cs:1.4863, -1.3383);
				\end{axis}
				\path (current bounding box.north west) -- 
				(current bounding box.south west) coordinate[midway] (2BR);
			\end{tikzpicture}
		\end{figure}
	\end{example}
	
	
	\section{Complex Conjugate} \label{Complex Conjugate Section - Complex}
	\begin{definition}[Complex Conjugate]
		The complex conjugate of $z \in \mathbb{C}$ is denoted $\bar{z}$.
		$$\bar{z} = x - iy = r(\cos(\theta) - i\sin(\theta)) = re^{-i\theta}$$
		\label{Complex Conjugate}
	\end{definition}
	Graphically, it is the reflection of $z$ across the real axis.
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				small,
				xlabel={$\Re$},
				ylabel={$\Im$},
				xmin=0, xmax=10,
				ymin=-10, ymax=10,
				xtick={100},
				ytick={100},
				axis lines = middle,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(7, 5) [$z = x + iy$]
					(7, -5) [$\bar{z} = x - iy$]
				};
			\end{axis}
		\end{tikzpicture}
	\end{center}
	It is then easy to see
	\begin{align*}
		\operatorname{Re}(z) &= \frac{z + \bar{z}}{2} & \operatorname{Im}(z) &= \frac{z - \bar{z}}{2i} & \abs{z}^2 = z \bar{z}
	\end{align*}
	As $\operatorname{Re}(z) = x = r \cos(\theta)$ and $\operatorname{Im}(z) = y = r \sin(\theta)$ and using \cref{Exponential Form of z - Complex}, we can obtain the complex forms of sine and cosine: 
	\begin{definition}[Complex Sine and Cosine]
		\begin{align*}
			\cos(\theta) &= \frac{e^{i \theta} + e^{-i \theta}}{2} 
			&\sin(\theta) &= \frac{e^{i \theta} - e^{-i \theta}}{2i}
		\end{align*}
		\label{Trig Identities - Complex}
	\end{definition}
	It is easy to prove \(\forall z_1, z_2 \in \mathbb{C}\): 
	\begin{align*}
		\overline{z_1 + z_2} &= \overline{z_1} + \overline{z_2} &
		\overline{z_1 \cdot z_2} &= \overline{z_1} \cdot \overline{z_2}
	\end{align*}

	\section{Operations as Transformations} \label{Operations as Transformations Section - Complex}
	
	Consider any $z \in \mathbb{C}$. A function $f: \mathbb{C} \rightarrow \mathbb{C}$ can be viewed as transformations of the complex plane. 
	
	\begin{example}[Addition as translation]
		Consider any $z_0 \in \mathbb{C}$, $z_0 = a + ib$ for $a, b \in \mathbb{R}$. Addition by $z_0$ can be seen as a shift in the complex plane by $a + bi$. (i.e. It takes the origin and shifts it by $z_0$.)
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
				\begin{axis}[
					width=8.9cm,
					unit vector ratio=1 1 1,
					xlabel={$\Re$},
					ylabel={$\Im$},
					xmin=-10, xmax=10,
					ymin=-10, ymax=10,
					axis lines = middle,
					xticklabels={}, yticklabels={},
					grid=both,
					axis lines=middle,
					minor tick num=4,
					minor tick style={draw=none},
					]
					
					\addplot[
					only marks,
					nodes near coords,
					point meta = explicit symbolic, 
					color=Blue,
					mark=*,
					]
					coordinates {
						(0,0)
					};
				\end{axis}
			\end{tikzpicture}
			{$\Large \xrightarrow{\mathmakebox[2cm] {f(z_0) = z + z_0}}$}
			\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
				\begin{axis}[
					%					small,
					width=8.9cm,
					unit vector ratio=1 1 1,
					xlabel={$\Re$},
					ylabel={$\Im$},
					xmin=-10, xmax=10,
					ymin=-10, ymax=10,
					xticklabels={}, yticklabels={},
					grid=both,
					axis lines=middle,
					minor tick num=4,
					minor tick style={draw=none},
					]
					\addplot[
					only marks,
					nodes near coords,
					point meta = explicit symbolic, 
					color=Blue,
					mark=*,
					]
					coordinates {
						(4, 3) [$z_0=a+bi$]
					};
					\draw[->, very thick](axis cs:0,0)--(axis cs:4, 3);
				\end{axis}
			\end{tikzpicture}
		\end{figure}
	\end{example}

	\begin{example}[Multiplication as scaling and rotation]
		Consider any $z_0 \in \mathbb{C}$, $z_0 = re^{i\theta}$. Multiplication by $z_0$ scales the entire complex plane by $r$ and rotates it by $\theta$. (Imagine rotating and stretching out a net.)
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
				\begin{axis}[
					width=8.9cm,
					unit vector ratio=1 1 1,
					xlabel={$\Re$},
					ylabel={$\Im$},
					xmin=-10, xmax=10,
					ymin=-10, ymax=10,
					axis lines = middle,
					xticklabels={}, yticklabels={},
					grid=both,
					axis lines=middle,
					minor tick num=4,
					minor tick style={draw=none},
					]
					
					\addplot[
					only marks,
					nodes near coords,
					point meta = explicit symbolic, 
					color=Blue,
					mark=*,
					]
					coordinates {
						(1,0) [$1 = e^{0}$]
					};
				\draw[->, very thick](axis cs:0,0)--(axis cs:1, 0);
				\end{axis}
			\end{tikzpicture}
			{$\Large \xrightarrow{\mathmakebox[2cm] {f(z_0) = z \cdot z_0}}$}
			\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
				\begin{axis}[
					width=8.9cm,
					unit vector ratio=1 1 1,
					xlabel={$\Re$},
					ylabel={$\Im$},
					xmin=-10, xmax=10,
					ymin=-10, ymax=10,
					xticklabels={}, yticklabels={},
%					ytick distance={3},
%					xtick distance={4},
%					grid=both,
					axis lines=middle,
					minor tick num=0,
					minor tick style={draw=none},
					]
					\addplot[
					only marks,
					nodes near coords,
					point meta = explicit symbolic, 
					color=Blue,
					mark=*,
					]
					coordinates {
						(4, 3) [$z_0=re^{i\theta}$]
					};
%					\draw[->,thick] (axis cs:0,0)--(axis cs:4,3);
					\draw[->,thick,draw=Blue]  (axis cs: 0,0) to ["$r$"] (axis cs: 4,3);
					% angle
					\draw[->, draw=Blue] (axis cs:0,0)++(0:30) arc (0:35:30)
					node[midway,above right,inner sep=2pt,font={\footnotesize}]{$\theta$};
				\end{axis}
%				\begin{axis}[
%					anchor=center, % Shift the axis so its origin is at (0,0)
%					rotate around={36.87:(current axis.origin)}, % Rotate around the origin
%					width=8.9cm,
%					unit vector ratio=1 1 1,
%					xmin=-10, xmax=10,
%					ymin=-10, ymax=10,
%					xticklabels={}, yticklabels={},
%					grid=both,
%					axis lines=middle,
%					minor tick num=0,
%					minor tick style={draw=none},
%					]
%				\end{axis}
%				\pgftransformrotate{36.87}
%				\pgfpathgrid[stepx=50,stepy=50]{\pgfpoint{0}{-50}}{\pgfpoint{200}{150}}
%				\pgfusepath{stroke}
			\end{tikzpicture}
		\end{figure}
	\end{example}

%	\begin{example}[Logarithm as compression (Speculation)]
%		content...
%	\end{example}
	
	\section{Complex Analysis Definitions} \label{Complex Analysis Definitions Section - Complex}
	
	\begin{definition}[Neighbourhood]
		\label{Neighbourhood Definition - Complex}
		A neighbourhood of a point $z_0$ is the set of all points $z$ with distance less than $\epsilon$. 
		$$\{z : \abs{z-z_0} < \epsilon \}$$
		i.e. It is the set of all points that lie within a circle centred at $z_0$ with radius $\epsilon$. Points on the circumference not included. 
	\end{definition}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[remember picture]
			\begin{axis}[
				small,
				unit vector ratio=1 1 1,
				xlabel={$\Re$},
				ylabel={$\Im$},
				xmin=0, xmax=10,
				ymin=0, ymax=8,
				%					xtick distance=2,
				%					ytick distance=2,
				xtick=100,
				ytick=100,
				axis lines = middle,
				axis line style = Black,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(7, 4) [$z_0$]
				};
				\draw[dashed,line width=1pt,Grey](axis cs: 7, 4) circle (2);
				\draw[-,draw=Grey]  (axis cs: 7,4) to ["$\epsilon$"] (axis cs: 5,4);
			\end{axis}
		\end{tikzpicture}
	\end{figure}

	\begin{definition}[Deleted Neighbourhood]
		\label{Deleted Neighbourhood Definition - Complex}
		A deleted neighbourhood is the set of all points $z$ with distance less than $\epsilon$ from a point $z_0$, not including $z_0$. That is, it is a neighbourhood of $z_0$ without $z_0$.
		$$\{z : \abs{z-z_0} < \epsilon, \ z \neq z_0 \}$$
	\end{definition}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[remember picture]
			\begin{axis}[
				small,
				unit vector ratio=1 1 1,
				xlabel={$\Re$},
				ylabel={$\Im$},
				xmin=0, xmax=10,
				ymin=0, ymax=8,
				%					xtick distance=2,
				%					ytick distance=2,
				xtick=100,
				ytick=100,
				axis lines = middle,
				axis line style = Black,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=o,
				]
				coordinates {
					(7, 4) [$z_0$]
				};
				\draw[dashed,line width=1pt,Grey](axis cs: 7, 4) circle (2);
				\draw[-,draw=Grey]  (axis cs: 7,4) to ["$\epsilon$"] (axis cs: 5,4);;
			\end{axis}
		\end{tikzpicture}
	\end{figure}

	\begin{definition}[Interior Point]
		\label{Interior Point Definition - Complex}
		Let $S$ be a set. A point $z_0$ is an interior point of $S$ if $\exists \epsilon$ such that $\forall z$, $\abs{z-z_0} < \epsilon \implies z \in S$. That is, $z_0$ is an interior point of $S$ if it has a neighbourhood where all points in the neighbourhood is an element of $S$.
	\end{definition}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[remember picture]
			\begin{axis}[
				small,
				unit vector ratio=1 1 1,
				xlabel={$\Re$},
				ylabel={$\Im$},
				xmin=0, xmax=10,
				ymin=0, ymax=8,
				%					xtick distance=2,
				%					ytick distance=2,
				xtick=100,
				ytick=100,
				axis lines = middle,
				axis line style = Black,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(7, 4) [$z_0$]
				};
				\draw[dashed,line width=1pt,Grey](axis cs: 7, 4) circle (1);
				\draw[line width=1pt, Black](axis cs: 5.2, 3.5) circle (3);
				\draw[-,draw=Grey]  (axis cs: 7,4) to ["$\epsilon$"] (axis cs: 6,4);
				\draw[]  (axis cs: 5,2) to ["$S$"] (axis cs: 5,2);
			\end{axis}
		\end{tikzpicture}
	\end{figure}

	\begin{definition}[Exterior Point]
		\label{Exterior Point Definition - Complex}
		Let $S$ be a set. A point $z_0$ is an exterior point of $S$ if $\exists \epsilon$ such that $\forall z$, $\abs{z - z_0}<\epsilon \implies z \notin S$. That is, $z_0$is an exterior point of $S$ if it has neighbourhood that does not contain any element of $S$. 
	\end{definition}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[remember picture]
			\begin{axis}[
				small,
				unit vector ratio=1 1 1,
				xlabel={$\Re$},
				ylabel={$\Im$},
				xmin=0, xmax=10,
				ymin=0, ymax=8,
				%					xtick distance=2,
				%					ytick distance=2,
				xtick=100,
				ytick=100,
				axis lines = middle,
				axis line style = Black,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(9, 5) [$z_0$]
				};
				\draw[dashed,line width=1pt,Grey](axis cs: 9, 5) circle (1);
				\draw[line width=1pt, Black](axis cs: 5.2, 3.5) circle (3);
				\draw[-,draw=Grey]  (axis cs: 9,5) to ["$\epsilon$"] (axis cs: 8,5);
				\draw[]  (axis cs: 5,2) to ["$S$"] (axis cs: 5,2);
			\end{axis}
		\end{tikzpicture}
	\end{figure}

	\begin{definition}[Boundary Point]
		\label{Boundary Point Definition - Complex}
		Let $S$ be a set. A point $z_0$ is a boundary point of $S$ if $\forall \epsilon$, $\exists z \in S, z' \notin S$, such that $\abs{z - z_0} \epsilon$ and $\abs{z' - z_0} < \epsilon$. That is, for all neighbourhoods of $z_0$ there exists a point that is in $S$ and a point not in $S$.
	\end{definition}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[remember picture]
			\begin{axis}[
				small,
				unit vector ratio=1 1 1,
				xlabel={$\Re$},
				ylabel={$\Im$},
				xmin=0, xmax=10,
				ymin=0, ymax=8,
				%					xtick distance=2,
				%					ytick distance=2,
				xtick=100,
				ytick=100,
				axis lines = middle,
				axis line style = Black,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(5.2, 6.5) [$z_0$]
				};
				\draw[dashed,line width=1pt,Grey](axis cs: 5.2, 6.5) circle (1);
				\draw[line width=1pt, Black](axis cs: 5.2, 3.5) circle (3);
				\draw[-,draw=Grey]  (axis cs: 5.2,6.5) to ["$\epsilon$"] (axis cs: 4.2,6.5);
				\draw[]  (axis cs: 5,2) to ["$S$"] (axis cs: 5,2);
			\end{axis}
		\end{tikzpicture}
	\end{figure}

	Note: A boundary point of $S$ may or may not be in $S$.

	\begin{definition}[Boundary of a Set]
		\label{Boundary of a Set Definition - Complex}
		A boundary of a set $S$ is the set of all boundary points of $S$. The set containing all boundary points of $S$.
		$$\{z_0 : \forall \epsilon \exists z\in S, z'\notin S(\abs{z-z_0}<\epsilon \land \abs{z'-z_0}<\epsilon )\}$$ 
	\end{definition}

	\begin{definition}[Open Set]
		\label{Open Set Definition - Complex}
		A set that does not contain any boundary points. 
	\end{definition}

	\begin{theorem}
		Set $S$ is open $\iff$ $\forall s \in S$, $s$ is an interior point of $S$ 
	\end{theorem}
	\begin{proof}
		\textcolor{Grey}{
		\underline{$\implies$}:
		Suppose $S$ is open $\nRightarrow$ $\forall s \in S$, $s$ is an interior point of $S$, for contradiction. That is, $\exists s \in S$ that is either a boundary point or an exterior point. $s \in S$ implies $s$ is not an exterior point of $S$, so $s$ has to be a boundary point of $S$. This contradicts that $S$ is an open set. 
		$$S \text{ is open } \implies \forall s \in S (s \text{ is an interioir point of }S)$$
		\underline{$\impliedby$:}
		\begin{align*}
			&\forall s \in S (s \text{ is an interior point of S}) \\
			&\implies \forall s' \forall \epsilon (\abs{s'-s} < \epsilon \implies s' \in S)\\
			&\implies S \text{ does not contain boundary points} \implies S \text{ is open}
		\end{align*}
		}
	\end{proof}

	A set can be neither open or closed. Consider the set $S = \{z : 0 < \abs{z} \leq 1\}$. $S$ is not closed since it does not contain the boundary point $0$, and it is not open since it contains boundary points where $\abs{z} = 1$. The set $\mathbb{C}$ is both open and closed since it has no boundary points. 
	
	\begin{definition}[Closed Set]
		\label{Closed Set Definition - Complex}
		A set that contains all of its boundary points. 
	\end{definition}

	\begin{definition}[Closure of a Set]
		\label{Closure of a Set Definition - Complex}
		Let $S$ be a set. The closure of S is a closed set containing all points of $S$ and all boundary points of $S$. 
	\end{definition}

	\begin{definition}[Connected Set]
		\label{Connected Set Definition - Complex}
		An opens set $S$ is connected if $\forall z_1, z_2 \in S$, $z_1$ and $z_2$ can be connected by a polygonal line lying within $S$.
	\end{definition}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[remember picture]
			\begin{axis}[
				small,
				unit vector ratio=1 1 1,
				xlabel={$\Re$},
				ylabel={$\Im$},
				xmin=0, xmax=10,
				ymin=0, ymax=8,
				%					xtick distance=2,
				%					ytick distance=2,
				xtick=100,
				ytick=100,
				axis lines = middle,
				axis line style = Black,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(3, 3) [$z_1$]
					(6, 6) [$z_2$]
				};
				\draw[line width=1pt, Black](axis cs: 5, 4) circle (1);
				\draw[line width=1pt, Black](axis cs: 5, 4) circle (3.5);
				\draw[-,draw=Grey]  (axis cs: 3,3) -- (axis cs: 4,5);
				\draw[-,draw=Grey]  (axis cs: 4,5) -- (axis cs: 6,6);
				\draw[]  (axis cs: 5,2) to ["$S$"] (axis cs: 5,2);
			\end{axis}
		\end{tikzpicture}
	\end{figure}

	\begin{definition}[Polygonal Line]
		\label{Polygonal Line Definition - Complex}
		A finite set of line segments joined end to end. 
	\end{definition}

	\begin{definition}[Domain]
		\label{Domain Definition - Complex}
		A nonempty connected set. 
	\end{definition}
	Note: All neighbourhoods are domains. 

	\begin{definition}[Region]
		\label{Region Definition - Complex}
		A domain with none, some, or all of its boundary points. 
	\end{definition}
	
	\begin{definition}[Bounded Set/Region]
		\label{Bounded Set or Region Definition - Complex}
		A set $S$ is bounded if $\exists R = \abs{z} > 0$ such that $\forall s \in S$, $\abs{s} < R$. That is, $S$ is bounded if $\forall s \in S$, $s$ is contained in some circle of radius $R$ centred at the origin. 
	\end{definition}

	\begin{definition}[Closed Region]
		\label{Closed Regoin Definition - Complex}
		A domain with all its boundary points. A bounded and closed region.
	\end{definition}

	\begin{definition}[Accumulation/Limit Point]
		\label{Accumulation/Limit Point Definition - Complex}
		A point $z_0$ is a accumulation point of a set $S$ if all deleted neighbourhood of $z_0$ contains an element of $S$. 
		$$\forall \epsilon \exists s \in S (s \neq z_0 \land \abs{z-s} < \epsilon)$$
	\end{definition}
	Note: Unlike a boundary point, an accumulation point does not require that all neighbourhood of $z_0$ contain an element not in $S$.
	
	\begin{theorem}
		Set $S$ is closed $\iff$ $\forall$ accumulation points $z_0$ of $S$, $z_0 \in S$
	\end{theorem}
	\begin{proof}
		\underline{$\implies$:}
		Let $S$ is closed and $z_0$ is an accumulation point of a set $S$ where $z_0 \notin S$ for contradiction. If $\exists z_0 \notin S$, then $z_0$ is a boundary point of $S$. Contradicts closed set contains all boundary points. 
		
		\textcolor{Grey}{
		\underline{$\impliedby$:}
		Suppose all accumulation points of $S$ are elements of $S$ but $S$ is not closed for contradiction. Then $S$ does not contain one or more boundary points. Suppose $z_0$ is a boundary point of $S$ that is not in $S$. Then $\forall \epsilon \ \exists s\in S$ where $\abs{s-z_0} < \epsilon$, so by considering the deleted neighbourhood of $z_0$, this makes $z_0$ an accumulation point of $S$. This contradicts that all accumulation points of $S$ is in $S$. 		
		}
	\end{proof}

	\chapter{Analytic Functions} \label{Analytic Functions Chapter - Complex}
	\section{Functions as mappings} \label{Functions as Mappings Section - Complex}
	A function $f: S \rightarrow S'$ is a function that maps elements from $S$ to elements on $S'$. The value of $f$ at $z$ is denoted $f(z)$ and the set $S$ is the domain of $f$ while $S'$ is the image of $f$. Recall \cref{Operations as Transformations Section - Complex}, a function can likewise be viewed as a transformation or mapping, that maps $z \in \operatorname{dom}(f) = S$ to values $z' \in \operatorname{img}(f) = S'$.
	
	\begin{definition}[Range]
		Let $f$ be a function with domain $S$ and image $S'$. The range of $f$ is the entire image of $S$.
	\end{definition}

	Note: Image is a subset of range, and can be a single point or a set of points.

	\begin{definition}[Inverse Range]
		The set of all points $s \in S$ with the value $f(s) = s'$ for some $s' \in S'$.
		$$\{s : f(s) = s', \ s' \in S'\}$$
	\end{definition}
	
	Note: The domain of a function is often a domain, but it does not need to be a domain. 
	
	We will consider functions $f: S \rightarrow S'$ where both $S, S' \subseteq \mathbb{C}$. For such functions we can break it into a two real valued functions: 
	\begin{align*}
		f(z) &= u(x, y) + i v(x, y)
			& \operatorname{dom}(u) \subseteq \mathbb{R}, \operatorname{dom}(v) \subseteq \mathbb{R} \\
			&= u(r, \theta) + i v(r, \theta)
	\end{align*}
	Recall that a real-valued function is a function with a domain that is a subset of $\mathbb{R}$ (\cref{Real-Valued Function Definition - Real Analysis}).
	If $\forall z$, $v(x, y) = 0$, then $f$ is called a real-valued function of a complex variable. 
	
	\begin{definition}[Polynomial]
		\label{Polynomial Definition - Complex}
		Let $a_i \in \mathbb{C}$, $0 \leq i \leq n$ where $i, n \in \mathbb{N}\cup\{0\}$. If $a_n \neq 0$, then a polynomial of degree n is
		$$P(z) = a_0 + a_1 z + a_2 z^2 + \ldots + a_n z^n = \sum_{i=0}^n a_i z^i$$
	\end{definition}

	\begin{definition}[Rational Functions]
		\label{Rational Functions Definition - Complex}
		Let $P(z)$ and $Q(z)$ are polynomials, then rational functions are quotients:
		$$\frac{P(z)}{Q(z)}$$
		Defined for all $z$ where $Q(z) \neq 0$.
	\end{definition}
	
	\begin{definition}[Multiple-Valued Function]
		\label{Multiple-Valued Function Definition - Complex}
		Let $f$ be a function and $z \in \operatorname{dom}(f)$. $f$ is a multiple-valued function if it assigns more than one value to a point $z$.
	\end{definition}
	``When multiple-valued functions are studied, usually just one of the possible values assigned at each point is taken, in a systematic manner and a (single-valued) function is constructed from the multiple-valued one'' - Brown and Churchill \cite{Brown.J;Churchill.R-Complex-Variables-2014}
	
	What this means that for $z \in \mathbb{C}$ a function $f$ assigns $u(z)$ and $v(z)$ to to $z$. By taking just $u$ or $v$, we create a single-valued function from a multiple-valued function. 
	
	\begin{example}[$f(z) = z^2$]
		\begin{align*}
			f(z) &= z^2  = x^2 - y^2 + i2xy \\
				&\implies u(x,y) = x^2 - y^2 \qquad v(x,y) = 2xy
		\end{align*}
		By setting $u = x^2 - y^2 = c_1$ where $c_1 \in \mathbb{R}_{>0}$ we can see that
		\begin{align*}
			u &= x^2 - y^2 = c_1 & v &= 2xy = \pm 2y \sqrt{y^2 + c_1}
		\end{align*}
		This tells us that in the complex plane of $u$ and $v$, if we fix $u$ to a constant $c_1$ and move along $v = \pm 2 y \sqrt{y^2 + c_1}$ by incrementing $y$ we draw out two hyperbolas in the complex plane of $x$ and $y$.  This means that the function $f(z) = z^2$ takes points on hyperbolas the complex plane of $x$ and $y$ and translates them onto a vertical line in the complex plane of $u$ and $v$ where $u$ is a constant.
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
				\begin{axis}[
					width=8.9cm,
					unit vector ratio=1 1 1,
					xlabel={$x$},
					ylabel={$y$},
					xmin=-10, xmax=10,
					ymin=-10, ymax=10,
					axis lines = middle,
					xticklabels={}, yticklabels={},
					grid=none,
					major tick style={draw=none},
					minor tick style={draw=none},
					legend pos = south west,
					]
					
					\addplot [domain=2.23:10, samples=1000, color=blue,]
					{(x^2 - 5)^(0.5)};
					\addlegendentry{$x^2 - y^2 = c_1$}
					
					\addplot [domain=2.23:10, samples=1000, color=blue,]
					{-(x^2 - 5)^(0.5)};
					
					\addplot [domain=-10:-2.23, samples=1000, color=blue,]
					{(x^2 - 5)^(0.5)};
					
					\addplot [domain=-10:-2.23, samples=1000, color=blue,]
					{-(x^2 - 5)^(0.5)};
					
					\draw[->, draw=Blue] (axis cs:6.02,-5.6) -- (axis cs:5.5,-5);
					\draw[->, draw=Blue] (axis cs:-6.02,5.6) -- (axis cs:-5.5,5);
				\end{axis}
%				\path (current bounding box.north east) -- 
%				(current bounding box.south east) coordinate[midway] (2BL);
			\end{tikzpicture}
			{$\Large \xrightarrow{\mathmakebox[2cm] {f(z)=z^2}}$}
%			\hspace*{2.8cm}
			\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
				\begin{axis}[
					width=8.9cm,
					unit vector ratio=1 1 1,
					xlabel={$u$},
					ylabel={$v$},
					xmin=-10, xmax=10,
					ymin=-10, ymax=10,
					axis lines = middle,
					xticklabels={}, yticklabels={},
					grid=none,
					major tick style={draw=none},
					minor tick style={draw=none},
					legend pos = south west,
					]
					
					\addplot [mark=none, color=Blue] coordinates {(5, -10) (5, 10)};
					\addlegendentry{$u = c_1, \ v = \pm 2y \sqrt{y^2 + c_1}$}
					
					\draw[->, draw=Blue] (axis cs:5,-10) -- (axis cs:5,-5);
%					node[midway,above right,inner sep=2pt,font={\footnotesize}]{$\theta$};
				\end{axis}
%				\path (current bounding box.north west) -- 
%				(current bounding box.south west) coordinate[midway] (2BR);
			\end{tikzpicture}
%			\tikz[overlay,remember picture]{\draw[->, very thick] 
%				($(2BL)+(0.5, 0)$) -- ($(2BR)+(-0.5,0)$)
%				node[midway,above,text width=2.5cm]{$f(z) = z^2$};}
		\end{figure}
		Likewise if we set $v = c_2$ where $c_2 \in \mathbb{R}_{>0}$, we get:
		\begin{align*}
			u &= x^2 - \frac{c_2^2}{4x^2} 
			& v &= 2xy = c_2
		\end{align*}
		Taking the limits: 
		\begin{align}
			\lim_{x\rightarrow 0^+} u &= -\infty 
				& \lim_{x \rightarrow \infty, x>0} u &= \infty \\
			\lim_{x\rightarrow -\infty, x<0} u &= \infty 
				& \lim_{x \rightarrow 0^-} u &= -\infty 
		\end{align}
		Equation 11.1 tells us as $x$ goes from $0$ to $\infty$, $u$ moves from $-\infty$ to $\infty$, which corresponds to the hyperbola in the first quadrant of the $x$$y$ complex plane. Similarly for equations 11.2.
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
				\begin{axis}[
					width=8.9cm,
					unit vector ratio=1 1 1,
					xlabel={$x$},
					ylabel={$y$},
					xmin=-10, xmax=10,
					ymin=-10, ymax=10,
					axis lines = middle,
					xticklabels={}, yticklabels={},
					grid=none,
					major tick style={draw=none},
					minor tick style={draw=none},
					legend pos = south west,
					]
					\addplot [domain=0.01:10, samples=50, color=Red,]
					{5/(2*x)};
					
					\addplot [domain=-10:-0.01, samples=50, color=Red,]
					{5/(2*x)};
					\addlegendentry{$2xy = c_2$};
					
					\draw[->, draw=Red] (axis cs:0.485,5.1) -- (axis cs:0.51,5);
					\draw[->, draw=Red] (axis cs:-0.485,-5.1) -- (axis cs:-0.51,-5);
				\end{axis}
			\end{tikzpicture}
			{$\Large \xrightarrow{\mathmakebox[2cm] {f(z)=z^2}}$}
			\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
				\begin{axis}[
					width=8.9cm,
					unit vector ratio=1 1 1,
					xlabel={$u$},
					ylabel={$v$},
					xmin=-10, xmax=10,
					ymin=-10, ymax=10,
					axis lines = middle,
					xticklabels={}, yticklabels={},
					grid=none,
					major tick style={draw=none},
					minor tick style={draw=none},
					legend pos = south west,
					]
					
					\addplot [domain=-10:10, samples=10, color=Red,]
					{5};
					\addlegendentry{$u = x^2 - c_2^2/(4x^2), \ v=c_2$}
					
					\draw[->, draw=Red] (axis cs:-10,5) -- (axis cs:-5,5);
					%					node[midway,above right,inner sep=2pt,font={\footnotesize}]{$\theta$};
				\end{axis}
			\end{tikzpicture}
		\end{figure}
		If we look at $f$ using the polar representation, we get $f(z) = r^2 e^{i2\theta}$. This tells us $\forall r \geq 0$, $r \mapsto r^2 = \rho \geq 0$, and $\forall \theta$, $\theta \mapsto \phi = 2\theta$. It is worth noting that mapping of points between $0 \leq 0 < 2\pi$ is not one-to-one, since points in $0 \leq \theta < \pi$ and points in $\pi \leq \theta < 2\pi$ both get mapped to $0 \leq \phi < 2\pi$. 
	\end{example}

	\section{Limits} \label{Limits Section - Complex}
	\begin{definition}[Limit]
		\label{Limit Definition - Complex}
		Let $z, z_0, w_0 \in \mathbb{C}$ and $f$ be a function. We say $f(z)$ has limit $w_0$ as $z$ approaches $z_0$ if: 
		\begin{align*}
			\forall \epsilon \exists \delta [0<\abs{z-z_0} < \delta \implies \abs{f(z) - w_0} < \epsilon]
		\end{align*} 
		We then denote: $\lim_{z \rightarrow z_0} f(z) = w_0$
	\end{definition}
	This tells us that $\lim_{z \rightarrow z_0} f(z) = w_0$ if some deleted neighbourhood $\abs{z-z_0} < \delta$ corresponds to a neighbourhood $\abs{f(z) - w_0} < \epsilon$. Note that the mapping of all points $z$ in $\abs{z-z_0} < \delta$ to $\abs{f(z) - w_0} < \epsilon$ need not be subjective. It just needs to be mapped less than distance $\epsilon$ from $w_0$.
	
	Note: \Cref{Limit Definition - Complex} allows us to verify if a limit exists, but it is not a method for determining a limit. 
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
			\begin{axis}[
				width=8cm,
				unit vector ratio=1 1 1,
				xlabel={$x$},
				ylabel={$y$},
				xmin=-10, xmax=10,
				ymin=-1, ymax=10,
				ticks = none,
				axis lines = middle,
				axis line style = Black,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=o,
				]
				coordinates {
					(8, 8) [$z_0$]
				};
				\draw[dashed, line width=1pt,Grey](axis cs: 8, 8) circle (2);
				\draw[-,draw=Black]  (axis cs: 8,8) to ["$\delta$"] (axis cs: 6,8);
			\end{axis}
			\path (current bounding box.north east) -- 
			(current bounding box.south east) coordinate[midway] (2BL);
		\end{tikzpicture}
		{$\Large \xrightarrow{\mathmakebox[2cm]f}$}
%		\hspace*{2.8cm}
		\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
			\begin{axis}[
				width=8cm,
				unit vector ratio=1 1 1,
				xlabel={$v$},
				ylabel={$u$},
				xmin=-10, xmax=10,
				ymin=-1, ymax=10,
				ticks = none,
				axis lines = middle,
				axis line style = Black,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(-4, 5) [$w_0$]
				};
				\draw[dashed, line width=1pt,Grey](axis cs: -4, 5) circle (3);
				\draw[-,draw=Black]  (axis cs: -4,5) to ["$\epsilon$"] (axis cs: -7,5);
			\end{axis}
			\path (current bounding box.north west) -- 
			(current bounding box.south west) coordinate[midway] (2BR);
		\end{tikzpicture}
%		\tikz[overlay,remember picture]{\draw[->, very thick] 
%			($(2BL)+(0.5, 0)$) -- ($(2BR)+(-0.5,0)$)
%			node[midway,above]{$f$};} 
		\label{Limit Definition Figure - Complex}
	\end{figure}
	
	\begin{theorem}[Uniqueness of Limits]
		\label{Uniqueness of Limits Theorem - Complex}
		Suppose the limit of $f$ at $z_0$ exists, then it is unique.
	\end{theorem}
	\begin{proof}
		Suppose two limits of $f$ at $z_0$ exists for contradiction.
		\begin{align*}
			&[\lim_{z \rightarrow z_0} f(z) = w_0] \land [\lim_{z \rightarrow z_0} f(z) = w_1] \\
			&\implies [0 < \abs{z - z_0} < \delta_0 \implies \abs{f(z) - w_0} < \epsilon] 
				\land [0 < \abs{z - z_0} < \delta_0 \implies \abs{f(z) - w_0} < \epsilon] \\
			\\
			&w_1 - w_0 = [f(z) - w_0] + [w_1 - f(z)]\\
			&\implies \abs{w_1 - w_0} = \abs{[f(z) - w_0] + [w_1 - f(z)]} 
				\leq \abs{f(z) - w_0} + \abs{f(z) - w_1} 
		\end{align*}
		Now choosing $\delta = \min\{\delta_1, \delta_2\}$, we get:
		$$\abs{w_1 - w_0} < \epsilon + \epsilon = 2 \epsilon$$
		Choosing $\epsilon$ to be arbitrary small, we end up with: 
		$$w_1 - w_0 = 0 \implies w_1 = w_0$$
	\end{proof}

	\Cref{Limit Definition - Complex} requires that $f$ be defined at all points in the deleted neighbourhood of $z_0$. That is, $z_0$ is interior to the region which $f$ is defined. We can extend the definition by agreeing that $0<\abs{z-z_0} < \delta \implies \abs{f(z) - w_0} < \epsilon$ also holds for $z$ that lie in the region where $f$ is defined and the deleted neighbourhood of $z_0$.  That is $f(z_0)$ need not be defined for a limit at $z_0$ to exist.
	
	\begin{example}
		Show $(f(z) = iz/2) \land (\abs{z} < 1) \implies \lim_{z\rightarrow 1} f(z) = i/2$.
		
		We can see that we have restricted the domain of $f$ to the region $\abs{z} < 1$, this puts $z = 1$ right at the boundary of the domain of definition of $f$.
		\begin{align*}
			\abs{z} < 1 \implies& \abs{f(z) - \frac{i}{2}} = \abs{\frac{iz}{2} - \frac{i}{2}} = \frac{\abs{z - 1}}{2} \\
			\implies& \forall z \forall \epsilon \exists \delta \left[ 0 < \abs{z - 1} < \delta = 2\epsilon \implies \abs{f(z) - \frac{i}{2}} < \epsilon \right] \\
			\implies& \lim_{z \rightarrow 1} f(z) = \frac{i}{2}
		\end{align*}
	\end{example}
	
	This highlights the fact that if the limit exists, then $z$ is allowed to approach $z_0$ from any arbitrary direction. 
	
	\begin{example}
		Limit of $f(z) = z /\bar{z}$ does not exist at $z = 0$
		
		Consider $\lim_{z \rightarrow 0} f(z)$. Let us approach the limit from the $x$-axis and the $y$-axis. 
		\begin{align*}
			\lim_{z = (x, 0) \rightarrow 0} f(z) &= \frac{x + i0}{x - i0} = 1
			& \lim_{z = (0, y) \rightarrow 0} f(z) &= \frac{0 + iy}{0 - iy} = -1
		\end{align*} 
		We end up with two different limits. As limits are unique, we conclude that $\lim_{z\rightarrow0} f(z)$ does not exist. 
	\end{example}

	\subsection{Limit Theorems} \label{Limit Theorems Subsection - Complex}
	
	\begin{theorem}
		\label{Limit of f=u+iv Theorem - Complex}
		Consider $f(z) = u(x, y) + iv(x,y)$. Let $z_0 = x_0 + iy_0$ and $w_0 = u_0 + iv_0$.
		\begin{align*}
			\left[\lim_{(x,y) \rightarrow (x_0, y_0)} u(x,y) = u_0 \right] \land
			\left[\lim_{(x,y) \rightarrow (x_0, y_0)} v(x,y) = v_0 \right]
			\iff
			\lim_{z \rightarrow z_0} f(z) = w_0
		\end{align*}
	\end{theorem}
	\begin{proof}
		\underline{$\implies$:}
		
		By definition: 
		\begin{align}
			\label{Limit of f=u+iv Theorem Proof Eqn 1 - Complex}
			&\left[\lim_{(x,y) \rightarrow (x_0, y_0)} u(x,y) = u_0 \right] \land
			 \left[\lim_{(x,y) \rightarrow (x_0, y_0)} v(x,y) = v_0 \right] \\ \nonumber
			&\implies \forall \epsilon \exists \delta_1, \delta_2
			 \left[\left(0< \sqrt{(x-x_0)^2 + (y-y_0)^2} < \delta_1 \implies \abs{u-u_0} < \frac{\epsilon}{2}\right) \right.\\ \nonumber
			&\qquad \qquad \qquad \land \left.
			 \left(0< \sqrt{(x-x_0)^2 + (y-y_0)^2} < \delta_2 \implies \abs{v-v_0} < \frac{\epsilon}{2}\right)
			 \right]
		\end{align}
		Triangle inequality for the distance between points:
		\begin{align*}
			\abs{(u+iv) - (u_0 - iv_0)} &= \abs{(u-u_0) + i(v-v_0)} \leq \abs{u-u_0} + \abs{v-v_0} \\
			\sqrt{(x-x_0)^2 + (y-y_0)^2} &= \abs{(x-x_0) + i(v-v_0)} = \abs{(x+iy) - (x_0 - iy_0)}
		\end{align*}
		Let $\delta = \min\{\delta_1, \delta_2\}$, it follows from 
		\cref{Limit of f=u+iv Theorem Proof Eqn 1 - Complex}: 
		$$0 < \abs{(x+iy) - (x_0 + iy_0)} < \delta 
			\implies \abs{(u+iv)-(u_0 - iv_0)} < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon$$
		Thus, $\lim_{z \rightarrow z_0} f(z) = w_0$.
		
		\underline{$\impliedby$:}
		
		Suppose $\lim_{z \rightarrow z_0} f(z) = w_0$.
		\begin{align}
			\label{Limit of f=u+iv Theorem Proof Eqn 2 - Complex}
			&\lim_{z \rightarrow z_0} f(z) = w_0 \\ \nonumber
			&\implies \forall \epsilon \exists \delta>0 [\abs{(x+iy)-(x_0-iy_0)}<\delta \implies \abs{(u+iv) - (u_0+iv_0)}<\epsilon]
		\end{align}
		By the triangle inequality: 
		\begin{align*}
			\abs{u-u_0} &\leq \abs{(u-u_0) + i(v-v_0)} = \abs{(u+iv) - (u_0 + iv_0)} \\
			\abs{v-v_0} &\leq \abs{(u-u_0) + i(v-v_0)} = \abs{(u+iv) - (u_0 + iv_0)} 
		\end{align*}
		$$\abs{(x+iy) - (x_0 + iy_0)} = \abs{(x-x_0) + i(y-y_0)} = \sqrt{(x-x_0)^2 + (y-y_0)^2}$$
		Thus, it follows from the inequalities in \cref{Limit of f=u+iv Theorem Proof Eqn 2 - Complex}:
		\begin{align*}
			&0 < \sqrt{(x-x_0)^2 + (y-y_0)^2} < \delta  \\
			&\implies [\abs{u-u_0} < \epsilon ]\land [\abs{v-v_0} < \epsilon] \\
			&\implies \left[\lim_{(x,y) \rightarrow (x_0, y_0)} u(x,y) = u_0 \right] \land
			\left[\lim_{(x,y) \rightarrow (x_0, y_0)} v(x,y) = v_0 \right]
		\end{align*}
	\end{proof}

	\begin{theorem}
		Suppose
		\begin{align*}
		\left[\lim_{z\rightarrow z_0} f(z) = w_0 \right] \land \left[\lim_{z\rightarrow z_0} F(z) = W_0 \right]
		\end{align*}
		Then
		\begin{align*}
%			\label{Limit of f(z)+F(z) Theorem Equation - Complex}
			\lim_{z \rightarrow z_0} [f(z) + F(z)] &= w_0 + W_0 & \\
%			\label{Limit of f(z)F(z) Theorem Equation - Complex} 
			\lim_{z \rightarrow z_0} [f(z)F(z)] &= w_0 W_0  & \\
%			\label{Limit of f(z)/F(z) Theorem Equation - Complex}
			\lim_{z \rightarrow z_0} \frac{f(z)}{F(z)} &= \frac{w_0}{W_0} & W_0 \neq 0
		\end{align*}
		\label{Limit of f(z)+F(z) f(z)F(z) and f(z)/F(z) Theorem - Complex}
	\end{theorem}
	\begin{proof}
		Let: 
		\begin{align*}
			f(z) &= u(x,y) + iv(x,y) & F(z) &= U(x,y) + iV(x,y) 
		\end{align*}
		\begin{align*}
			z_0 &= x_0 + iy_0 	& w_0 &= u_0 + iv_0 & W_0 &= U_0 + iV_0
		\end{align*}
		\underline{$\lim_{z \rightarrow z_0} [f(z) + F(z)] = w_0 + W_0$}
		
		\textcolor{Grey}{
		From \Cref{Limit of f=u+iv Theorem - Complex}:
		\begin{align*}
			&f(z) + F(z) = (u + U) + i(v + V) \\
			&\implies \lim_{(x,y) \rightarrow (x_0, y_0)} f(z)F(Z) = (u_0 + U_0) + i(v_0 + V_0) = w_0 + W_0
		\end{align*}
		}
		
		\underline{$\lim_{z \rightarrow z_0} [f(z)F(z)] = w_0 W_0$}
		
		From \Cref{Limit of f=u+iv Theorem - Complex}:
		\begin{align*}
			&f(z)F(z) = (uU - vV) + i(vU + uV) \\
			&\implies \lim_{(x,y) \rightarrow (x_0, y_0)} f(z)F(Z) = (u_0 U_0 - v_0 V_0) + i(v_0 U_0 + u_0 V_0) = w_0 W_0
		\end{align*}
		
		\underline{$\lim_{z \rightarrow z_0} \frac{f(z)}{F(z)} = \frac{w_0}{W_0}$ if $W_0 \neq 0$}
		
		\textcolor{Grey}{
		From \Cref{Limit of f=u+iv Theorem - Complex}:
		\begin{align*}
			&\frac{f(z)}{F(z)} = \frac{u+iv}{U+iV} 
			\implies \lim_{(x, y) \rightarrow (x_0, y_0)} \frac{f(z)}{F(z_0)} = \frac{u_0 + v_0}{U_0 + iV_0} = \frac{w_0}{W_0}
		\end{align*}
		}
	\end{proof}

	\begin{corollary}
		\label{Limits of f=u+iv Theorem Corollary - Complex}
		Let $c$ be a constant, $z, z_0 \in \mathbb{C}$, and $P(z)$ be a polynomial. Then
		\begin{align*}
			\lim_{z \rightarrow z_0} c &= c & \lim_{z \rightarrow z_0} z &= z_0 &
			\lim_{z \rightarrow z_0} z^n &= z_0^n & n \in \mathbb{N}
		\end{align*}
		\begin{align*}
			\lim_{z \rightarrow z_0} P(z) = P(z_0)
		\end{align*}
	\end{corollary}

	\begin{observation}
		It is surprisingly quick that Brown and Churchill went from $\epsilon - \delta$ proofs straight to proving with limits. This is different to the approach in Sequences of Limits Theorem for Sequences Section by Kennith A. Ross. \cite{Ross.K-Elementary-Analysis-2013}. (\Cref{Limit Theorems Subsection - Real Analysis})
	\end{observation}
	\begin{question}
		It might be possible use a series approach to prove limit theorems for $z \in \mathbb{C}$ by having separate series for $x$ and $y$ (real and imaginary components of $z$), or a series in the form of $s_n = (x_n, y_n)$. Which would be the proper approach?
	\end{question}
	
	\subsection{Limits of Points at Infinity} \label{Limits of Points at Infinity Subsection - Complex}
	
	\begin{definition}[Extended Complex Plane]
		The complex plane union with the points at infinity: 
		$$\mathbb{C} \cup \{\pm \infty, \pm i \infty\}$$
	\end{definition}
	
	\begin{definition}[Riemann Sphere]
		A unit sphere centred at the origin of the complex plane, which is consequently bisected by the complex plane.
		\label{Riemann Sphere Definition - Complex}
	\end{definition}

	\begin{definition}[Stereographic Projection]
		Consider the Riemann Sphere. Let $N$ be the northern point of the sphere (the point on the sphere above the origin of the complex plane) and $z$ be any point in the complex plane. Let $l$ be a line that goes through $N$ and $z$, then $l$ will intersect the Riemann Sphere. Let $P$ be the point where $l$ intersects the Riemann Sphere. If we let $N$ correspond to the points at infinity, then there is a one-to-one correspondence between points on the sphere and the points on the extended complex plane. This correspondence is called the Stereographic Projection. (\Cref{Riemann Sphere and Stereographic Projection Figure - Complex})
		\label{Stereographic Projection Definition - Complex}
	\end{definition}

	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\begin{axis}[
%				title = Riemann Sphere with Stereographic Projection,
				axis equal,
				width=28cm,
				axis lines = center,
				xlabel = {$x$},
				ylabel = {$y$},
				zlabel = {$h$},
				xmin=-3, xmax=3,
				ymin=-3, ymax=3,
				zmin=-2, zmax=2,
				ticks = none,
				axis lines = middle,
				axis line style = Black,
%				enlargelimits=0.3,
				view/h=45,
				scale uniformly strategy=units only,
				]
				\addplot3[
				opacity = 0.2,
				surf,
				z buffer = sort,
				samples = 50,
				domain = 0:360,
				y domain = 0:180,
				]
				({cos(x)*sin(y)}, {sin(x)*sin(y)}, {cos(y)});
				
				\addplot3[
				samples = 50,
				domain = 0:360,
				y domain = 0:180,
				color = Black,
				]
				({cos(x)}, {sin(x)}, {0});
				
				\addplot3[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(1.5, 2, 0) [$z$]
					(0, 0, 1) [$N$]
					(0.413793, 0.551724, 0.724138) [$P$]
				};
				\draw[thick,draw=Black] (axis cs: 0,0,1)--(axis cs: 1.5,2,0);
				\draw[dashed,draw=Grey] (axis cs: 1.5,0,0)--(axis cs: 1.5,2,0);
				\draw[dashed,draw=Grey] (axis cs: 0,2,0)--(axis cs: 1.5,2,0);
				\draw[dashed,draw=Grey] (axis cs: (0.413793, 0, 0)--(axis cs: (0.413793, 0.551724, 0);
				\draw[dashed,draw=Grey] (axis cs: (0, 0.551724, 0)--(axis cs: (0.413793, 0.551724, 0);
				\draw[dashed,draw=Grey] (axis cs: (0.413793, 0.551724, 0)--(axis cs: (0.413793, 0.551724, 0.724138);
			\end{axis}
		\end{tikzpicture}
	    \caption{Riemann Sphere and Stereographic Projection}
		\label{Riemann Sphere and Stereographic Projection Figure - Complex}
	\end{figure}
	
	The region outside the unit circle enveloped by the Riemann sphere corresponds to the upper hemisphere of the Riemann sphere, with the point $N$ deleted. $N$ corresponds to the points at infinity, since $l$ will be parallel to the complex plane. 
	
	Note: In some texts, the  Riemann Sphere is a sphere of unit diameter (not a unit sphere, which is of unit radius) sitting on top of the Complex Plane. That is, with the south pole sitting at \((0,0)\). The definitions for line \(L\), and points \(N\), \(P\), and \(z\) remains the same. In either case, the Stereographic Projection maps to a unique point \(P\) on the sphere, and the definition of the point at infinity remains unchanged.
	
	\begin{definition}[Neighbourhood of $\infty$]
		The set: $\{\abs{z} > 1/\epsilon : \epsilon \in \mathbb{R}_{>0} \}$
	\end{definition}
	Note that since $\epsilon$ is a small positive number, $\abs{z} > 1/\epsilon$ corresponds to points far away from the unit circle, hence $P$ is close to $N$. 
	
	\textcolor{red}{Note: When referring to any point $z$, it is referring to a point in the finite plane. Points at infinity will be specifically mentioned.}
	
	\begin{definition}[Limit at Infinity]
		\label{Limit at Infinity Definition - Complex}
		Let \(f(z)\) be a function, and \(z, z_0 \in \mathbb{C}\).
		\begin{align*}
			\forall \epsilon \in \mathbb{R}_{>0}, \exists r \in \mathbb{R}_{>0}[\abs{z} > r \implies \abs{f(z)-z_0}<\epsilon] \iff \lim_{z\rightarrow \infty} f(z) = z_0
		\end{align*}
		That is, if \(\forall z\) in the neighbourhood of infinity implies \(\abs{f(z) - z_0} < \epsilon\), then \(\lim_{z \rightarrow \infty} f(z) = z_0\).
	\end{definition}
	
	\begin{theorem}
		\label{Properties of Limits at Infinity Theorem - Complex}
		Let $z_0, w_0 \in \mathbb{C}$, then
		\begin{align*}
			\lim_{z \rightarrow z_0} \frac{1}{f(z)} = 0
			&\implies \lim_{z \rightarrow z_0} f(z) = \infty \\
			\lim_{z \rightarrow 0} f\left(\frac{1}{z}\right) = w_0
			&\implies \lim_{z \rightarrow \infty} f(z) = w_0 \\
			\lim_{z \rightarrow 0} \frac{1}{f(1/z)} = 0 
			&\implies \lim_{z \rightarrow \infty} f(z) = \infty
		\end{align*}
	\end{theorem}
	\begin{proof}
		\underline{$\lim_{z \rightarrow z_0} \frac{1}{f(z)} = 0 
		\implies \lim_{z \rightarrow z_0} f(z) = \infty$}
		\begin{align*}
			\lim_{z \rightarrow z_0} \frac{1}{f(z)} = 0 
			&\implies \forall \epsilon \exists \delta>0 \left[\abs{z-z_0} < \delta \implies \abs{\frac{1}{f{z}} - 0} < \epsilon \right] \\
			&\implies \forall \epsilon \exists \delta>0 \left[\abs{z-z_0} < \delta \implies \abs{f(z)} > \frac{1}{\epsilon} \right]  \\
			&\implies \lim_{z \rightarrow z_0} f(z) = \infty 
		\end{align*}
	
		\underline{$\lim_{z \rightarrow 0} f\left(\frac{1}{z}\right) = w_0
		\implies \lim_{z \rightarrow \infty} f(z) = w_0$}
		\begin{align*}
			\lim_{z \rightarrow 0} f\left(\frac{1}{z}\right) = w_0
			&\implies \forall \epsilon \exists \delta>0 \left[ \abs{z-0} < \delta \implies \abs{f\left(\frac{1}{z}\right) - w_0} < \epsilon \right] \\
			&\implies \forall \epsilon \exists \delta>0 \left[ \abs{z} > \frac{1}{\delta} \implies \abs{f(z) - w_0} < \epsilon \right] \\
			&\implies \lim_{z \rightarrow \infty} f(z) = w_0
		\end{align*}
	
		\underline{$\lim_{z \rightarrow 0} \frac{1}{f(1/z)} = 0 
		\implies \lim_{z \rightarrow \infty} f(z) = \infty$}
		\begin{align*}
			\lim_{z \rightarrow 0} \frac{1}{f(1/z)} = 0 
			&\implies \forall \epsilon \exists \delta > 0 \left[ \abs{z - 0} < \delta \implies \abs{\frac{1}{f(1/z)} - 0} < \epsilon \right] \\
			&\implies \forall \epsilon \exists \delta > 0 \left[ \abs{z} > \frac{1}{\delta} \implies \abs{f(z)} > \frac{1}{\epsilon} \right] \\
			&\implies \lim_{z \rightarrow \infty} f(z) = \infty 
		\end{align*}
	\end{proof}
	Note: As $\delta$ goes to $0$, $1/\delta$ goes to $\infty$, hence $\abs{z}$ goes to $\infty$ if $\abs{z} > 1/\delta$. 
	\begin{observation}
		As expected, \cref{Properties of Limits at Infinity Theorem - Complex} is consistent if $z \in \mathbb{R}$. (Check: \Cref{Limit Theorems Subsection - Real Analysis}).
	\end{observation}
	
	\section{Continuity} \label{Continuinty Section - Complex}
	\begin{definition}[Continuous]
		\label{Continuous Function Definition - Complex}
		Let $f$ be a function. We say $f$ is continuous at all point $z_0 \in \mathbb{C}$ if it satisfies the following: 
		\begin{align*}
			\lim_{z \rightarrow z_0} f(z) \text{ exists } \land f(z_0) \text{ exists } \land
			\lim_{z\rightarrow z_0} f(z) &= f(z_0)
		\end{align*} 
	\end{definition}
	Note: 
	\begin{align*}
		&\lim_{z \rightarrow z_0} f(z) = f(z_0) \implies \lim_{z \rightarrow z_0} f(z) \text{ exists } \land f(z_0) \text{ exists } \\
		&\forall \epsilon \exists \delta>0 \left[\abs{z-z_0} < \delta \implies \abs{f(z) - f(z_0)} < \epsilon \right] \iff \lim_{z \rightarrow z_0} f(z) = f(z_0)
	\end{align*}
	
	\begin{definition}[Continuous at a Region]
		Let $f$ be a function, $R \subset \mathbb{C}$ be a region, and $z \in R$: 
		\begin{center}
			$f$ is continuous in $R \iff$ $\forall z \in R$($f$ is continuous)
		\end{center}
	\end{definition}
	
	\begin{theorem}
		Let $f(z)$ and $g(z)$ be continuous functions at $z_0 \in \mathbb{C}$. Then the following are also continuous at $z_0$:
		\begin{align*}
			f(z_0) + g(z_0) \qquad f(z_0)g(z_0) \qquad \frac{f(z_0)}{g(z_0)} \qquad g(z_0) \neq 0
		\end{align*}
		\label{Continuity of f(z)+g(z) f(z)g(z) and f(z)/g(z) Theorem - Complex}
	\end{theorem}
	\begin{proof}
		Consequence of \cref{Limit of f(z)+F(z) f(z)F(z) and f(z)/F(z) Theorem - Complex}.
	\end{proof}

	\begin{corollary}
		Let $P(z)$ be a polynomial, then $P(z)$ is continuous $\forall z \in \mathbb{C}$. That is $P(z)$ is continuous in the entire plane of $\mathbb{C}$.
		\label{Continuity of f(z)+g(z) f(z)g(z) and f(z)/g(z) Corollary - Complex}
	\end{corollary}
	\begin{proof}
		Consequence of \cref{Limits of f=u+iv Theorem Corollary - Complex}.
	\end{proof}
	
	\begin{observation}
		Both \cref{Continuity of f(z)+g(z) f(z)g(z) and f(z)/g(z) Theorem - Complex} and \cref{Continuity of f(z)+g(z) f(z)g(z) and f(z)/g(z) Corollary - Complex} rely on \cref{Continuous Function Definition - Complex}, which state for a function $f$ and point $z_0 \in \mathbb{C}$:
		$$\lim_{z \rightarrow z_0} f(z) \text{ exists } \implies f(z) \text{ is continuous at } z_0$$
		This is why the proofs cite the results of \cref{Limit of f(z)+F(z) f(z)F(z) and f(z)/F(z) Theorem - Complex} and \cref{Limits of f=u+iv Theorem Corollary - Complex}.
	\end{observation}

	\begin{theorem}
		Let $f(z)$ and $g(z)$ be functions.
		$$f(z) \text{ and } g(z) \text{ continuous} \implies g(f(z)) \text{ continuous}$$
		\label{Continuity of Composition of Functions Theorem - Complex}
	\end{theorem}
	\begin{proof}
		Let $f(z) = w$ be defined in the neighbourhood $\forall z[\abs{z-z-0}<\delta]$, and $g(w) = W$ where $\operatorname{dom} (g) = \operatorname{img}(f)$. Suppose that $f$ is continuous at $z_0$ and $g$ is continuous at $f(z_0)$.
		\begin{align*}
			f \text{ continuous at } z_0 
			&\iff \forall \gamma \exists \delta>0 \left[ \abs{z-z_0} < \delta \implies \abs{f(z) - f(z_0)}<\gamma \right] \\
			&\implies \forall \epsilon \exists \gamma>0 \left[ \abs{f(z) - f(z_0)} < \gamma \implies \abs{g(f(z)) - g(f(z_0))} < \epsilon \right]
		\end{align*} 
		We can always find a small enough $\delta$ for $\gamma$ to satisfy $\abs{g(f(z)) - g(f(z_0))} < \epsilon$.
	\end{proof}

	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
			\begin{axis}[
				width=6cm,
				unit vector ratio=1 1 1,
				xlabel={$x$},
				ylabel={$y$},
				xmin=-1, xmax=10,
				ymin=-1, ymax=10,
				ticks = none,
				axis lines = middle,
				axis line style = Black,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(8, 8) [$z_0$]
				};
				\draw[dashed, line width=1pt,Grey](axis cs: 8, 8) circle (2);
				\draw[-,draw=Black]  (axis cs: 8,8) to ["$\delta$"] (axis cs: 6,8);
			\end{axis}
		\end{tikzpicture}
		{$\Large \xrightarrow{\mathmakebox[1cm]f}$}
		\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
			\begin{axis}[
				width=6cm,
				unit vector ratio=1 1 1,
				xlabel={$v$},
				ylabel={$u$},
				xmin=-10, xmax=2,
				ymin=-1, ymax=10,
				ticks = none,
				axis lines = middle,
				axis line style = Black,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(-4, 2) [$w_0$]
				};
				\draw[dashed, line width=1pt,Grey](axis cs: -4, 2) circle (3);
				\draw[-,draw=Black]  (axis cs: -4,2) to ["$\gamma$"] (axis cs: -7,2);
			\end{axis}
		\end{tikzpicture}
		{$\Large \xrightarrow{\mathmakebox[1cm]{g(f)}}$}
		\begin{tikzpicture}[remember picture, baseline=(current bounding box.center)]
			\begin{axis}[
				width=6cm,
				unit vector ratio=1 1 1,
				xlabel={$V$},
				ylabel={$U$},
				xmin=-1, xmax=10,
				ymin=-1, ymax=10,
				ticks = none,
				axis lines = middle,
				axis line style = Black,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(5, 5) [$w_0$]
				};
				\draw[dashed, line width=1pt,Grey](axis cs: 5, 5) circle (4);
				\draw[-,draw=Black]  (axis cs: 5,5) to ["$\epsilon$"] (axis cs: 1,5);
			\end{axis}
		\end{tikzpicture}
		\label{Limit Composition of Functions Figure - Complex}
	\end{figure}
	
	\begin{theorem}
		\label{f(z) neq 0 Neighbourhood Theorem - Complex}
		Let $f(z)$ be a function and $f(z_0) \neq 0$.
		\begin{align*}
			f(z_0) \neq 0 
			\implies \exists \epsilon \forall z [\abs{f(z) - f(z_0)}<\epsilon \implies f(z) \neq 0]
		\end{align*}
		That is, if $f(z_0) \neq 0$ then it has a neighbourhood where $f(z) \neq 0$.
	\end{theorem}
	\begin{proof}
		Suppose $f(z)$ is continuous and non-zero at $z_0$, and let $\epsilon = \abs{f(z_0)}/2$:
		\begin{align*}
			&\exists z [f(z) = 0]  \land 
			\forall \epsilon \exists \delta>0 \left[ \abs{z-z_0}<\delta \implies \abs{f(z) - f(z_0)}  < \frac{\abs{f(z_0)}}{2} \right] & \\
			&\implies \abs{f(z_0)} < \frac{\abs{f(z_0)}}{2} & \text{Contradiction!}
		\end{align*}
	\end{proof}

	\begin{theorem}
		Let $f(z) = u(x,y) + iv(x,y)$ be a function, and $z = x + iy$, $z \in \mathbb{C}$. 
		\begin{align*}
			f \text{ continuous at } z_0 \iff \left[ u \text{ continuous at } z_0 \right] \land \left[ v \text{ continuous at } z_0 \right]
		\end{align*}
		\label{Continuity of f=u+iv Theorem - Complex}
	\end{theorem}
	\begin{proof}
		Direct consequence of \cref{Limit of f=u+iv Theorem - Complex}
	\end{proof}
	
	\begin{theorem}
		\label{Function continuous in closed and bounded region implies function is bounded Theorem - Complex}
		Let $f$ be continuous in a closed and bounded region $R$, then
		\begin{align*}
			\forall z \in R, \exists M \in \mathbb{R}_{>0}  \left[\abs{f(z) \leq M}\right] 
			\land \abs{\{z : f(z) = M \}} \geq 1
		\end{align*}
		That is, for $\forall z \in R$, $\abs{f(z)} \leq M$ and there is at least one point $z$ where $\abs{f(z)}= M$. $f(z)$ is bounded in $R$.
	\end{theorem}
	\begin{proof}
		Let $f(z) = u(x,y) + iv(x,y)$ be continuous, then 
		\begin{align*}
			\abs{f(z)} = \sqrt{[u(x,y)]^2 + [v(x,y)]^2} \text{ is continuous in } R 
			\implies \exists M \in \mathbb{R}_{>0} [\abs{f(z)}\leq M]
		\end{align*}
	\end{proof}

	\subsection{Exercises}
	
	\begin{example}
		Prove: $$ \lim_{z \rightarrow z_0} f(z) = w_0 \implies \lim_{z \rightarrow z_0} \abs{f(z)} = \abs{w_0}$$
		Note: $\abs{\abs{f(z_0)} - \abs{w_0}} \leq \abs{f(z) - w_0}$
	\end{example}
	\begin{proof}
		\textcolor{Grey}{Use definition of limit, then plug and chug.}
	\end{proof}
	
	\begin{example}
		Prove: Limits involving points at infinity are unique.
	\end{example}
	\begin{proof}
		\textcolor{Grey}{
		Suppose that limit of the point at infinity is not unique, that is there is two neighbourhoods of infinity. Using he definition of the limit, we will arrive at a contradiction where the two neighbourhoods are the same. 
		}
	\end{proof}
	
	\begin{example}
		Prove:
		\begin{align*}
			S \text{ is unbounded } \iff \forall \epsilon \exists z \left[ z \in S : \abs{z} > \frac{1}{\epsilon} \right]
		\end{align*}
		That is, $S$ is unbounded $\iff$ every neighbourhood of the point at infinity contains at least one point in $S$
	\end{example}
	\begin{proof}
		\textcolor{Grey}{
		Proof Sketch: 
		Recall the Riemann Sphere. (\Cref{Riemann Sphere Definition - Complex}). The set $\abs{z} > 1/\epsilon$ corresponds to the points close to $N$, which is the neighbourhood of the point at infinity. If we let $\gamma = 2\epsilon$, $\exists z$ where $\abs{z} > 1/\gamma$ holds. This along with $z \in \mathbb{C}$ (which is $S$ in our case), implies the direction $\impliedby$ is true. That is, we can still find elements in $S$ as we shrink the circle around $N$. 
		}
		
		\textcolor{Grey}{
		$S$ is unbounded implies that for all circle with radius $R$ centred at the origin there is at least one element of $s \in S$ where $\abs{s} > R$. Suppose for contradiction that there is a neighbourhood of the point at infinity that does not contain any points in $S$. We will arrive at a contradiction, where there is $M \in \mathbb{R}_{>0}$ such that $\forall s \in S [\abs{s} < M]$. Thus $S$ is bounded, a contradiction. This implies that the direction $\implies$ is true. 
		}
	\end{proof}

	\section{Differentiation} \label{Differentiation - Complex}
	
	\begin{definition}[Derivative]
		\label{Derivative Definition - Complex}
		Let $f$ be a function where $\abs{z - z_0} < \epsilon$ and $z \in \operatorname{dom}(f)$. Then the derivative of $f$ at point $z_0$:
		\begin{align*}
			f'(z_0) = \lim_{z \rightarrow z_0} \frac{f(z) - f(z_0)}{z - z_0}
		\end{align*}
	\end{definition}

	\begin{definition}[Differentiable]
		\label{Differentiable Definition - Complex}
		A function $f$ is differentiable at $z_0 \in \mathbb{C}$ if $f'(z_0)$ exists.
	\end{definition}

	If we let $\Delta z = z - z_0$ where $z \neq z_0$: 
	\begin{align*}
		f'(z_0) = \lim_{\Delta z \rightarrow 0} \frac{f(z_0 + \Delta z) - f(z_0)}{\Delta z}
	\end{align*}
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[remember picture]
			\begin{axis}[
				small,
				unit vector ratio=1 1 1,
				xlabel={$\Re$},
				ylabel={$\Im$},
				xmin=0, xmax=20,
				ymin=0, ymax=16,
				%					xtick distance=2,
				%					ytick distance=2,
				xtick=100,
				ytick=100,
				axis lines = middle,
				axis line style = Black,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(12, 10) [$z_0$]
					(14, 7)
				};
				\draw[dashed,line width=1pt,Grey](axis cs: 12, 10) circle (5);
				\draw[-,draw=Grey]  (axis cs: 12,10)--(axis cs: 7,10)
				node[midway,above] {$\epsilon$};
				\draw[->,draw=Black]  (axis cs: 12,10) to ["$\Delta z$"] (axis cs: 14,7);
				\draw[->, thick, draw=Black]  (axis cs: 0,0)--(axis cs: 12,10)
				node[midway,sloped,above] {$z_0$};
				\draw[->, thick, draw=Black]  (axis cs: 0,0)--(axis cs: 14,7) node[midway,sloped,below]  {$z_0 + \Delta z$};
			\end{axis}
		\end{tikzpicture}
	\end{figure}
	
	There's another notation by letting $\Delta w = f(z + \Delta z) - f(z)$:
	\begin{align*}
		f'(z) = \dv{w}{z}  = \lim_{\Delta z \rightarrow 0} \frac{\Delta w}{\Delta z}
	\end{align*}
	\begin{observation}
		The definition of a derivative in \cref{Derivative Definition - Complex} looks similar to that of a derivative for the real numbers:
		\begin{align*}
			F'(x_0) = \lim_{x \rightarrow x_0} \frac{f(x) - f(x_0)}{x - x_0}
		\end{align*}
		However, the existence of $f'(z)$ possesses a much stronger requirement than the existence of $F'(z)$. That is, let $f(z) = u(x,y) + iv(x,y)$. The existence of $f'(z)$ at point $z_0$ requires the existence of both $u'(x,y)$ and $v'(x,y)$.
		\begin{align*}
			f'(z_0) &= \lim_{(x,y) \rightarrow (x_0, y_0)} \frac{f(z) - f(z_0)}{z - z_0} = \lim_{(x,y) \rightarrow (x_0, y_0)} \frac{u(z) - u(z_0)}{z - z_0} + i \frac{v(z) - v(z_0)}{z - z_0}
		\end{align*}
		and that 
		\begin{align*}
			\lim_{(x,y_0) \rightarrow (x_0, y_0)} \frac{u(x, y_0) - u(x_0, y_0)}{x - x_0} + i \frac{v(x_0, y_0) - v(x_0, y_0)}{x-x_0} \\ 
			= 
			\lim_{(x_0,y) \rightarrow (x_0, y_0)} \frac{u(x_0, y) - u(x_0, y_0)}{x - x_0} + i \frac{v(x_0,y) - v(x_0,y_0)}{x-x_0}
		\end{align*}
		That is
		\begin{align*}
			\lim_{(\Delta x, 0) \rightarrow (0,0)} \frac{u(x_0+\Delta x, y_0) - u(x_0,y_0)}{\Delta x}
			&= \lim_{(0, \Delta y) \rightarrow (0,0)} \frac{u(x_0, y_0 +\Delta y) - u(x_0,y_0)}{\Delta y} \\
			\lim_{(\Delta x, 0) \rightarrow (0,0)} \frac{v(x+\Delta x, y_0) - v(x_0,y_0)}{\Delta x}
			&= \lim_{(0, \Delta y) \rightarrow (0,0)} \frac{v(x_0, y+\Delta y) - v(x_0,y_0)}{\Delta y}
		\end{align*}
		This tells us that the existence of a derivative for a real valued function $F(x)$ does not imply the existence of a derivative for a similar function $f(z)$ in the complex plane, which we will see later. (i.e. Take $f(z) = \abs{z}^2$ and $F(x) = \abs{x}^2$.) We are dealing with a two-dimensional limit instead of a one dimensional limit.
	\end{observation}

	\begin{question}
		Under what conditions will differentiability in\(\mathbb{C}\) imply differentiability in \(\mathbb{R}\), and vice versa?
	\end{question}
	
	\begin{example}
		Let $f(z) = \bar{z}$:
		\begin{align*}
			\frac{\Delta w}{\Delta z} = \frac{\overline{z + \Delta z} - \bar{z}}{\Delta z} = \frac{\bar{z} + \overline{\Delta z} - \bar{z}}{\Delta z} = \frac{\overline{\Delta z}}{\Delta z}
		\end{align*}
	Consider $\Delta z = (\Delta x, \Delta y) \rightarrow (0,0)$. If we move on the real axis, that is $(\Delta x, 0)$:
	\begin{align*}
		\overline{\Delta z} = \overline{\Delta x + i0} = \Delta x - i0  = \Delta x + i0 = \Delta z 
		\implies \frac{\Delta w}{\Delta z} = \frac{\overline{\Delta z}}{\Delta z} = \frac{\Delta z}{\Delta z} = 1
	\end{align*}
	If we move on the imaginary axis, that is $(0, \Delta y)$:
	\begin{align*}
		\overline{\Delta z} = \overline{0 + i\Delta y} = 0 - i \Delta y = - \Delta z
		\implies \frac{\Delta w}{\Delta z} = \frac{\overline{\Delta z}}{\Delta y} = \frac{-\Delta z}{\Delta z} =  -1
	\end{align*}
	Limits are unique, so the limit of $\dd w / \dd z$ does not exist anywhere. 
	\end{example}

	\begin{example}
		Consider $f(z) = \abs{z}^2$:
		\begin{align*}
			\frac{\Delta w}{\Delta z} &= \frac{\abs{z + \Delta z}^2 - \abs{z}^2}{\Delta z}
				= \frac{(z + \Delta z)(\overline{z + \Delta z}) - z \bar{z}}{\Delta z} \\
				&= \frac{(z + \Delta z)(\bar{z} + \overline{\Delta z}) - z \bar{z}}{\Delta z} 
				= \frac{z\bar{z} + \Delta z \bar{z} + \overline{\Delta z} z + \overline{\Delta z} \Delta z - z \bar{z}}{\Delta z}
				= \bar{z} + \overline{\Delta z} + z\frac{\overline{\Delta z}}{\Delta z}
		\end{align*}
		As in the previous example, as $(\Delta x, \Delta y) \rightarrow (0,0)$:
		\begin{align*}
			\overline{\Delta z} &= \Delta z  & &\text{From the real axis} \\
			\overline{\Delta z} &= -\Delta z & &\text{From the imaginary axis}
		\end{align*}
		Thus
		\begin{align*}
			\frac{\Delta w}{\Delta z} &= \bar{z} + \Delta z + z & \Delta z &= (\Delta x, 0) \\
			\frac{\Delta w}{\Delta z} &= \bar{z} - \Delta z - z & \Delta z &= (0, \Delta y)
		\end{align*}
		Therefore, by uniqueness of limits as $\Delta z \rightarrow 0$:
		\begin{align*}
			\lim_{\Delta z \rightarrow 0} (\bar{z} + \Delta z + z) = \lim_{\Delta z \rightarrow 0} (\bar{z} - \Delta z - z )
			\implies  z = - z   \implies z = 0
		\end{align*}
		Hence, $\dd w /\dd z$ does not exist for $z \neq 0$.
		We can also see that:
		\begin{align*}
			\frac{\Delta w}{\Delta z}
			&= \bar{z} + \overline{\Delta z} + z\frac{\overline{\Delta z}}{\Delta z}
			= \overline{\Delta z}  & z = 0
		\end{align*}
		Thus, $\dd w /\dd z $ only exists at $z = 0$: 
		\[ \eval{\dv{w}{z}}_{z=0} = 0 \]
	\end{example}
	
	\begin{remark}
		The following are facts:
		\begin{itemize}
			\item[(1)] A function $f(z)$ can be differentiable at a point $z_0$, but nowhere else in the neighbourhood of $z_0$.
			\item[(2)] \(f(z) = \abs{z}^2 \implies u(x,y) = x^2 + y^2 \land v(x,y) = 0 \), hence $u(x,y)$ and $v(x,y)$ can have continuous partial derivatives of all orders at a point $z_0$, even though $f$ may not be differentiable at $z_0$.
			\item[(3)]
			\( f(z) \text{ differentiable at } z_0 \implies f(z) \text{ continuous at } z_0 \)
			\begin{proof}
				Assume \( f'(z_0) \) exists: 
				\begin{align*}
					&\lim_{z \rightarrow z_0} [f(z) - f(z_0)] = \lim_{z \rightarrow z_0} \frac{f(z) - f(z_0)}{z - z_0} \lim_{z \rightarrow z_0} (z - z_0) = f'(z_0) \cdot 0 = 0\\
					&\implies \lim_{z \rightarrow z_0} f(z) = f(z_0)
				\end{align*}
			So, $f$ is differentiable at \(z_0 \implies f \) is continuous at \(z_0\).
			\end{proof}
			Note: Continuity of a function  at \(z_0 \in \mathbb{C}\) $\nRightarrow$ existence of derivative at point \(z_0\). 
			
			Ex: \(f(z) = \abs{z}^2 \) is continuous everywhere in $\mathbb{C}$ for \(z_0 \neq 0\), but \(f(z_0) \) does not exist at \(z_0\). 
		\end{itemize}
	\end{remark}

	\subsection{Differentiation Rules} \label{Differentiation Rules Subsection - Complex}
	Definition of derivative in \(\mathbb{C}\) (\cref{Derivative Definition - Complex}) is the same of that in \(\mathbb{R}\), so rules remain the same.
	
	Let \(c \in \mathbb{C}\) be a constant and functions \(f\) and \(g\) be differentiable at point \(z\). Then
	\begin{align*}
		\dv{z} c &= 0 & \dv{z} z &= 1 & \dv{z} [cf(z)] &= cf'(z) & \dv{z} z^n &= nz^{n-1} & n \in \mathbb{Z}\setminus\{0\}
	\end{align*}
	Let functions \(f\) and \(g\) be differentiable at point \(z\). Then
	\begin{align*}
		\dv{z} [f(z) + g(z)] &= f'(z) + g'(z) & \dv{z}[f(z)g(z)] &= f(z)g'(z) + f'(z)g(z) 
	\end{align*}
	\begin{align*}
		\dv{z}\left[\frac{f(z)}{g(z)}\right] = \frac{g(z)f'(z) - f(z)g'(z)}{[g(z)]^2}
	\end{align*}
	
	\begin{proof}
		Deriving: \( \dv{z}[f(z)g(z)] = f(z)g'(z) + f'(z)g(z) \)
		
		Let \(w = f(z) g(z)\): 
		\begin{align*}
			\Delta w &= f(z + \Delta z) g(z + \Delta z ) - f(z) g(z)\\
				&=f(z)[g(z + \Delta z) - g(z)] + [f(z + \Delta z) - f(z)] g(z + \Delta z)
		\end{align*}
		Thus
		\begin{align*}
			\frac{\Delta w}{\Delta z}
			= f(z)\frac{g(z + \Delta z) - g(z)}{\Delta z} + \frac{f(z + \Delta z) - f(z)}{\Delta z} g(z + \Delta z)
		\end{align*}
		Hence
		\begin{align*}
			\dv{w}{z} =	\lim_{\Delta z \rightarrow 0} \frac{\Delta w}{\Delta z} = f(z)g'(z) + f'(z)g(z)
		\end{align*}
	\end{proof}
	
	\begin{theorem}[Chain Rule for Composite Functions]
		\label{Chain Rule for Composite Functions Theorem - Complex}
		Let function $f$ be differentiable at $z_0$ and function $g$ be differentiable at \(f(z_0)\). Then \(F(z) = g[f(z)] \text{ is differentiable at } z_0\).
		\begin{align*}
			F'(z_0) = g'[f(z_0)]f'(z_0) 
		\end{align*}
	\end{theorem}
	\begin{proof}
		Suppose $f$ is differentiable at \(z_0\). Let \(w_0 = f(z_0)\) and assume that \(g'(w_0)\) exists. Then 
		\[ \forall w \exists \epsilon [\abs{w - w_0} < \epsilon \implies \Phi(w_0) = 0 ] \]
		Where 
		\begin{align*}
			\Phi(w) &= \frac{g(w) - g(w_0)}{w - w_0} - g'(w_0) & w \neq w_0
		\end{align*}
		Note: \(\lim_{w \rightarrow w_0} \Phi(w) = 0 \), so \(\Phi\) is continuous at \(w_0\).
		Then 
		\begin{align*}
			g(w) - g(w_0) &= [g'(w_0) + \Phi(w)](w - w_0)	&	 \abs{w-w_0}<\epsilon
		\end{align*}
		Note: This is valid for \(w = w_0\). 
		\begin{align*}
			f'(z_0) \text{ exists } &\implies f \text{ continuous at } z_0 \\
			&\implies \forall \epsilon \exists \delta>0 [ \abs{z - z_0} < \delta \implies \abs{w - w_0}< \epsilon]
		\end{align*}
		Hence, we can replace \(w\) by \(f(z)\) when \(\abs{z - z_0} < \delta\). Subbing \(w = f(z)\) and \(w_0 = f(z_0)\):
		\begin{align*}
			\frac{g[f(z)] - g[f(z_0)]}{z - z_0} &= \{g'[f(z_0)] + \Phi[f(z)]\} \frac{f(z) - f(z_0)}{z - z_0} 	& 0 < \abs{z-z_0} < \delta, \ z\neq z_0
		\end{align*}
		Then
		\begin{align*}
			(f \text{ continous at } z_0) \land (\Phi \text{ continuoust at } w_0 = f(z_0))
			\implies \Phi[f(z)] \text{ continuous at } z_0
		\end{align*}
		\[\Phi(w_0) = 0 \implies \lim_{z \rightarrow z_0} \Phi[f(z) = 0] \]
		Thus
		\begin{align*}
			\lim_{z \rightarrow z_0} \frac{g[f(z)] - g[f(z_0)]}{z - z_0} 
			&= \lim_{z \rightarrow z_0} \{g'[f(z_0)] + \Phi[f(z)]\} \frac{f(z) - f(z_0)}{z - z_0} \\
			&= g'[f(z_0)] f'(z_0)
		\end{align*}
		We then get
		\begin{align*}
			F'(z_0) = g'[f(z_0)]f'(z_0) 
		\end{align*}
	\end{proof}
	Alternatively, if we let \(w = f(z) \) and \(W = F(z)\), then the Chain Rule becomes:
	\[ \dv{W}{z} = \dv{W}{w} \dv{w}{z }\]
	Note: Although this looks like a fraction, it is not a fraction and should not be treated as such! (Logical inconsistency when infinitesimals when viewed as ratios.)
	
	\begin{theorem}[L'Hopital's Rule]
		\label{L'Hopital's Rule - Complex}
		Suppose \(f(z_0) = 0\) and \(g(z_0) = 0\), \(f'(z_0)\) and \(g(z_0)\) exists, with \(g'(z_0) \neq 0\). Then 
		\begin{align*}
			\lim_{z \rightarrow z_0} \frac{f(z)}{g(z)} = \frac{f'(z_0)}{g'(z_0)}
		\end{align*}
	\end{theorem}
	\begin{proof}
		Let \(f(z_0) = 0\), \(g(z_0) = 0\), and \(z \neq z_0\).
		\begin{align*}
			\lim_{z \rightarrow z_0} \frac{f(z)}{g(z)}
				&= \lim_{z \rightarrow z_0} \frac{\frac{f(z)-f(z_0)}{z-z_0}}{\frac{g(z)-g(z_0)}{z-z_0}}
				= \lim_{\Delta z \rightarrow 0}
				\frac{\frac{f(z_0 + \Delta z)-f(z_0)}{\Delta z}}{\frac{g(z_0 + \Delta z)-g(z_0)}{\Delta z}}
				=\frac{f'(z_0)}{g'(z_0)}
		\end{align*}
	\end{proof}
	
	
	\subsection{Exercises}
	
	\begin{example}
		Show that \(f'(z)\) does not exist for all points \(z \in \mathbb{C}\) when:
		\begin{itemize}
			\item[(a)] \(f(z) = \Re{z}\)
			\item[(b)] \(f(z) = \Im{z}\)
		\end{itemize}
		\begin{proof}
			{\color{Grey}
			Let \(f(z) = u(x,y) + iv(x,y)\), \(\Delta w = f(x+\Delta x, y + \Delta y) - f(x,y)\).
			
			\underline{\(f(z) = \Re{z}\)}
			
			Recall \(\Re{z} = x + i0\).
			\begin{align*}
				\frac{\Delta w}{\Delta z}
					&= \frac{\Re{z + \Delta z} - \Re{z}}{\Delta z} 
					 = \frac{x + \Delta x - x }{\Delta z} 
					 = \frac{\Delta x}{\Delta x + \Delta y}
			\end{align*}
			Now as \((\Delta x, 0) \rightarrow (0,0)\):
			\begin{align*}
				\lim_{(\Delta x, 0) \rightarrow (0,0)} \frac{\Delta w}{\Delta z} = \lim_{(\Delta x, 0) \rightarrow (0,0)} \frac{\Delta x}{\Delta x} = 1
			\end{align*} 
			Now as \((0, \Delta y) \rightarrow (0,0)\):
			\begin{align*}
				\lim_{(0, \Delta y) \rightarrow (0,0)} \frac{\Delta w}{\Delta z} = \lim_{(0,\Delta y) \rightarrow (0,0)} \frac{0}{\Delta y} = 0
			\end{align*}
			Limits are unique, but this isn't the case, so we conclude that \(f'(z)\) when \(f(z) = \Re{z}\) does not exist.
			
			\underline{\(f(z) = \Im{z}\)}
			
			Recall \(\Im{z} = 0 + iy\).
			\begin{align*}
				\frac{\Delta w}{\Delta z}
					&= \frac{\Im{z + \Delta z} - \Im{z} }{\Delta z}
					 = \frac{y + \Delta y - y}{\Delta z}
					 = \frac{\Delta y}{\Delta x + \Delta y}
			\end{align*}
			Now as \((\Delta x, 0) \rightarrow (0,0)\):
			\begin{align*}
				\lim_{(\Delta x, 0) \rightarrow (0,0)}  \frac{\Delta w}{\Delta z} = \lim_{(\Delta x, 0) \rightarrow (0,0)}  \frac{0}{\Delta x} = 0
			\end{align*} 
			Now as \((0, \Delta y) \rightarrow (0,0)\):
			\begin{align*}
				\lim_{(0,\Delta y) \rightarrow (0,0)} \frac{\Delta w}{\Delta z} = \lim_{(0,\Delta y) \rightarrow (0,0)} \frac{\Delta y}{\Delta y} = 1
			\end{align*}
			Limits are unique, but this isn't the case, so we conclude that \(f'(z)\) when \(f(z) = \Im{z}\) does not exist.
			}
		\end{proof}
	\end{example}

	
	\section{Cauchy-Riemann Equations} \label{Cauchy-Riemann Equations Section - Complex}
%	\subsection{Cartesian and Polar Forms of the Cauchy-Riemann Equations} \label{Cauchy-Riemann Equations (Cartesian and Polar) Subsection - Complex}
	\begin{theorem}[Cauchy-Riemann Equations (Cartesian)]
		\label{Cauchy-Riemann Equations (Cartesian) Theorem - Complex}
		Let \( f(z) = u(x, y) + iv(x,y) \).
		If \(f'(z)\) exists at a point \(z_0 = x_0 + iy_0\), then \(u'(x_0, y_0)\) and \(v'(x_0, y_0)\) exists and satisfy Cauchy-Riemann equations:
		\begin{align*}
			u_x &= v_y	&	u_y = -v_x
		\end{align*}
		Also, as a result of evaluating \(f'(z)\) from the horizontal and vertical direction:
		\begin{align*}
			f'(z_0) &= \eval{[u_x + iv_x]}_{(x_0, y_0)} 
					 = \eval{[v_y - i u_y]}_{(x_0, y_0)}
		\end{align*}
	\end{theorem}
	\begin{proof}
		Let \(f(z) = u(x,y) + iv(x,y)\), and suppose $f'(z)$ exists at $z_0$. Then 
		\begin{align*}
			z_0 &= x_0 + iy_0	&	\Delta z &= \Delta x + i\Delta y
			&	\Delta w &= f(z_0 + \Delta z) - f(z_0)
		\end{align*}
		So that
		\begin{align*}
			\Delta w = [u(x_0 + \Delta x, y_0 + \Delta y) + iv(x_0 + \Delta x, y_0 + \Delta y)] - [u(x_0, y_0) + iv(x_0, y_0)]
		\end{align*}
		Therefore
		\begin{align*}
			\frac{\Delta w}{\Delta z}
			&= \frac{u(x_0 + \Delta x, y_0 + \Delta y) - u(x_0, y_0)}{\Delta x + i\Delta y} + i\frac{v(x_0 + \Delta x, y_0 + \Delta y) - v(x_0, y_0)}{\Delta x + i\Delta y}
		\end{align*}
		Note: This equation remains valid as \((\Delta x, \Delta y) \rightarrow (0,0)\).
		
		\textbf{Horizontal Approach:}
		
		Let \((\Delta x, 0) \rightarrow (0,0)\) in the horizontal direction, then 
		\begin{align*}
			f'(z_0) 
			=& \lim_{\Delta x \rightarrow 0} \frac{u(x_0 + \Delta x, y_0) - u(x_0, y_0)}{\Delta x} + i \lim_{\Delta x \rightarrow 0} \frac{v(x_0 + \Delta x, y_0) - v(x_0, y_0)}{\Delta x} \\
			\implies& f'(z_0) = u_x(x_0, y_0) + iv_x(x_0, y_0)
		\end{align*}
		
		\textbf{Vertical Approach:}
		
		Let \((0, \Delta y) \rightarrow (0,0)\) in the vertical direction, then
		\begin{align*}
			f'(z_0) 
			=& \lim_{\Delta y \rightarrow 0} \frac{u(x_0, y_0 + \Delta y) - u(x_0, y_0)}{i \Delta y} + i \lim_{\Delta x \rightarrow 0} \frac{v(x_0, y_0+\Delta y) - v(x_0, y_0)}{i \Delta y} \\
			=& - i \lim_{\Delta y \rightarrow 0} \frac{u(x_0, y_0 + \Delta y) - u(x_0, y_0)}{ \Delta y} + \lim_{\Delta x \rightarrow 0} \frac{v(x_0, y_0+\Delta y) - v(x_0, y_0)}{\Delta y} \\
			\implies& f'(z_0) = v_y(x_0, y_0) - iu_y(x_0, y_0)
		\end{align*}
		
		\textbf{Putting it together:}
		
		For \(f'(z)\) to exists at \(z_0\), \(f(z_0)\) from the horizontal approach must equal that of the vertical approach. By equating the real and imaginary parts: 
		\begin{align*}
			&u_x(x_0, y_0) + iv_x(x_0, y_0) = v_y(x_0, y_0) - iu_y(x_0, y_0)\\
			&\implies (u_x = v_y) \land (u_y = - v_x)
		\end{align*}
		
	\end{proof}
	
	\begin{theorem}[Cauchy-Riemann Equations (Polar)]
		\label{Cauchy-Riemann Equations (Polar) Theorem - Complex}
		Let \(f(z) = u(r, \theta) + iv(r,\theta)\) be defined in some neighbourhood \(\epsilon\) of \(z_0 = r_0 e^{i\theta_0}\), $z_0 \neq 0$. If the first order partials derivatives of \(u\) and \(v\) with respect to \(r\) and \(\theta\) exists and are continuous at \(z_0\), and satisfies the polar form of the Cauchy-Riemann equations: 
		\begin{align*}
			ru_r &= v_\theta	&	u_\theta &= -r v_r
		\end{align*}
		Then \(f'(z_0)\) exists:
		\begin{align*}
			f'(z_0) = \eval{e^{-i\theta}(u_r + iv_r)}_{(r_0, \theta_0)}
					= \eval{\frac{-i}{z_0}(u_\theta + iv_\theta )}_{(r_0, \theta_0)}
		\end{align*}
	\end{theorem}
	\begin{proof}
		Let \(f(z) = u(r,\theta) + iv(r,\theta)\). Suppose that the first order partial derivatives of \(u\) and \(v\) exists in some neighbourhood \(\epsilon \) of \(z_0\) and is continuous at \(z_0\). By differentiating \(u\) with respect to \(x\) and \(y\):
		\begin{align*}
			\pdv{u}{r} &= \pdv{u}{x} \pdv{x}{r} + \pdv{u}{y}\pdv{y}{r} &
			\pdv{u}{\theta} &= \pdv{u}{x} \pdv{x}{\theta} + \pdv{u}{y}\pdv{y}{\theta} 
		\end{align*}
		Likewise for \(v\). As \(x = r \cos\theta \) and \(y = r\sin \theta\):
		\begin{align*}
			u_r &= u_x \cos \theta + u_y \sin \theta 
				&	u_\theta &= -u_x r \sin \theta + u_y r \cos \theta \\
			v_r &= v_x \cos \theta + v_y \sin \theta 
			&	v_\theta &= -v_x r \sin \theta + v_y r \cos \theta \\
		\end{align*}
		From \cref{Cauchy-Riemann Equations (Cartesian) Theorem - Complex} we have: 
		\begin{align*}
			u_x &= v_y	&	u_y = -v_x
		\end{align*}
		Subbing the Cauchy-Riemann equations into \(v_r\) and \(v_\theta\):
		\begin{align*}
			u_r &= u_x \cos \theta + u_y \sin \theta 
			&	u_\theta &= -u_x r \sin \theta + u_y r \cos \theta \\
			v_r &= -u_y \cos \theta + u_x \sin \theta 
			&	v_\theta &= u_y r \sin \theta + u_x r \cos \theta \\
		\end{align*}
		We can see that: 
		\begin{align*}
			ru_r &= v_\theta 	&	u_\theta &= -rv_r
		\end{align*}
		Which are the Cauchy Riemann equations in polar form. Let's verify it without relying on the Cauchy-Riemann equations in Cartesian form: 
		
		{\color{Grey}
			Recall:
			\begin{align*}
				u_r &= u_x \cos \theta + u_y \sin \theta 
				&	u_\theta &= -u_x r \sin \theta + u_y r \cos \theta \\
				v_r &= v_x \cos \theta + v_y \sin \theta 
				&	v_\theta &= -v_x r \sin \theta + v_y r \cos \theta \\
			\end{align*}
			Writing \(u_r\) and \(v_r\) in matrix notation: 
			\begin{align*}
				\begin{bmatrix}
					u_r \\ u_\theta
				\end{bmatrix}
				&= 
				\begin{bmatrix}
					\cos\theta & \sin\theta \\
					-r\sin\theta & r\cos\theta
				\end{bmatrix}
				\begin{bmatrix}
					u_x \\ u_y
				\end{bmatrix}
			\end{align*}
			Solving for \(u_x\) and \(u_y\):
			\begin{align*}
				\begin{bmatrix}
					u_x \\ u_y
				\end{bmatrix}
				&=
					\begin{bmatrix}
						\cos\theta & \sin\theta \\
						-r\sin\theta & r\cos\theta
					\end{bmatrix}^{-1}
					\begin{bmatrix}
						u_r \\ u_\theta
					\end{bmatrix} & \\
				&= \frac{1}{r \cos^2\theta + r \sin^2\theta}
					\begin{bmatrix}
						r\cos\theta & -\sin\theta \\
						-r\sin\theta & \cos\theta
					\end{bmatrix}
					\begin{bmatrix}
						u_r \\ u_\theta
					\end{bmatrix} 
					& \begin{bmatrix}
						a & b \\
						c & d \\
					  \end{bmatrix}^{-1}
				  	  &=
				  	  \frac{1}{ad - cb}
				  	  \begin{bmatrix}
				  	  	d & -b \\
				  	  	-c & a
				  	  \end{bmatrix} \\
				&= \frac{1}{r}
					\begin{bmatrix}
						r\cos\theta & -\sin\theta \\
						r\sin\theta & \cos\theta
					\end{bmatrix}
					\begin{bmatrix}
						u_r \\ u_\theta
					\end{bmatrix} & \\
			\end{align*}
		
			It is clear that for \(u_x\) and \(u_y\), and likewise for \(v_x\) and \(v_y\):
			\begin{align}
				\label{Cauchy-Riemann Equations (Polar) Theorem Proof Eqn 1 - Complex}
				u_x &= u_r \cos\theta - \frac{1}{r} u_\theta \sin\theta & 
				u_y &= u_r \sin\theta + \frac{1}{r} u_\theta \cos\theta \\
				\label{Cauchy-Riemann Equations (Polar) Theorem Proof Eqn 2 - Complex}
				v_x &= v_r \cos\theta - \frac{1}{r} v_\theta \sin\theta & 
				v_y &= v_r \sin\theta + \frac{1}{r} v_\theta \cos\theta 
			\end{align}
		
			Using the Cauchy-Riemann equations \(u_x = v_y\) and \(u_y = -v_x\), we see:
			\begin{align*}
				u_r \cos\theta - \frac{1}{r}u_\theta \sin\theta 
					&= v_r \sin\theta + \frac{1}{r} v_\theta \cos\theta \\
				u_r \sin\theta + \frac{1}{r}u_\theta \cos\theta 
					&= -v_r \cos\theta + \frac{1}{r} v_\theta \sin\theta
			\end{align*}
		
			Clearly, the equations are equal only if 
			\begin{align*}
				ru_r &= v_\theta & u_\theta = -rv_r
			\end{align*}
			Which are the polar forms of the Cauchy-Riemann equations.	
		}
		
		Show \(f'(z_0) = e^{-i\theta}(u_r + iv_r)\):
		
		{\color{Grey}
			Recall from \cref{Cauchy-Riemann Equations (Cartesian) Theorem - Complex}:
			\begin{align*}
				f'(z_0) =  u_x + iv_y
			\end{align*}
			Using \cref{Cauchy-Riemann Equations (Polar) Theorem Proof Eqn 1 - Complex} and \cref{Cauchy-Riemann Equations (Polar) Theorem Proof Eqn 2 - Complex} from before and substituting them into \(f'(z_0)\):
			\begin{align*}
				f'(z_0) &= \eval{\left(u_r \cos\theta - \frac{1}{r}u_\theta \sin\theta + iv_r \cos\theta - \frac{i}{r} v_\theta \sin\theta \right)}_{(r_0, \theta_0)} \\
				&= \eval{\left(u_r \cos\theta + v_r \sin\theta + iv_r \cos\theta -iu_r \sin\theta \right)}_{(r_0, \theta_0)} \\
				&= \eval{\left[u_r(\cos\theta - i\sin\theta) + v_r(\sin\theta + i\cos\theta)\right]}_{(r_0,\theta_0)} \\
				&= \eval{\left[u_r(\cos\theta - i\sin\theta) + i v_r(\cos\theta - i\sin\theta)\right]}_{(r_0,\theta_0)} \\
				&= \eval{\left[\left(\frac{e^{i\theta} + e^{-i \theta}}{2} - \frac{e^{i\theta} - e^{-i \theta}}{2} \right)(u_r+iv_r)\right]}_{(r_0, \theta_0)} \\
				&= \eval{e^{-i\theta}(u_r + iv_r)}_{(r_0,\theta_0)} \\
				&= \eval{\frac{-i}{re^{i\theta}} (u_\theta + iv_\theta)}_{(r_0, \theta_0)}
				 = \eval{\frac{-i}{z_0} (u_\theta + iv_\theta)}_{(r_0, \theta_0)}
					& (ru_r = v_\theta) \land (u_\theta = -rv_r)
			\end{align*}
			Thus
			\begin{align*}
				f'(z_0) &= \eval{e^{-i\theta}(u_r + iv_r)}_{(r_0,\theta_0)} 
						 = \eval{\frac{-i}{z_0} (u_\theta + iv_\theta)}_{(r_0, \theta_0)}
			\end{align*}
		}
		
		
		
	\end{proof}

	\begin{question}
		When comparing the Cartesian form to the polar form of the Cauchy-Riemann equations:
		\begin{align*}
			&f'(z_0) \text{ exists} \implies \forall z_0[(u_x = v_y)\land(u_y = -v_x)] \\
			&(z_0 \neq 0) \land \forall z_0 [(ru_r = v_\theta) \land (u_\theta = -r v_r)]  
				\implies f'(z_0) \text{ exists }
		\end{align*}
		Should both be $\iff$ instead of $\implies$? 
		No, satisfying Cauchy-Riemann equations does not guarantee differentiability at a point as we will see in \cref{f(z) satisfy Cauchy-Riemann but f'(z_0) does not exist Example - Complex}. However, satisfying certain conditions allows allows differentiability to exist (\cref{Cauchy-Riemann Differentiablity Conditions Theorem - Complex}). 
	\end{question}

	\begin{example}[Solving the \(f'(z)\) using the partial derivative with respect to one variable] 
		Recall in \cref{Cauchy-Riemann Equations (Cartesian) Theorem - Complex}: 
		\begin{align*}
			f'(z_0) &= \eval{[u_x + iv_x]}_{(x_0, y_0)} 
			= \eval{[v_y - i u_y]}_{(x_0, y_0)}
		\end{align*}
		This implies we can solve \(\dd{f(z)} /\dd{z}\) by taking the partial of \(f(z)\) with respect to \(x\) or \(y\). Consider \(f(z) = z^2\):
		\begin{align*}
			f(z) = z^2 = x^2 - y^2 + i2xy
		\end{align*}
		We then have:
		\begin{align*}
			u(x,y) &= x^2 - y^2	& v(x,y) &= 2xy
		\end{align*}
		Hence
		\begin{align*}
			u_x &= 2x = v_y	&
			u_y &= -2y = -v_x	
		\end{align*}
		Thus
		\begin{align*}
			f'(z) = 2x + i2y = 2(x+iy) = 2z 
		\end{align*}
	\end{example}
	
	\begin{example}[Using Cauchy-Riemann equations to find where \(f(z)\) is not differentiable]
		Using the contrapositive of \(f'(z_0) \text{ exists} \implies \exists u' \exists v' [(u_x = v_y)\land(u_y = -v_x)]\):
		\begin{align*}
			\exists z_0 [(u_x \neq v_y)\lor(u_y \neq -v_x)] \implies f(z) \text{ not differentiable at } z_0 
		\end{align*}
		Consider \(f(z) = \abs{z}^2\):
		\begin{align*}
			u(x,y) &= x^2 + y^2	&
			v(x,y) = 0
		\end{align*}
		By Cauchy-Riemann: 
		\begin{align*}
			2x &= 0 & 2y &= 0
		\end{align*}
		Therefore, \(f'(z)\) only exists at \((0,0)\) and does not exist elsewhere. 
	\end{example}	

	Note: \Cref{Cauchy-Riemann Equations (Cartesian) Theorem - Complex} does not guarantee the existence of \(f'(z)\) at \(z_0\).
	
	\begin{example}[\(f(z)\) satisfy Cauchy-Riemann equations at (0,0), but \(f'(0)\) does not exist]
	\label{f(z) satisfy Cauchy-Riemann but f'(z_0) does not exist Example - Complex}
		Consider
		\begin{align*}
			f(z) = 
			\begin{cases}
				\bar{z}^2 / z & z \neq 0 \\
				0			  & z = 0
			\end{cases}
		\end{align*}
		Then
		\begin{align*}
			u(x,y) &= \frac{x^3 - 3xy^2}{x^2 + y^2} &
			v(x,y) &= \frac{y^3 - 3x^2y}{x^2 + y^2} & (x,y) \neq (0,0)
		\end{align*}
		Checking differentiability at \((0,0)\), note \(u(0,0) = 0\) and \(v(0,0) = 0\):
		\begin{align*}
			u_x(0,0) 
			&= \lim_{\Delta x \rightarrow 0} \frac{u(0+\Delta x, 0) - u(0,0)}{\Delta x}
			 = \lim_{\Delta x \rightarrow 0} \frac{\Delta x}{\Delta x} = 1 \\
			v_y(0,0) 
			&= \lim_{\Delta y \rightarrow 0} \frac{v(0, 0+\Delta y) - v(0,0)}{\Delta y}
			 = \lim_{\Delta y \rightarrow 0} \frac{\Delta y}{\Delta y} = 1 \\
			u_y(0,0) 
			 &= \lim_{\Delta y \rightarrow 0} \frac{u(0, 0+\Delta y) - u(0,0)}{\Delta y}
			  = \lim_{\Delta y \rightarrow 0} \frac{0/(\Delta y)^2}{\Delta y} = 0 \\
			v_x(0,0) 
			 &= \lim_{\Delta x \rightarrow 0} \frac{v(0+\Delta x, 0) - v(0,0)}{\Delta x}
			  = \lim_{\Delta x \rightarrow 0} \frac{0/(\Delta x)^2}{\Delta x} = 0
		\end{align*}
		We can see that the Cauchy-Riemann equations are satisfied: 
		\begin{align*}
			u_x &= v_y = 1	&	u_y &= -v_x = 0
		\end{align*}
		However, \(f'(0)\) does not exist: (Brown and Churchill - Complex Variables and Applications, Section 20, Exercise 9 \cite{Brown.J;Churchill.R-Complex-Variables-2014})
		
		Let \(\Delta w = f(z + \Delta z) - f(z)\). We need to show for all nonzero points on the real and imaginary axis, \(\Delta w/\Delta z = -1\), but for all nonzero points on the line \(\Delta x = \Delta y\), \(\Delta w / \Delta z = -1\). Hence, a contradiction, so \(f('0)\) does not exist.
		
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[remember picture]
				\begin{axis}[
					unit vector ratio=1 1 1,
					xlabel={$\Re$},
					ylabel={$\Im$},
					xmin=-1, xmax=12,
					ymin=-1, ymax=12,
					%					xtick distance=2,
					%					ytick distance=2,
					xtick=100,
					ytick=100,
					axis lines = middle,
					axis line style = Black,
					]
					\addplot[
					only marks,
					nodes near coords,
					point meta = explicit symbolic, 
					color=Black,
					mark=*,
					]
					coordinates {
						(7, 7) []
						(7, 0) [$(\Delta x, 0)$]
						(0, 7) [$(0, \Delta y)$]
						(0, 0) [$(0, 0)$]
					};
					\draw[->, very thick, draw=Black]  (axis cs: 7,0)--(axis cs: 5,0)
					node[midway,above] {};
					\draw[->, very thick, draw=Black]  (axis cs: 0,7)--(axis cs: 0,5);
					\draw[->, very thick, draw=Black]  (axis cs: 7,7)--(axis cs: 5,5)
					node[midway,sloped,above] {$\Delta y = \Delta x$};
					\draw[draw=Black]  (axis cs: 10,10)--(axis cs: 0,0);
				\end{axis}
			\end{tikzpicture}
		\end{figure}
		
		{\color{Grey}
			\begin{align*}
				\frac{\Delta w}{\Delta z}
					&=\frac{f(z+\Delta z) - f(z)}{\Delta z}
					 =\frac{u(x+\Delta x, y+\Delta y) + v(x+\Delta x, y+\Delta y)}{\Delta x + \Delta y} - \frac{u(x,y) + v(x,y)}{\Delta x + \Delta y}
			\end{align*}
		
			\textbf{Along the real axis:}
			
			Evaluating along \((\Delta x, 0) \rightarrow (0,0)\).
			\begin{align*}
				\lim_{(\Delta x, 0) \rightarrow (0,0)} \frac{\Delta w}{\Delta z}
					&= \frac{u(\Delta x, 0) + v(\Delta x, 0)}{\Delta x}
						- \frac{u(0,0) + v(0,0)}{\Delta x} \\
					&= \frac{1}{\Delta x} \left[ \frac{(\Delta x)^3}{(\Delta x)^2} + \frac{0}{(\Delta x)^2}\right] - 0
					 = \frac{\Delta x}{\Delta x} = 1
			\end{align*}
			
			\textbf{Along the imaginary axis:}
			
			Evaluating along \((0, \Delta y) \rightarrow (0,0)\).
			\begin{align*}
				\lim_{(0,\Delta y) \rightarrow (0,0)} \frac{\Delta w}{\Delta z}
					&= \frac{u(0,\Delta y) + v(0, \Delta y)}{\Delta y}
						- \frac{u(0,0) + v(0,0)}{\Delta y} \\
					&= \frac{1}{\Delta y} \left[ \frac{0}{(\Delta y)^2} + \frac{(\Delta y)^3}{(\Delta y)^2}\right] - 0 
					 = \frac{\Delta y}{\Delta y} = 1
			\end{align*}
		
			\textbf{Along the axis \(\Delta x = \Delta y\):}
			
			Evaluating along \((\Delta x, \Delta x) \rightarrow (0,0)\).
			\begin{align*}
				\lim_{(\Delta x,\Delta x) \rightarrow (0,0)} \frac{\Delta w}{\Delta z}
					&= \frac{u(\Delta x, \Delta x)}{\Delta x + \Delta x} - \frac{u(0,0) + v(0,0)}{\Delta x + \Delta x} \\
					&= \frac{1}{2\Delta x} \left[ \frac{(\Delta x)^3 - 3(\Delta x)^3}{2(\Delta x)^2} + \frac{(\Delta x)^3 - 3(\Delta x)^3}{2(\Delta x)^2}\right] \\
					&= \frac{1}{2\Delta x}  \left[ - \frac{2(\Delta x)^3}{2(\Delta x)^2} - \frac{2(\Delta x)^3}{2(\Delta x)^2} \right] 
					= \frac{1}{2\Delta x} \left[-\Delta x - \Delta x \right] = -\frac{2\Delta x}{2\Delta x} = -1
			\end{align*}
			As we can see, the limits are not unique regardless of the path we take to approach \((0,0)\), hence \(f'(0)\) does not exist. Therefore, an equation can satisfy the Cauchy-Riemann equations at \(0,0\), yet have a derivative that does not exist. The Cauchy-Riemann equations does not guarantee differentiability at \(z_0\).
		}
		
	
	\end{example}

	\begin{example}(Any branch of \(f(z) = z^{1/2}\) is differentiable everywhere in domain of definition)
		Let 
		\begin{align*}
			f(z) = z^{1/2} &= \sqrt{r}e^{i\theta}	&	r>0, \ \alpha<\theta<\alpha+2\pi
		\end{align*}
		Hence
		\begin{align*}
			u(r, \theta) &= \sqrt{r}\cos \left(\frac{\theta}{2}\right) &
			v(r, \theta) &= \sqrt{r}\sin \left(\frac{\theta}{2}\right)
		\end{align*}
		By Cauchy-Riemann: 
		\begin{align*}
			ru_r &= \frac{\sqrt{r}}{2} \cos \left(\frac{\theta}{2}\right) = v_\theta &
			u_\theta &= - \frac{\sqrt{r}}{2} \sin \left(\frac{\theta}{2}\right) = -r v_r
		\end{align*}
		Thus, the derivative exists wherever \(f(z)\) is defined. Also, by \cref{Cauchy-Riemann Equations (Polar) Theorem - Complex}:
		\begin{align*}
			f'(z) 
				&= \eval{e^{i\theta} (u_r + iv_r)}_{(r_0, \theta_0)} \\
				&= e^{-i\theta} \left[\frac{1}{2\sqrt{r}}\cos\left(\frac{\theta}{2}\right) + i \frac{1}{2\sqrt{r}}\sin\left(\frac{\theta}{2}\right)\right] 
				= \frac{1}{2\sqrt{r}} e^{-i\theta} \left[\cos \left(\frac{\theta}{2}\right) + i \sin \left(\frac{\theta}{2}\right)\right] \\
				&= \frac{1}{2\sqrt{r}e^{i\theta/2}} = \frac{1}{2f(z)} = \frac{1}{2} z^{-1/2}
		\end{align*}
	\end{example}

	\subsection{Complex Form of the Cauchy-Riemann Equations}
	\label{Cauchy-Riemann Equations (Complex) Subsection - Complex}
	
	\begin{theorem}[Cauchy-Riemann Equation (Complex Form)]
		\label{Cauchy-Riemann Equations (Complex) Theorem - Complex}
		Let \(f(z) = u(x,y) + iv(x,y)\). If the first order partial derivatives of \(u\) and \(v\) with respect to \(x\) and \(y\) exists and satisfy the Cauchy-Riemann equations. Then
		\[\pdv{\bar{z}} f(z) = 0\]
	\end{theorem}
	\begin{proof}{\color{Grey}
		Recall:
		\begin{align*}
			x &= \frac{z + \bar{z}}{2} & y &= \frac{z - \bar{z}}{2i}
		\end{align*}
		Let \(F\) be a real valued function, that is \(x, y \in \mathbb{R}\). Then
		\begin{align*}
			\pdv{F}{\bar{z}} &= \pdv{F}{x} \pdv{x}{\bar{z}} + \pdv{F}{y} \pdv{y}{\bar{z}}
		\end{align*}
		Substituting \(\pdv{x}{\bar{z}} = 1/2\) and \(\pdv{y}{\bar{z}} = i/2\):
		\begin{align*}
			\pdv{F}{\bar{z}} &= \frac{1}{2} \left(\pdv{F}{x} + i \pdv{F}{y} \right)
		\end{align*}
		Define the operator:
		\begin{align*}
			\pdv{\bar{z}} &= \frac{1}{2} \left(\pdv{x} + i \pdv{y} \right)
		\end{align*}
		Then 
		\begin{align*}
			\pdv{f}{\bar{z}} &= \frac{1}{2} \left(\pdv{f}{x} + i \pdv{f}{y} \right)
				= \frac{1}{2} \left(\pdv{u}{x} + i \pdv{v}{x} + i\pdv{u}{y} -  \pdv{v}{y}  \right) \\
				&= \frac{1}{2} \left[ (u_x - v_y) + i (u_y + v_x)\right]
		\end{align*}
		We can see that if \(\pdv{f}{\bar{z}}\) satisfies the Cauchy-Riemann equations (\cref{Cauchy-Riemann Equations (Cartesian) Theorem - Complex}):
		\begin{align*}
			\pdv{\bar{z}} f(z) &= 0 & \pdv{x}f &= -i \pdv{f}{y} \implies i\pdv{x}f = \pdv{f}{y} 
		\end{align*}
		}
	\end{proof}
	
	\subsection{Conditions for Differentiability}
	\label{Conditions for Differentiablity Subsection - Complex}
	
	\begin{theorem}
		\label{Cauchy-Riemann Differentiablity Conditions Theorem - Complex}
		Let \(f(z) = u(x,y) + iv(x,y)\) be defined in some neighbourhood \(\epsilon\) of point \(z_0 = x_0 + iy_0\). Consider the first order partial derivatives of \(u\) and \(v\) with respect to \(x\) and \(y\). If they 
		\begin{itemize}
			\item[(1)] Exist for all \(z\), \(\abs{z-z_0}<\epsilon\).
			\item[(2)] Are continuous at \(z_0\).
			\item[(3)] Satisfies the Cauchy-Riemann equations at \(z_0\).
		\end{itemize}
		Then \(f'(z_0)\) exists:
		\[f'(z_0) = \eval{(u_x + iv_x)}_{(x_0, y_0)}\]
	\end{theorem}
	\begin{proof}
		Assume the first order partial derivatives of \(u\) and \(v\) with respect to \(x\) and \(y\) exists \(\forall z [\abs{z-z_0}<\epsilon]\), are continuous at \(z_0\), and satisfies the Cauchy-Riemann equations. 
		Let \(\Delta z = \Delta x + i\Delta y\), \(0<\abs{\Delta z}<\epsilon\), and \(\Delta w = f(z_0 + \Delta z) - f(z_0)\).
		We then have \[\Delta w = \Delta u + i\Delta v\]
		Where
		\begin{align*}
			\Delta u &= u(x_0 + \Delta x, y_0 + \Delta y) - u(x_0, y_0) \\
			\Delta v &= v(x_0 + \Delta x, y_0 + \Delta y) - v(x_0, y_0) 
		\end{align*}
		Since first order partials of \(u\) and \(v\) are continuous at \(z_0\):
		\begin{align*}
			\Delta u &= u_x(x_0, y_0)\Delta x + u_y(x_0, y_0)\Delta y + \epsilon_1\Delta x + \epsilon_2 \Delta y \\
			\Delta v &= v_x(x_0, y_0)\Delta x + v_y(x_0, y_0)\Delta y + \epsilon_3\Delta x + \epsilon_4 \Delta y
		\end{align*}
		\[(\epsilon_1, \epsilon_2, \epsilon_3, \epsilon_4) \rightarrow (0,0,0,0) \text{ as } (\Delta x, \Delta y) \rightarrow (0,0)\]
		Substituting \(\Delta u\) and \(\Delta v\) into \(\Delta w\):
		\begin{align*}
			\Delta w 
				=& u_x(x_0, y_0)\Delta x + u_y(x_0, y_0)\Delta y + \epsilon_1\Delta x + \epsilon_2 \Delta y \\
				&+ i[v_x(x_0, y_0)\Delta x + v_y(x_0, y_0)\Delta y + \epsilon_3\Delta x + \epsilon_4 \Delta y]
		\end{align*}
		Using the Cauchy-Riemann equations and dividing by \(\Delta z\):
		\begin{align*}
			\frac{\Delta w}{\Delta z}
			&= u_x(x_0, y_0) + iv_x(x_0, y_0) + (\epsilon_1 + i\epsilon_3) \frac{\Delta x}{\Delta z} + (\epsilon_2 + i\epsilon_4) \frac{\Delta y}{\Delta z}
		\end{align*}
		From the inequalities \(\abs{\Delta x} \leq \abs{\Delta z}\) and \(\abs{\Delta y} \leq \abs{\Delta z}\):
		\begin{align*}
			\abs{\frac{\Delta x}{\Delta z}} &\leq 1		&
			\abs{\frac{\Delta y}{\Delta z}} &\leq 1	
		\end{align*}
		So
		\begin{align*}
			\abs{(\epsilon_1 + i\epsilon_3 )\frac{\Delta x}{\Delta z}} 
				&\leq \abs{\epsilon_1 + i \epsilon_3} \leq \abs{\epsilon_1} + \abs{\epsilon_3} \\
			\abs{(\epsilon_2 + i\epsilon_4 )\frac{\Delta y}{\Delta z}} 
				&\leq \abs{\epsilon_2 + i \epsilon_4} \leq \abs{\epsilon_2} + \abs{\epsilon_4}
		\end{align*}
		Then \(\abs{\epsilon_2} + \abs{\epsilon_4} \rightarrow 0\) and \(\abs{\epsilon_1} + \abs{\epsilon_3} \rightarrow 0\) as \(\Delta z = \Delta x + i\Delta y \rightarrow 0\).
		\begin{align*}
			\implies& \frac{\Delta w}{\Delta z} = u_x(x_0, y_0) + iv_x(x_0, y_0) 
			\implies f'(z_0) \text{ exists}
		\end{align*}
	\end{proof}

	\begin{example}[All 3 conditions must be satisfied for \(f'(z_0)\) to exist]
		Do not use expression of \(f'(z)\) before existence of \(f'(z_0)\)  is established.
		Consider \(f(z) = x^3 + i(1-y)^3\).
		\begin{align*}
			u(x,y) &= x^3	& 	v(x,y) &= (1-y)^3
		\end{align*}
		Taking the partial derivatives:
		\begin{align*}
			u_x &= 3x^2	& 	v_x &= 0\\
			u_y &= 0	&	v_y &= -3(1-y)^2
		\end{align*}
		It would be foolish to ignore Cauchy-Riemann and directly use:
		\begin{align*}
			f'(z) = u_x + iv_x = 3x^2
		\end{align*}
		We can see that the Cauchy-Riemann equations are satisfied only if:
		\begin{align*}
			3x^2 = -3(1-y)^2 \implies  x^2 + (1-y)^2 = 0 \implies (x=0)\land(y=1)
		\end{align*}
		Therefore, \(f'(z)\) exists only if \(z = i\), and that \(f'(i) = 0\)
	\end{example}
	
	\section{Analytic Functions} \label{Analytic Functions - Complex}
	
	\begin{definition}[Analytic/Regular/Holomorphic]
		\label{Analytic Definition - Complex}
		Let $S$ be an open set, \(S \subset \mathbb{C}\). Let \(f\) be a function.
		\begin{align*}
			f \text{ is analytic in } S \iff \forall z \in S[ f'(z) \text{ exists }]
		\end{align*}
		We say \(f(z)\) is analytic at a point \(z_0\) if it is analytic in some neighbourhood of \(z_0\). If we say that \(f(z)\) is analytic in a closed set \(S'\) then we mean that it is analytic in an open set \(S\) where \(S' \subset S\).
	\end{definition}
	
	\begin{definition}[Entire]
		A function \(f(z)\) is entire if it is analytic at all points in the plane. 
	\end{definition}

	\begin{example}
		\begin{align*}
			\text{Derivative of polynomial exists everywhere} \implies \text{All polynomials are entire functions}
		\end{align*}
	\end{example}
	
	See \cref{Conditions for Differentiablity Subsection - Complex} for conditions for a function te be differentiable, hence analytic in a set \(S\).
	
	\begin{corollary}
		Let \(f(z)\) and \(g(z)\) be analytic in a domain \(D\). Then the following are analytic in \(D\):
		\begin{align*}
			&f(z)  + g(z) 		& \\
			&f(z)g(z)	 		& \\
			&\frac{f(z)}{g(z)} 	& g(z) \neq 0 \forall z \in D
		\end{align*}
		Likewise, if \(P(z)\) and \(Q(z)\) are polynomials, then \(P(z)/Q(z)\) is analytic if \(\forall z \in D[Q(z) \neq 0]\).
	\end{corollary}

	\begin{corollary}
		Let \(w\) be the image of \(D\) under \(f(z)\) and \(w\) be the domain of \(g\). Then \(g(f(z))\) is analytic in \(D\) and 
		\begin{align*}
			\dv{z} g[f(z)] = g'[f(z)]f'(z)
		\end{align*}
	\end{corollary}

	\begin{theorem}
		Let \(D\) be the domain of a function \(f(z)\).
		\begin{align*}
			\forall z \in D[f'(z) = 0] \implies f(z) \text{ is constant in } D
		\end{align*}
	\end{theorem}
	\begin{proof}
		Let \(f(z) = u(x,y) + iv(x,y)\) with domain \(D\), and \(P\), \(P'\), and \(Q\) be points in \(D\). Let \(\vec{U}\) be the unit vector on the line segment \(L\) connecting \(P\) and \(P'\), and \(s\) be the distance along \(L\).
		\begin{align*}
			f'(z) = 0 &\implies \forall z \in D [u_x = u_y = v_x = v_y = 0]
		\end{align*}
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[remember picture]
				\begin{axis}[
					unit vector ratio=1 1 1,
					xlabel={$x$},
					ylabel={$y$},
					xmin=0, xmax=20,
					ymin=0, ymax=12,
					%					xtick distance=2,
					%					ytick distance=2,
					xtick=100,
					ytick=100,
					axis lines = middle,
					axis line style = Black,
					]
					\addplot[
					only marks,
					nodes near coords,
					point meta = explicit symbolic, 
					color=Black,
					mark=*,
					]
					coordinates {
						(3, 3) [$P$]
						(12, 9) [$P'$]
						(16, 5) [$Q$]
					};
					\draw[->, draw=Black]  (axis cs: 6,5)--(axis cs: 11,8.333)
					node[midway,sloped,above] {$L$};
					\draw[->, very thick, draw=Black]  (axis cs: 4,3.667)--(axis cs: 6,5)
					node[midway,above] {$\vec{U}$};
					\draw[-,draw=Grey]  (axis cs: 3,3) -- (axis cs: 12,9) ;
					\draw[-,draw=Grey]  (axis cs: 12,9) -- (axis cs: 16,5);
					\draw[]  (axis cs: 7,2) to ["$D$"] (axis cs: 7,2);
					\draw[]  (axis cs: 11,8.333)--(axis cs: 11,8.333) node[midway, below] {$s$};
					\draw[dashed, draw=Grey] plot [smooth cycle] coordinates {(axis cs: 1,3) (axis cs: 10,1) (axis cs: 18,3) (axis cs: 12,11)};
				\end{axis}
			\end{tikzpicture}
		\end{figure}
		We know that the directional derivative:
		\begin{align*}
			\dv{u}{s} &= \grad{u} \cdot \vec{U} & \grad{u} &= u_x \hat{i} + u_y \hat{j}
		\end{align*}
		Previously, \(u_x = u_y = 0\), so for all points on \(L\):
		\begin{align*}
			u_x = u_y = 0 \implies \grad{u} = 0 \implies \dv{u}{s} = 0 
			\implies u \text{ constant on } L
		\end{align*}
		Now, that we have established that \(u\) is constant on any given line \(L\) in \(D\), we can see that since \(D\) is simply connected and there are finitely many lines connecting \(P\) and \(Q\), the values of \(u\) at \(P\) and \(Q\) must be equal and constant. Hence, \(\exists a \in \mathbb{R}\) such that \(u(x,y) = a \) in \(D\). Likewise, \(v(x,y) = b\) in \(D\). Thus
		\begin{align*}
			f(z) &= a + bi = c & c \text{ is constant}
		\end{align*}
	\end{proof}
	
	\begin{definition}[Singular Point]
		\label{Singular Point Definition - Complex}
		Let \(\epsilon\) be a neighbourhood of point \(z_0\), and \(f(z)\) be a function. \(z_0\) is a singular point if \(f'(z_0)\) does not exist, but \(f(z)\) is differentiable in all neighbourhoods of \(z_0\).
	\end{definition}
	
	\subsection{Examples} \label{Analytic Functions Examples Subsection - Complex}
	
	\begin{example}[Determining analyticity using Cauchy-Riemann equations]
		Consider \(f(z) = \sin(x)\cosh(y) + i\cos(x)\sinh(y) \).
		\begin{align*}
			u(x,y) &= \sin(x)\cosh(y)	& 	v(x,y) = \cos(x)\sinh(y)
		\end{align*}
		Cauchy-Riemann:
		\begin{align*}
			u_x &= \cos(x)\cosh(y) = v_y	&	u_y = \sin(x)\sinh(y) = -v_x
		\end{align*}
		Therefore, it is clear that \(f(z)\) is entire.
		\begin{align*}
			f'(z) = u_x + iv_x = \cos(x)\cosh(y) - i\sin(x)\sinh(y) 
		\end{align*}
		Another application of Cauchy-Riemann see that \(f'(z)\) is also entire.
	\end{example}

	\begin{example}[\(f(z)\) and \(\overline{f(z)}\) is analytic in \(D \implies f(z)\) is constant in \(D\)]
		Let 
		\begin{align*}
			f(z) &= u(x,y) + iv(x,y) & \overline{f(z)} &= u(x,y) - iv(x,y) = U(x,y) + iV(x,y)
		\end{align*}
		Because of \(f(z)\) and \(\overline{f(z)}\) is analytic in \(D\), the Cauchy-Riemann equations hold:
		\begin{align*}
			u_x &= v_y		&	u_y &= -v_x \\
			U_x &= V_y		&	U_y &= -V_x
		\end{align*}
		We can see that:
		\begin{align*}
			u_x &= -v_y	= v_y &	u_y &= v_x = -v_x
		\end{align*}
		Hence, \(u_x = 0\) and \(v_x = 0\), then we can conclude
		\begin{align*}
			f'(z) = 0 \implies f(z) \text{ is constant in } D
		\end{align*}
	\end{example}

	\begin{example}[\(f(z)\) is analytic in \(D\) and \(\abs{f(z)}\) is constant in \(D \implies f(z)\) is constant in \(D\)]
		Let \(\forall z \in D[\abs{f(z)} = c]\), where \(c\) is a constant. It is easy to see that \(c = 0 \implies \forall z \in D[f(z) = 0]\), so consider \(c \neq 0\). Then
		\begin{align*}
			f(z)\overline{f(z)} = c^2 \neq 0 \implies \forall z \in D[f(z) \neq 0]
		\end{align*}
		Thus
		\begin{align*}
			\overline{f(z)} = \frac{c^2}{f(z)} \qquad \forall z \in D
		\end{align*}
		Hence \(\overline{f(z)}\) is analytic everywhere in \(D\), so \(f(z)\) is constant in \(D\).
		\label{Abs of analytic function is constant in domain implies that function is constant Example - Complex}
	\end{example}
	
	\section{Harmonic Functions} \label{Harmonic Functions - Complex}
	
	Harmonic functions are functions where the curvature in each component direction cancels each other out.
	
	\begin{definition}[Laplace's Equation]
		\label{Laplace's Equation Definition - Complex}
		Let \(F(x,y)\) be a real-valued function. That is \(x,y \in \mathbb{R}\). Laplace's equation: 
		\begin{align*}
			\pdv[2]{x} F + \pdv[2]{y} F = 0
		\end{align*}
		In polar form:
		\begin{align*}
			r^2 u_{rr}(r, \theta) + ru_r(r, \theta) + u_{\theta \theta}(r, \theta) &= 0 \\
			r^2 v_{rr}(r, \theta) + rv_r(r, \theta) + v_{\theta \theta}(r, \theta) &= 0
		\end{align*}
		See \cref{Laplace's Equation (Polar) Exercise - Complex}
	\end{definition}

	\begin{definition}[Harmonic]
		A real-valued function \(F(x,y)\) is harmonic in the \(xy\)-plane if it satisfies Laplace's equation.
	\end{definition}

	\begin{theorem}
		\label{Analytic implies Harmonic Theorem - Complex}
		Let \(D\) be the domain of a function \(f(z) = u(x,y) + iv(x,y)  \).
		\begin{align*}
			f(z) \text{ is analytic in } D \implies u(x,y) \land v(x,y) \text{ are harmonic in } D
		\end{align*}
	\end{theorem}
	\begin{proof}
		\(f\) is analytic in \(D\), so its component functions must satisfy the Cauchy-Riemann equations: 
		\begin{align*}
			(u_x = v_y)\land(u_y = -v_x) 
				&\implies (u_{xy} = v_{yy})\land(u_{yx} = -v_{xx})  \\
			(u_x = v_y)\land(u_y = -v_x) 
				&\implies (u_{xx} = v_{yx})\land(u_{yy} = -v_{xy}) 
		\end{align*}
		Now, we know from calculus that \(u_{xy} = u_{yx}\) and \(v_{yx} = v_{xy}\), so we conclude
		\begin{align*}
			u_{xx} + u_{yy} &= 0 & v_{xx} + v_{yy} = 0
		\end{align*}
	\end{proof}

	Note: The converse (\(\impliedby\)) is true for simply connected domains, hence, \cref{Analytic implies Harmonic Theorem - Complex} becomes \(\iff\) in simply connected domains. (R, Boas - Invitation to Complex Analysis. (1987) Section 19.)
	
	\begin{corollary}
		Let \(F(x,y)\) is a real-valued function in a simply connected domain \(D\). Then there exists a function \(f(z)\) and \(g(z)\) in \(D\) such that \(f(z) = F(x,y) + iv(x,y)\) and \(g(z) = u(x,y) + iF(x,y)\). That is, there exists a function where the real part equals \(F\) and a function where the imaginary part equals \(F\).
	\end{corollary}

	\begin{definition}[Harmonic Conjugate]
		\label{Harmonic Conjugate Definition - Complex}
		If \(f(z) = u(x,y) + iv(x,y)\) is analytic in a domain \(D\), then \(v(x,y)\) is the harmonic conjugate of \(v(x,y)\). This is not to be confused with the complex conjugate.
	\end{definition}
	
	\begin{example}
		\label{Laplace's Equation (Polar) Exercise - Complex}
		Let \(f(z) = u(r, \theta) + iv(r, \theta)\) be analytic in domain \(D' = D \setminus \{0\}\). Show \(u(r, \theta)\) and \(v(r, \theta)\) satisfies the polar form of Laplace's equation.
		\begin{proof}{\color{Grey}
			We know from the Polar form of the Cauchy-Riemann equation:
			\begin{align*}
				ru_r &= v_\theta & u_\theta &= -rv_r
			\end{align*}
			Operating by \(r \pdv*{r}\) and \(\pdv*{\theta}\), we obtain:
			\begin{align*}
				r \pdv{r} r u_r &= ru_r + r^2 u_{rr} = r v_{\theta r} \\
				r \pdv{r} u_\theta &= ru_{\theta r} = r \pdv{r} (-rv_r) = -rv_r - r^2v_{rr} \\
				\pdv{\theta} ru_{r} &= r u_{\theta r} = v_{\theta \theta} \\
				\pdv{\theta} u_\theta &= u_{\theta \theta} = -r v_{r \theta}
			\end{align*}
			We can see that 
			\begin{align*}
				\begin{cases}
					ru_r + r^2 u_{rr} = -u_{\theta \theta} \\
					rv_r + r^2 v_{rr} = -v_{\theta \theta}
				\end{cases}
				\implies
				\begin{cases}
					r^2 u_{rr} + ru_r + u_{\theta \theta} = 0 \\
					r^2 v_{rr} + rv_r + v_{\theta \theta} = 0
				\end{cases}
			\end{align*}
			}
		\end{proof}
	\end{example}

	\begin{example}
		\label{f' neq 0 implies level curve are orthogonal Example - Complex}
		Let \(f(z) = u(x,y) + iv(x,y)\) be analytic in domain \(D\). Consider the families of level curves \(u(x,y) = c_1\) and \(v(x,y) = c_2\), with \(c_1, c_2 \in \mathbb{R}\) being constants. Show for \(z_0 = (x_0, y_0) \in \mathbb{C}\) common to \(u(x,y) = c_1\) and \(v(x,y) = c_2\) and \(f'(z_0) \neq 0\), then the lines tangent to \(u(x,y) = c_1\) and \(v(x,y) = c_2\) at \(z_0\) are orthogonal. 
		
		Note: 
		\begin{align*}
			[u(x,y) = c_1] \land [v(x,y) = c_2] \implies 
			\left(\pdv{u}{x} + \pdv{u}{y} \dv{y}{x} = 0\right) \land 
			\left(\pdv{v}{x} + \pdv{v}{y} \dv{y}{x} = 0\right)
		\end{align*}
	
		\begin{proof}{\color{Grey}
			The tangent lines of \(u(x,y)\) and \(v(x,y)\) are
			\begin{align*}
				\grad{u} &= \left(\pdv{u}{x}, \pdv{u}{y}\right) = (u_x, u_y)&
				\grad{v} &= \left(\pdv{v}{x}, \pdv{v}{y}\right) = (v_x, v_y)
			\end{align*}
			Taking the dot product, and applying the Cauchy-Riemann equations: 
			\begin{align*}
				u_x v_x + u_y v_y = u_x (-u_y) + u_y (u_x) = 0
			\end{align*}
			Hence, \(u(x,y)\) and \(v(x,y)\) are orthogonal.
			
			Note:
			\begin{align*}
				f'(z_0) = 0 
				&\implies u_x + iv_x = 0 \implies v_y - iu_y = 0 \\
				&\implies u_x = u_y = v_x = v_y = 0
			\end{align*}
			Hence, we can see that \(f'(z_0) = 0\) is required for \(u(x,y)\) and \(v(x,y)\) to exist and be orthogonal.
		}
		\end{proof}
	\end{example}
	
	
	\section{Uniquely Determined Analytic Functions} \label{Uniquely Determined Analytic Functions Section - Complex}
	
	\begin{lemma}
		\label{Analytic function is zero in domain or line segment in domain implies function is zero throughout domain Lemma - Complex}
		Suppose a function \(f\) is analytic throughout domain \(D\), and \(f(z) = 0\) \(\forall z \in D' \subset D\)  or line segment contained in \(D\). Then \(f(z) \equiv 0\) throughout \(D\).
	\end{lemma}
	\begin{proof}
		Let \(f\) be a function analytic in domain \(D\) and \(f(z) = 0\) for all point or line segment in \(D\). Let \(z_0\) in the subdomain of \(D\) or on a line segment in \(D\).
		
		\(D\) is connected open set, so there is a polygonal line \(L\) jointing any point \(P\) in \(D\) to \(z_0\) lying entirely in \(D\). (Recall: A polygonal line consists of a finite number of lines connected end-to-end.) Let \(d\) be the shortest distance from points on \(L\) to the boundary on \(D\), so \(d > 0\), unless \(D\) is the entire plane. Then there is a sequence of points along \(L\):
		\[ \{z_0, z_1, z_2, \ldots, z_{n-1}, z_n = P\} \qquad \abs{z_k - z_{k+1}}< d \qquad k \in \mathbb{N}\]
		That is, each point is sufficiently close to each other. We construct neighbourhoods of each point with radius \(d\), all of which are in \(D\), so points \(z_{k-1}\) and \(z_{k+1}\) lie in the neighbourhood of \(z_k\), \(k \in \mathbb{N}\):
		\[\{N_0, N_1, N_2, \ldots, N_{n-1}, N_n\}\]
		
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[remember picture]
				\begin{axis}[
					height=12cm,
					rotate around={-10:(current axis.origin)},
					clip=false,
%					enlarge y limits={upper, value=0.1},
					unit vector ratio=1 1 1,
					xlabel={$x$},
					ylabel={$y$},
					xmin=0, xmax=100,
					ymin=0, ymax=50,
					%					xtick distance=2,
					%					ytick distance=2,
					xtick=100,
					ytick=100,
					axis lines = none,
					axis line style = Black,
					]
					\addplot[
					only marks,
					nodes near coords,
					point meta = explicit symbolic, 
					color=Black,
					mark=*,
					]
					coordinates {
						(10, 10) [$z_0$]
						(15, 15) [$z_1$]
						(20, 20) [$z_2$]
						(80, 40) [$z_n = P$]
					};
					\draw[draw=Black]  (axis cs: 10,10)--(axis cs: 30,30)
					node[midway,sloped,above] {};
					\draw[draw=Black]  (axis cs: 30,30)--(axis cs: 40,20)
					node[midway,above] {$L$};
					\draw[-,draw=Black]  (axis cs: 40,20) -- (axis cs: 80,40);
					\draw[dashed, draw=Grey] (axis cs: 10, 10) circle (10);
					\draw[dashed, draw=Grey] (axis cs: 15, 15) circle (10);
					\draw[dashed, draw=Grey] (axis cs: 20, 20) circle (10);
					\draw[dashed, draw=Grey] (axis cs: 80, 40) circle (10);
					
					\addplot[
					only marks,
					nodes near coords,
					point meta = explicit symbolic, 
					color=Black,
					mark=none,
					]
					coordinates {
						(10, 0) [$N_0$]
						(15, 5) [$N_1$]
						(20, 10) [$N_2$]
						(80, 30) [$N_n$]
					};
				\end{axis}
			\end{tikzpicture}
		\end{figure}
		
		Now as \(f\) is analytic in \(N_0\) and \(f(z) = 0\) in a domain or line segment containing \(z_0\), then \(f(z) \equiv 0 \) in \(N_0\). \(z_1\) is in \(N_0\), so \(f(z_0) \equiv 0\) in \(N_1\). Continuing this we can see that \(f(z_n) \equiv 0\) in \(N_n\), hence, \(f(z) \equiv 0\) in \(D\).
	\end{proof}
	
	\begin{theorem}
		\label{Uniquely determined analytic functions Theorem - Complex}
		Let \(f\) be analytic in domain \(D\). Then it's uniquely determined over \(D\) by its values in \(D\) or along a line segment in \(D\).
	\end{theorem}
	\begin{proof}
		Let functions \(f\) and \(g\) be analytic in some domain \(D\), and \(f(z) = g(z)\) \(\forall z \in D\). Then \(h(z) = f(z) - g(z))\) is also analytic in \(D\), and \(h(z) = 0\) in the subdomain or along the line segment, so \(h(z) \equiv 0\) throughout \(D\). 
	\end{proof}

	\begin{theorem}[Coincidence Principle]
		\label{Coincidence Principle Theorem - Complex}
		If functions \(f\) and \(g\) are analytic in \(D\) and \(f(z) = g(z)\) in \(D' \subset D\) with limit point \(z_0 \in D\), then \(f(z) = g(z)\) everywhere in \(D\).
		
		This is a more generalized version of \cref{Uniquely determined analytic functions Theorem - Complex}
	\end{theorem}
	
	
	\begin{definition}[Analytic continuation]
		\label{Analytic continuation of a function Definition - Complex}
		Consider the domains \(D_1\) and \(D_2\) with intersection \(D_1 \cap D_2\), and functions \(f_1\) and \(f_2\). If \(f_1\) is analytic in \(D_1\), and there exists \(f_2\) that is analytic in \(D_2\) such that \(f_1(z) = f_2(2)\) for all \(z \in D_1 \cap D_2\). Then \(f_2\) is the analytic continuation of \(f_1\). 
	\end{definition}

	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\begin{axis}[
				width=15cm,
				unit vector ratio=1 1 1,
				xmin=-10, xmax=25,
				ymin=0, ymax=16,
				xtick=100,
				ytick=100,
				axis lines = none,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=none,
				]
				coordinates {
					(3, 8) [$D_1$]
					(17, 8) [$D_2$]
					(10, 3) [$D_3$]
				};
				\draw[dashed, draw=Black] plot [smooth cycle] coordinates {(axis cs: 1,3) (axis cs: 1,10) (axis cs: 12,15) (axis cs: 14,12)};
				\draw[dashed, draw=Black] plot [smooth cycle] coordinates {(axis cs: 20,3) (axis cs: 20,10) (axis cs: 7,15) (axis cs: 6,12)};
				\draw[dashed, draw=Black] plot [smooth cycle] coordinates {(axis cs: 1,7) (axis cs: 1,3) (axis cs: 20,3) (axis cs: 20,7)};
			\end{axis}
		\end{tikzpicture}
	\end{figure}

	\Cref{Uniquely determined analytic functions Theorem - Complex} tells us that if such analytic continuation exists, then it is unique. Now if there exists \(f_3\) in \(D_3\) that is an analytic continuation of \(f_2\), then it is not necessarily true that \(f_3(z) = f_1(z)\) for all \(z \in D_1 \cap D_3\). (See \cref{f_1 neq f_3 in analytic continuation Example - Complex}.)
	
	\begin{definition}[Elements of a function]
		\label{Elements of a function Definition - Complex}
		Let \(f_2\) be the analytic continuation of a function \(f_1\) in \(D_1\) into domain \(D_2\), and let \(F(z)\) be analytic in \(D_1 \cup D_2\).
		\begin{align*}
			F(z) = 
			\begin{cases}
				f_1(z) & z \in D_1 \\
				f_2(z) & z \in D_2
			\end{cases}
		\end{align*}
		Then \(F\) is the analytic continuation of \(f_1\) and \(f_2\) into \(D_1 \cup D_2\), and \(f_1\) and \(f_2\) are elements of \(F\).
	\end{definition}
	
	\subsection{Reflection Principle} \label{Reflection Principle Subsection - Complex}
	
	Generally, \(\overline{f(z)} \neq f(\bar{z})\) for all \(z\), but....
	
	\begin{theorem}[Reflection Principle]
		\label{Reflection Principle Theorem - Complex}
		Let \(f\) be a function with domain \(D\) containing a segment of the real axis \(R \subset D\). Then 
		\begin{align*}
			\forall z \in D[ \overline{f(z)} = f(\bar{z}) ]
			\iff
			\forall x \in R [f(x) \in \mathbb{R}]
		\end{align*}
		See \cref{Reflection Principle f(x) purely imaginary Example - Complex} for the case when \(f(x)\) is purely imaginary.
	\end{theorem}
	\begin{proof} 
		Let \(f(z)\) and \(F(z)\) be analytic functions: 
		\begin{align*}
			f(z) &= u(x,y) + iv(x,y) & F(z) &= U(x,y) + iV(x,y)
		\end{align*}
		\underline{\((\impliedby):\)} \newline
		Suppose \(\forall x \in R[f(x) \in \mathbb{R}]\), and that \(F(z) = \overline{f(\bar{z})}\).
		\begin{align*}
			f(z) &= u(x,y) + iv(x,y) & F(z) &= U(x,y) + iV(x,y)
		\end{align*}
		Then
		\begin{align*}
			\overline{f(\bar{z})} = u(x, -y) - iv(x,-y)
		\end{align*}
		Therefore
		\begin{align*}
			U(x,y) &= u(x,t) & V(x,y) &= -v(x,t) & t = -y
		\end{align*}
		\(f(x,t)\) is analytic, so it satisfies the Cauchy-Riemann equations:
		\begin{align*}
			u_x &= v_t & u_t &= -v_x
		\end{align*}
		Hence 
		\begin{align*}
			U_x &= u_x & V_y &= -v_t \dv{t}{y} = v_t \\
			U_y &= u_t \dv{t}{y} = -u_t & V_x &= -v_x
		\end{align*}
		Thus, we can see that \(F(z)\) also satisfies the Cauchy Riemann equations 
		\begin{align*}
			U_x &= V_y  & U_y = -V_x
		\end{align*}
		Since, the partial derivatives of \(U\) and \(V\) are continuous in \(D\), we can say hat \(F(z)\) is analytic in \(D\). On the segment of the real axis \(R \subset D\), \(f(z)\) is real, so \(v(x,0) = 0\). 
		\begin{align*}
			F(x) &= U(x, 0) + iV(x,0) = u(x,0) - iv(x, 0) = u(x, 0)\\
			&\implies \forall z \in R[F(z) = f(z) ]\\
			&\implies \forall z \in D [\overline{f(\bar{z})} = f(z)] 
				& \Cref{Uniquely determined analytic functions Theorem - Complex}
		\end{align*}
		
		
		\underline{\((\implies):\)} \newline
		Suppose \(\overline{f(z)} = f(\bar{z})\). Then 
		\begin{align*}
			u(x, -y) - iv(x, -y) = u(x,y) + iv(x,y)
		\end{align*}
		Consider any point \((x, 0) \in R \subset D\):
		\begin{align*}
			u(x,0) - iv(x, 0) = u(x, 0) + iv(x, 0) \implies v(x, 0) = 0
		\end{align*}
		Hence, \(f(x)\) is real \(\forall x \in R \subset D\).
	\end{proof}

	\Cref{Reflection Principle Theorem - Complex} tells us that if a complex function is real for all points on the real axis, then it will obey the Reflection Principle, and vice versa.
	
	\subsection{Examples} 
	\begin{example}
		\label{f_1 neq f_3 in analytic continuation Example - Complex}
		Consider 
		\begin{align*}
			f_1(z) &= \sqrt{r} e^{i\theta / 2} & r>0,&\ 0<\theta<\pi \\
			f_2(z) &= \sqrt{r} e^{i\theta / 2} & r>0,&\ \frac{\pi}{2}<\theta<2\pi \\
			f_3(z) &= \sqrt{r} e^{i\theta / 2} & r>0,&\ \pi<\theta<\frac{5\pi}{2} 
		\end{align*}
		{\color{Grey}
		It is clear that \(f_1, \ f_2, \ f_3\) are continuous and satisfies the Cauchy-Riemann equations throughout their domain of definition, since they have a derivative everywhere in their domain of definition. Hence, they are analytic continuations of each other. Let \(D_1, \ D_2\), and \(D_3\) be the domains of \(f_1, \ f_2\), and \(f_3\), respectively. Consider \(f_1\) and \(f_3\) in the domain \(D_1 \cap D_3\), and any \(z\) in the first quadrant of the complex plane. Then \(z = re^{i \theta} = re^{i(\theta + 2\pi)}\)
		and we have
		\begin{align*}
			f_1(z) &= \sqrt{r} e^{i(\theta/2)}  &  0<\theta<\pi \\
			f_3(z) &= \sqrt{r} e^{i(\theta/2 + \pi)}  & 0<\theta<\pi 
		\end{align*}
		Hence, 
		\begin{align*}
			f_1(z) &= \sqrt{r}[\cos(\theta/2) + i\sin(\theta/2)] &  0<\theta<\pi \\
			f_3(z) &= \sqrt{r}[\cos(\theta/2 + \pi) + i\sin(\theta/2+\pi)] & 0<\theta<\pi \\
				   &= - \sqrt{r}[\cos(\theta/2) + i\sin(\theta/2)] 
		\end{align*}
		Thus we can see that \(f_1 = -f_3\) in \(D_1 \cap D_3\).
		}
	\end{example}

	\begin{example}
		\label{Reflection Principle f(x) purely imaginary Example - Complex}
		{\color{Grey}
		Consider \cref{Reflection Principle Theorem - Complex}, but \(f(x)\) is purely imaginary \(\forall x \in \mathbb{R}\). We know that \(\impliedby\) holds, and that \(F(z) = \overline{f(\bar{z})}\) satisfies the Cauchy-Riemann equations. We have 
		\begin{align*}
			F(x) &= U(x,0) + iV(x,0) = u(x,0) - iv(x,0) = -iv(x,0) = -f(x)
		\end{align*}
		Hence
		\begin{align*}
			\overline{f(\bar{z})} = -f(z) \implies \overline{f(z)} = -f(\bar{z})
		\end{align*}
		}
	\end{example}
	
	
	
	\chapter{Elementary Functions} \label{Elementary Functions Chapter - Complex}
	
	\section{Exponential Function} \label{Exponential Function Section - Complex}
	
	\begin{definition}[Exponential Function]
		\label{Exponential Function Definiton - Complex}
		Consider \(z \in \mathbb{C}\), the exponential function is defined:
		\begin{align*}
			f(z) = e^z = e^{x + iy} = e^x [\cos(y) + i\sin(y)]
		\end{align*}
		Where \(y\) is taken in radians. 
	\end{definition}
	Note: This is not the same as the polar form of a complex number (\cref{Polar Form of z - Complex}).
	
	It is clear that the set of \(n\)-th roots of \(e\):
	\[\{e^{1/n} : n \in \mathbb{N} \} \]
	and 
	\begin{align*}
		\abs{e^z} &= e^x & \operatorname{arg}(e^z) &= y + 2n\pi & n \in \mathbb{N}\cup \{0\}
	\end{align*}
	The exponential function follows from the usual properties of exponentials. We also know that 
	\begin{align*}
		\dv{z} e^z &= e^z & \forall z \in \mathbb{C}
	\end{align*}
	so, \(e^z\) is entire. We should also note that \(e^z\) is periodic due to \(e^{iy}\).
	
	\section{Logarithmic Function} \label{Logarithmic Function Section - Complex}
	
	\begin{definition}[Logarithmic Function]
		\label{Logarithmic Function Definition - Complex}
		Consider any \(z \in \mathbb{C}\) in exponential form:
		\begin{align*}
			\log(z) &= \ln(r) + i(\theta + 2n\pi) = \ln(\abs{z}) + i\arg(z) & n \in \mathbb{Z }
		\end{align*}
		Note: This is a multi-valued function.
	\end{definition}
	
	\begin{definition}[Principal Value of the Logarithmic Function]
		\label{Principal Value of the Logarithmic Function Definition - Complex}
		Let \(z \in \mathbb{C}\), the principal value of the logarithmic function is denoted by \(\operatorname{Log}(z)\).
		\begin{align*}
			\operatorname{Log}(z) = \ln(r) + i\theta
		\end{align*}
	\end{definition}
	
	It is clear that 
	\[\log(z) = \operatorname{Log}(z) + 2n \pi \qquad n \in \mathbb{Z}\]
	
	and for any \(z\) on the real axis, the logarithmic function reduces to 
	\[\operatorname{Log}(z) = \ln(x) \qquad x \in \mathbb{R}\]
	
	\subsection{Branches and Derivatives of Logarithms} \label{Branches and Derivatives of Logarithms Subsection - Complex}
	
	\(\log(z)\) is a multi-valued function. Let \(\alpha \in \mathbb{R}\):
	\begin{align*}
		\log(z) &= \ln(r) + i \theta = u(r, \theta) + i v(r,\theta) &
			r>0, \alpha < \theta < \alpha + 2 \pi
	\end{align*}
	
	Note: If \(\log(z)\) is defined on \(\theta = \alpha\), then it is not continuous there, as there is a discontinuity between points near \(\alpha\) and \(\alpha + 2\pi\).
	
	The first order partials of \(u\) and \(v\) are continuous in the domain, and satisfies the Cauchy-Riemann equations:
	\begin{align*}
		ru_r &= v_\theta & u_\theta &= -rv_\theta
	\end{align*}
	So its derivative exists everywhere in the domain. 
	\begin{align*}
		\dv{z} \log(z) &= e^{-i\theta} (u_r + iv_r) = e^{i\theta} \left(\frac{1}{r} + i0\right) = \frac{1}{re^{i\theta}} \\
		&\implies \dv{z} \log(z) = \frac{1}{z} & \abs{z}>0,& \ \alpha < \arg(z) < \alpha + 2\pi\\
		&\implies \dv{z} \operatorname{Log}(z) = \frac{1}{z} & \abs{z}>0,& \ -\pi < \operatorname{Arg}(z) < \pi
	\end{align*}

	\begin{definition}[Branch]
		\label{Branch Definition - Complex}
		A branch is a single-valued function \(F\) of a multi-valued function \(f\). \(F\) is analytic throughout some domain of \(f\) and assumes the one of the values of \(f\).
	\end{definition}
	
	\begin{definition}[Principal Branch]
		\label{Principal Branch Definition - Complex}
		\begin{align*}
			\operatorname{Log}(z) &= \ln(r) + i\theta & r>0, \ -\pi < \theta < \pi
		\end{align*}
	\end{definition}

	\begin{definition}[Branch Cut]	
		\label{Branch Cut Definition - Complex}
		A portion of a line or curved introduced to define a branch \(F\) of a multi-valued function \(f\). Points on the branch cut of \(F\) are singular points of \(F\).
	\end{definition}

	\begin{definition}[Branch Point]
		\label{Branch Point Definition - Complex}
		A singular point common to all branch cuts of a multi-valued function \(f\).
	\end{definition}

	\begin{example}
		The branch cut for \(\operatorname{Log}(z) = \ln(r) + i\theta\), \(r>0,\ -\pi<\theta<\pi\), is the origin and \(\theta = \pi\).
		
		Branch points for all branches of \(\log(z)\) is the origin.
	\end{example}
	
	Different branches may result in different values.
	\begin{example}
		Consider \(\log(i^2)\) in the branch: 
		\begin{align*}
			\log(z) &= \ln(r) + i\theta & r>0, \ \frac{\pi}{4}<\theta<\frac{9\pi}{4}
		\end{align*}
		Then
		\begin{align*}
			\log(i^2) &= \log(-1) = \ln(1) + i\pi = i\pi \\
			2\log(i) &= 2\left(\ln(1) + i\frac{\pi}{2}\right) = \pi i
		\end{align*}
		Therefore
		\begin{align*}
			\log(i^2) &= 2\log(i) & r>0, \ \frac{\pi}{4}<\theta<\frac{9\pi}{4}
		\end{align*}
		Now consider the branch:
		\begin{align*}
			\log(z) &= \ln(r) + i\theta & r>0, \ \frac{3\pi}{4}<\theta<\frac{11\pi}{4}
		\end{align*}
		{\color{Grey}
		Then 
		\begin{align*}
			\log(i^2) &= \log(-1) = \ln(1) + i\pi = i\pi\\
			2\log(i) &= 2\left(\ln(1) + i\frac{5\pi}{2}\right) = 5\pi i
		\end{align*}
		Therefore,
		\begin{align*}
			\log(i^2) &\neq 2\log(i) & r>0, \ \frac{3\pi}{4}<\theta<\frac{11\pi}{4}
		\end{align*}
		}
	\end{example}

	\subsection{Identities of Logarithms} \label{Identities of Logarithms Subsection - Complex}
	
	Let \(z_1, z_2 \in \mathbb{C}\), then 
	\begin{align*}
		\log(z_1 z_2) = \log(z_1) + \log(z_2)
	\end{align*}
	can be interpreted as 
	\begin{align*}
		\arg(z_1 z_2) = \arg(z_1) + \arg(z_2)
	\end{align*}
	therefore
	\begin{align*}
		\ln\abs{z_1 z_2} + i\arg(z_1 z_2) = (\ln\abs{z_1} + i\arg(z_1)) + (\ln\abs{z_2} + i\arg(z_2))
	\end{align*}
	Rest of the identities are the same as for elements in \(\mathbb{R}\), but beware of branches and arguments.
	
	\begin{example}
		Show \(\forall z_1, z_2 \in \mathbb{C}\)
		\begin{align*}
			\operatorname{Log}(z_1 z_2) &= \operatorname{Log}(z_1) + \operatorname{Log}(z_2) + 2N\pi i & N &\in \{0, \pm 1\}
		\end{align*}
		{\color{Grey}
		Consider:
		\begin{align*}
			\log(z_1 z_2) 
			&= \ln\abs{z_1 z_2} + i\arg(z_1 z_2) \\
			&= \ln(r_1) + \ln(r_2) + i\arg(z_1) + i\arg(z_2) \\
			&= \ln(r_1) + \ln(r_2) + i\theta_1 + i\theta_2 + 2n\pi i & \ n\in \mathbb{Z} \\
			&= \ln(r_1) + \ln(r_2) + i\operatorname{Arg}(z_1) + i\operatorname{Arg}(z_2) + 2n \pi i & \ n\in \mathbb{Z}
		\end{align*}
		Then, since \(-\pi < \operatorname{Arg}(z_1)<\pi\) and \(-\pi < \operatorname{Arg}(z_2)<\pi\):
		\begin{align*}
			\operatorname{Log}(z_1 z_2) 
			&= \ln(r_1) + \ln(r_2) + i\operatorname{Arg}(z_1) + i\operatorname{Arg}(z_2) + 2N \pi i & \ N\in \{0, \pm 1\} \\
			&=\operatorname{Log}(z_1) + \operatorname{Log}(z_2) + 2N \pi i & \ N\in \{0, \pm 1\} 
		\end{align*}
		}
	\end{example}

	\subsection{Power Function} \label{Power Function Subsection - Complex}
	
	\begin{definition}[Power Function]
		\label{Power Function Definition - Complex}
		Let \(z, c \in \mathbb{C}\). The Power Function: 
		\begin{align*}
			z^c &= e^{c \log(z)} & z\neq 0
		\end{align*}
		Likewise
		\begin{align*}
			c^z &= e^{z \log(c)} & c\neq 0
		\end{align*}
	\end{definition}
	The logarithm is multi-valued \(\implies\) the power function is multi-valued.
	
	The principle branch of the Power Function is \(\log\) being replaced by \(\operatorname{Log}\):
	\begin{align*}
		z^c &= e^{c \operatorname{Log}(z)} & z\neq 0 \\
		c^z &= e^{z \operatorname{Log}(c)} & c\neq 0
	\end{align*}

	When a branch is specified, \(\log(z)\) becomes single-valued and analytic. Hence the derivatives: 
	\begin{align*}
		\dv{z} z^c &= \dv{z} e^{c\log(z)} = \frac{c}{z} e^{c \log(z)} = cz^{z-1}
		& \abs{z}>0,& \ \alpha < \arg(z) < \alpha + 2 \pi, \ \alpha \in \mathbb{R}
	\end{align*}
	When value of \(\log(c)\) is specified, \(c^z\) is entire function of \(z\) and
	\begin{align*}
		\dv{z}c^z = \dv{z}e^{z\log(c)} = e^{z\log(c)} \log(c) = c^z \log(c)
	\end{align*}
	
	\section{Trigonometric Functions} \label{Trigonometric Functions Section - Complex}
	
	Recall: \Cref{Trig Identities - Complex}. Likewise for any \(z \in \mathbb{C}\):
	
	\begin{definition}[Complex Sine and Cosine Functions]
		\label{Sine and Cosine Functions - Complex}
		For any \(z \in \mathbb{C}\):
		\begin{align*}
			\cos(z) &= \frac{e^{iz} + e^{-iz}}{2} &
			\sin(z) &= \frac{e^{iz} - e^{-iz}}{2i} 
		\end{align*}
	\end{definition}
	Sine and cosine are entire functions as \(e^{iz}\) and \(e^{-iz}\) are entire.
	
	Taking the derivatives:
	\begin{align*}
		\dv{z} e^{iz} = ie^{iz}
		&\implies \left(\dv{z} \sin(z) = \cos{z} \right) \land \left( \dv{z} \cos(z) = -\sin(z)\right)
	\end{align*}
	It's also easy to see that:
	\begin{align*}
		\sin(-z) &= \sin(z)  & \cos(-z) &= -\cos(z) & e^{iz} &= \cos(z) + i\sin(z)
	\end{align*}
	The usual trigonometric identities apply, such as: 
	\begin{align*}
		\sin(z_1 + z_2) &= \sin(z_1) \cos(z_2) + \cos(z_1) \sin(z_2) \\
		\cos(z_1 + z_2) &= \cos(z_1) \cos(z_2) - \sin(z_2) \sin(z_2)
	\end{align*}
	
	Now suppose \(y \in \mathbb{R}\), and take the hyperbolic functions:
	\begin{align*}
		\sinh(y) &= \frac{e^y - e^{-y}}{2} & \cosh(y) &= \frac{e^y + e^{-y}}{2}
	\end{align*}
	Then we get: 
	\begin{align*}
		\sin(iy) &= i\sinh(y) & \cos(iy) &= \cosh(y)
	\end{align*}

	If we let \(z = x + iy\), we can define:
	\begin{align*}
		\sin(z) &= \sin(x) \cosh(y) + i\cos(x) \sinh(y) \\
		\cos(z) &= \cos(x) \cosh(y) - i\sin(x) \sinh(y)
	\end{align*}
	and that 
	\begin{align*}
		\abs{\sin(z)}^2 &= \sin[2](x) + \sinh[2](y)\\
		\abs{\cos(z)}^2 &= \cos[2](x) + \sinh[2](y)
	\end{align*}
	Note: Unlike in \(\mathbb{R}\) where sine and cosine are bounded by \(1\) and \(-1\), 
	it is clear that sine and cosine are not bounded in the complex plane, since \(\sinh\) is unbounded for all values of \(y\). 
	
	\subsection{Zeros and Singularities} \label{Zeros and Singularities (Trig) Subsection - Complex}
	
	\begin{definition}[Zero (function)]
		\label{Zero (function) Definition - Complex}
		Let \(f(z)\) be a function. A zero of \(f\) is a point \(z_0\) such that 
		\[f(z_0) = 0\]
	\end{definition}
	
	\begin{theorem}
		The zeros of \(\cos(z)\) and \(\sin(z)\) for \(z \in \mathbb{C}\) is the same as the zeros of \(\cos(x)\) and \(\sin(x)\) for \(x \in \mathbb{R}\), that is 
		\begin{align*}
			&\forall x \in \mathbb{R} \forall z \in \mathbb{C} \forall n \in \mathbb{Z}
			\left[(\cos(x) = 0) \land (\cos(z) = 0) \iff z = x = \frac{\pi}{2} + n \pi \right] \\
			&\forall x \in \mathbb{R} \forall z \in \mathbb{C} \forall n \in \mathbb{Z}
			\left[(\sin(x) = 0) \land (\sin(z) = 0) \iff z = x = n\pi \right]
		\end{align*}
	\end{theorem}	
	\begin{proof}
		Let \(z = x + iy\) and consider \(\sin(z) = 0\):
		\begin{align*}
			\sin(z) = 0 
			&\implies \sin[2](x) + \sinh[2](y) = 0 & \abs{\sin(z)}^2 = \sin[2](x) + \sinh[2](y) \\
			&\implies [\sin(x) = 0] \land [\sinh(y) = 0] \\
			&\implies [x = n\pi] \land [y = 0] & n \in \mathbb{Z}\\
			&\implies z = x = n \pi & n \in \mathbb{Z}
		\end{align*}
		As for cosine, we know that:
		\begin{align*}
			\cos(z) = \sin \left(z + \frac{\pi}{2}\right)
		\end{align*}
		Thus
		\begin{align*}
			\cos(z) = 0 &\implies z = x = n \pi + \frac{\pi}{2} & n \in \mathbb{Z}
		\end{align*}
	\end{proof}

	\begin{example}
		Show \(\forall z \in \mathbb{C}\):\newline
		The Reflection Principle:
		\begin{align*}
			\forall z \in D\subset \mathbb{C} [\overline{f(z)} = f(\bar{z})] \iff \forall x \in \mathbb{R}[f(x) \in \mathbb{R}]
		\end{align*}
		\begin{itemize}
			\item[(a)] \(\overline{\cos(z)} = \cos(\bar{z})\) 
			\begin{proof}
			{\color{Grey}
				It is clear that \(\forall x \in \mathbb{R}\), \(\sin(x) \in \mathbb{R}\). The result follows from the Reflection Principle. Also
				\begin{align*}
					\overline{\sin(z)} &= \overline{\frac{z - \bar{z}}{2i}} = \frac{\bar{z} 
						- z}{2i} = \sin(\bar{z})
				\end{align*}
			}
			\end{proof}
			
			\item[(b)] \(\overline{\sin(z)} = \sin(\bar{z})\) 
			\begin{proof}
			{\color{Grey}
				It is clear that \(\forall x \in \mathbb{R}\), \(\cos(x) \in \mathbb{R}\). The result follows from the Reflection Principle. Also
				\begin{align*}
					\overline{\cos(z)} &= \overline{\frac{z + \bar{z}}{2}} = \frac{\bar{z} 
						+ z}{2} = \cos(\bar{z})
				\end{align*}
			}
			\end{proof}
		\end{itemize}
	\end{example}

	\begin{example}
		Show: 
		\begin{itemize}
			\item[(a)] \(\forall z \in \mathbb{C}[\overline{\cos(iz)} = \cos(i\bar{z})]\) 
			\begin{proof}
			{\color{Grey}
				\begin{align*}
					\overline{\cos(iz)} 
					&= \overline{\frac{iz + \overline{iz}}{2}} = \frac{\overline{iz} + iz}{2} = \frac{-i\bar{z} + iz}{2} = i\frac{z - \bar{z}}{2} = i \Im{z}\\
					\cos(i\bar{z}) 
					&= \frac{i\bar{z} + \overline{i\bar{z}}}{2} = \frac{i\bar{z} - i z}{2} = i\frac{\bar{z} - z}{2} = i\Im{z}
				\end{align*}
				Hence \(\forall z \in \mathbb{C}\)
				\begin{align*}
					\overline{\cos(iz)} = \cos(i\bar{z})
				\end{align*}
			}
			\end{proof}
			
			\item[(b)] \(\forall z \in \mathbb{C} \forall n \in \mathbb{Z} [\overline{\sin(iz)} = \sin(i\bar{z}) \iff z = n \pi i]\) 
			\begin{proof}
			{\color{Grey}
				\begin{align*}
					\overline{\sin(iz)} 
					&= \overline{\frac{iz - \overline{iz}}{2i}} = \frac{-i\bar{z} - iz}{2i} = -\frac{z + \bar{z}}{2} = -\Re{z} \\
					\sin(i\bar{z})
					&= \frac{i\bar{z} - \overline{i\bar{z}}}{2i} = \frac{i \bar{z} + iz}{2i} = \frac{z + \bar{z}}{2} = \Re{z}
				\end{align*}
				We know that 
				\begin{align*}
					\Re{z} = -\Re{z} \implies \Re{z} = 0 \implies \overline{\sin(iz)} = \sin(i\bar{z}) = 0 \iff z = n\pi i
				\end{align*}
			}
			\end{proof}
		\end{itemize}
	\end{example}

	\section{Hyperbolic Functions} \label{Hyperbolic Functions Section - Complex}
	
	\begin{definition}[Hyperbolic Sine and Cosine Functions]
		\label{Hyperbolic Sine and Cosine Functions Definition - Complex}
		Let \(z \in \mathbb{C}\):
		\begin{align*}
			\sinh(z) &= \frac{e^z - e^{-z}}{2} & \cosh(z) &= \frac{e^z + e^{-z}}{2}
		\end{align*}
	\end{definition}
	
	It is clear that the derivatives: 
	\begin{align*}
		\dv{z} \sinh(z) &= \cosh(z) & \dv{z} \cosh(z) = \sinh(z)
	\end{align*}
	
	The relationships with sine and cosine:
	\begin{align*}
		-i\sinh(iz) &= \sin(z) & \cosh(iz) &= \cos(z) \\
		-i\sin(iz) &= \sinh(z) & \cos(iz) &= \cosh(z)
	\end{align*}
	
	Hence in the complex plane, \(\sinh\) and \(\cosh\) are periodic with period \(2\pi i\).
	
	Identities: 
	\begin{align*}
		\sinh(-z) &= -\sinh(z) & \cosh(-z) &= \cosh(z) & \cosh[2](z) - \sinh[2](z) = 1 
	\end{align*}
	\begin{align*}
		\sinh(z_1 + z_2) &= \sinh(z_1) \cosh(z_2) + \cosh(z_1) \sinh(z_2) \\
		\cosh(z_1 + z_2) &= \cosh(z_2)\cosh(z_2) + \sinh(z_1)\sinh(z_2) \\
		\\
		\sinh(z) &= \sinh(x) \cos(y) + i \cosh(x) \sin(y) \\
		\cosh(z) &= \cosh(x) \cos(y) + i \sinh(x) \sin(y) \\
		\\
		\abs{\sinh(z)}^2 &= \sinh[2](x) + \sin[2](y) \\
		\abs{\cosh(z)}^2 &= \sinh[2](x) + \cos[2](y)
	\end{align*}
	
	\begin{theorem}
		\label{Zeros of Hyperbolic Sine and Cosine Theorem - Complex}
		The zeros of hyperbolic zine and cosine:
		\begin{align*}
			\sinh(z) = 0 &\iff z = n \pi i & n \in \mathbb{Z} \\
			\cosh(z) = 0 &\iff z = \left(\frac{\pi}{2} + n\pi\right)i & n \in \mathbb{Z}
		\end{align*}
	\end{theorem}

	\begin{example}
		Show: 
		\begin{itemize}
			\item[(a)] \(\sinh(z + \pi i) = -\sinh(z)\)
			\begin{proof}
				{\color{Grey}
					\begin{align*}
						\sinh(z + \pi i) 
						&= \frac{e^{z + \pi i} - e^{-z - \pi i}}{2}
						 = \frac{-e^z + e^{-z}}{2}
						 = -\frac{e^z - e^{-z}}{2} = -\sinh(z)
					\end{align*}
				}
			\end{proof}
			\item[(b)] \(\cosh(z + \pi i) = - \cosh(z) \)
			\begin{proof}
				{\color{Grey}
				\begin{align*}
					\cosh(z + \pi i) 
					&= \frac{e^{z + \pi i} + e^{-z - \pi i}}{2} 
					 = -\frac{e^z + e^{-z}}{2} = -\cosh(z)
				\end{align*}
				}
			\end{proof}
			\item[(c)] \(\tanh(z + \pi i) = \tanh(z)\)
			\begin{proof}
				{\color{Grey}
				\begin{align*}
					\tanh(z + \pi i) = \frac{\sinh(z + \pi i)}{\cosh(z + \pi i)} = \frac{-\sinh(z)}{-\cosh(z)} = \tanh(z)
				\end{align*}
				}
			\end{proof}
		\end{itemize}
	\end{example}
	
	\begin{example}
		Show \(\forall z \in \mathbb{C}\):
		\begin{align*}
			\overline{\sinh(z)} &= \sinh(\bar{z}) & \overline{\cosh(z)} &= \cosh(\bar{z}) &
			\forall z \neq 0 &\left[\overline{\tanh(z)} = \tanh(\bar{z})\right]
		\end{align*}
		\begin{proof}
		{\color{Grey}
			We can see that \(\forall x \in \mathbb{R}\), \(\sinh(x) \in \mathbb{R}\) and \(\cosh(x) \in \mathbb{R}\), so we can conclude from the Reflection Principle (\cref{Reflection Principle Theorem - Complex}) that \(\forall z \in \mathbb{C}\):
			\begin{align*}
				\overline{\sinh(z)} &= \sinh(\bar{z}) & \overline{\cosh(z)} &= \cosh(\bar{z})
			\end{align*}
			Thus it follows that 
			\begin{align*}
				\forall z \neq 0 &\left[\overline{\tanh(z)} = \tanh(\bar{z})\right]
			\end{align*}
		}
		\end{proof}
	\end{example}
	
	\section{Inverse Trigonometric and Hyperbolic Functions} \label{Inverse Trigonometric and Hyperbolic Functions Section - Complex}
	
	\begin{definition}[Inverse Trigonometric Functions]
		\label{Inverse Trigonometric Functions Definition - Complex}
		Let \(z \in \mathbb{C}\):
		\begin{align*}
			\sin[-1](z) &= -i \log[iz + (1-z^2)^{1/2}] \\
			\cos[-1](z) &= -i \log[z + i(i-z^2)^{1/2}] \\
			\tan[-1](z) &= \frac{i}{2} \log\left(\frac{i + z}{i - z}\right)
		\end{align*}
		\(\cos[-1](z)\) and \(\tan[-1](z)\) are multi-valued. All inverse trigonometric functions become single-valued and analytic when in specific branches of the square root and logarithmic functions. 
	\end{definition}
	\begin{proof}
	\underline{\(\sin[-1](z) = -i \log[iz + (1-z^2)^{1/2}] \)} \newline
	Let \(w = \sin[-1](z)\) whenever \(z = \sin(w)\)
	\begin{align*}
		z = \sin(w)
		&\implies z = \frac{e^{iw} - e^{-iw}}{2i}
		 \implies (e^{iw})^2 - 2iz e^{iw} - 1 = 0
	\end{align*}
	Using the quadratic formula to solve for \(e^{iw}\): 
	\begin{align*}
		e^{iw} = iz + (1 - z^2)^{1/2} 
		&\implies iw = \log\left( iz + (1 - z^2)^{1/2} \right) \\
		&\implies \sin[-1](z) = -i \log\left[ iz + (1 - z^2)^{1/2} \right]
	\end{align*}

	\underline{\(\cos[-1](z) = -i \log[z + i(i-z^2)^{1/2}]\)} \newline
	{\color{Grey}
	Likewise, let \(w = \cos[-1](z)\) whenever \(z = \cos(w)\)
	\begin{align*}
		z = \cos(w)
		&\implies z = \frac{e^{iw} + e^{-iw}}{2}
		 \implies (e^{iw})^2 - 2ze^{iw} + 1 = 0
	\end{align*}
	Using the quadratic formula to solve for \(e^{iw}\): 
	\begin{align*}
		e^{iw} 
		&= \frac{2z \pm \sqrt{4z^2 - 4}}{2} = z \pm \sqrt{z^2 - 1} = z \pm i (1-z^2)^{1/2} \\
		&\implies iw = \log\left[z \pm i(1-z^2)^{i/2}\right] \\
		&\implies  w = -i\log\left[z \pm i(1-z^2)^{i/2}\right] 
	\end{align*}
	}
	\underline{\(\tan[-1](z) = \frac{i}{2} \log\left(\frac{i + z}{i - z}\right)\)} \newline
	{\color{Grey}
	Again, let \(w = \tan[-1](z)\) whenever \(z = \tan(w)\)
	\begin{align*}
		z &= \tan(w) = \frac{e^{iw} - e^{-iw}}{i(e^{iw} + e^{iw})} \\
		&\implies iz = \frac{e^{iw} - e^{-iw}}{e^{iw} + e^{iw}}
		\implies iz e^{iw} + iz e^{-iw} = e^{iw} - e^{-iw} \\
		&\implies (iz - 1)e^{iw} + (iz + 1)e^{-iw} = 0 
		\implies (iz - 1)e^{2iw} + (iz + 1) = 0 \\
		&\implies e^{iw} = \left(\frac{-iz - 1}{iz - 1}\right)^\frac{1}{2} 
		 \implies iw = \frac{1}{2} \log\left(\frac{-iz - 1}{iz - 1}\right) 
	 	 \implies w = -\frac{i}{2} \log\left(\frac{-(iz + 1)}{iz - 1}\right) \\
		&\implies w = \tan[-1](z) = \frac{i}{2} \log\left(\frac{i + z}{i - z}\right)
	\end{align*}
	}
	\end{proof}
	Derivatives:
	\begin{align*}
		\dv{z} \sin[-1](z) &= \frac{1}{(1-z^2)^{1/2}}
			& &\text{Depends on value coosen for square root} \\
		\dv{z} \cos[-1](z) &= -\frac{1}{(1-z^2)^{1/2}}
			& &\text{Depends on value chosen for square root} \\
		\dv{z} \tan[-1](z) &= \frac{1}{1 + z^2}
			& &\text{Independent on value chosen for square root}
	\end{align*}

	Using the same procedures on the hyperbolic functions, we obtain the inverse hyperbolic functions:
	\begin{definition}[Inverse Hyperbolic Functions]
		\label{Inverse Hyperbolic Functions Definition - Complex}
		Let \(z \in \mathbb{C}\):
		\begin{align*}
			\sinh[-1](z) &= \log\left[z + (z^2 + 1)^{1/2}\right] \\
			\cosh[-1](z) &= \log\left[z + (z^2 - 1)^{1/2}\right] \\
			\tanh[-1](z) &= \frac{1}{2}\log\left(\frac{1+z}{1-z}\right)
		\end{align*}
	\end{definition} 



	\section{Phasors} \label{Phasors Section - Complex}
	
	\begin{definition}[Phasor]
		Consider the function 
		\begin{align*}
			f(t) &= \Re{Fe^{st}} & F &= F_0 e^{i\theta} & s = \sigma + i \omega, \ \sigma, \omega \in \mathbb{R}
		\end{align*}
		\(F\) is the phasor associated with \(f(t)\).
	\end{definition}
	We can see tha \(f(t)\) grows exponentially according to the value of \(\sigma\), has a phase of \(\theta\), and a phase frequency of \(\omega\). (It is clear why engineers love this.)
	
	Properties: 
	\begin{itemize}
		\item[1.] 
		\begin{align*}
			f(t) = \Re{Fe^{st}} \implies 
			\begin{cases}
				F \text{ is unique } & \omega \neq 0 \\
				\text{ Only } \Re{F} \text{ is unique } & \omega = 0
			\end{cases}
		\end{align*}
		\item[2.] 
		\begin{align*}
			\forall t\in \mathbb{R} [f(t) = g(t)] \implies 
			\begin{cases}
				F = G & \omega \neq 0 \\
				\Re{F} = \Re{G} & \omega = 0
			\end{cases}
		\end{align*}
		\item[3.] For any given \(s = \sigma + i \omega \), there is only one function of \(t\) corresponding to a phasor.
		\item[4.] Let \(f(t)\) and \(g(t)\) have the same complex frequency, that is, \(\omega_1 = \omega_2\), then the phasor for \(f(t) + g(t)\) is \(F + G\).
		\item[5.] \(\forall M \in \mathbb{R}\). The phasor for \(Mf(t)\) is \(MF\).
		\item[6.] ``For a given complex frequency, the function of \(t\) corresponding to the sum of two or more phasors is the sum of the time functions for each.'' -Wunsch
		\item[7.] Let \(n \in \mathbb{N}\).
		\begin{align*}
			(f(t) \text{ has phasor } F) \land (\dv*{f}{t} \text{ has phasor } sF) \implies \dv*[n]{f}{t} \text{ has phasor } s^n F
		\end{align*}
		\item[8.] 
		\begin{align*}
			f(t) \text{ has phasor } F &\implies \int^t f(t') dt'  \text{ has phasor } \frac{F}{s} & s\neq 0
		\end{align*}
	\end{itemize}
	These properties follow from the properties of \(e\).
	
	\begin{example}
		Consider:
		\begin{align*}
			R\mathtt{i}(t) + L \dv{\mathtt{i}}{t} = V_0 \cos(\omega t)
		\end{align*}
		Suppose the complex frequency is \(s = i\omega\). If \(I\) is the phasor for \(\mathtt{i}\), then substituting it into the differential equation:
		\begin{align*}
			RI + iwLI &= (R + i \omega L)I = V_0 & \text{Phasors on both side must equal}
		\end{align*}
		Then solving for the phasor:
		\begin{align*}
			I &= \frac{V_0}{R + i\omega L} = \frac{V_0 e^{i\theta}}{\sqrt{R^2 + \omega^2 L^2}} & \theta = - \tan[-1](\frac{\omega L}{R}) 
		\end{align*}
		Using \(s = i\omega\) to obtain \(\mathtt{i}(t)\), we have
		\begin{align*}
			\mathtt{i}(t) = \Re{\frac{V_0 e^{i\theta}}{\sqrt{R^2 + (\omega L)^2}}e^{i\omega t}}
		\end{align*}
	\end{example}

	\chapter{Integrals} \label{Integrals Chapter - Complex}
	
	\section{Derivatives of Functions} \label{Derivatives of Functions Section - Complex}
	
	\begin{definition}[Derivative of Complex-Valued Function]
		\label{Derivative of Complex-Valued Function Definition - Complex}
		Consider a complex-valued function \(w(t) = u(t) + iv(t)\), with \(u\) and \(v\) being real-valued functions. If \(w(t)\) is differentiable at \(t\), then it's derivative with respect to \(t\):
		\begin{align*}
			w'(t) = \dv{t} w(t) = u'(t) + iv'(t)
		\end{align*}
	\end{definition}
	The rules for calculus in \(\mathbb{R}\) still applies.
	
	Note: The mean value theorem for derivatives no longer apply for complex-valued functions. 
	\begin{example}
		Let \(w(t)= e^{it}\) be continuous on \([0, 2\pi]\), so \(u(t)\) and \(v(t)\) are also continuous on \([0,2\pi]\). For the mean value theorem to hold, there must exist \(a, b, c \in \mathbb{C}\), where \(a < c < b\), such that 
		\begin{align*}
			w'(c) = \frac{w(b) - w(a)}{b-a}
		\end{align*}
		We can see that 
		\begin{align*}
			\abs{w'(t)} &= \abs{ie^{it}} = 1 &
			\frac{w(b) - w(a)}{b-a} &= \frac{w(2\pi) - w(0)}{2\pi - 0} = \frac{e^{i2\pi} - e^{i0}}{2\pi} = \frac{1 - 1}{2\pi} = 0
		\end{align*}
		So we can see that there does not exist a \(c \in \mathbb{C}\) such that the mean value theorem holds.
	\end{example}
	
	\section{Definite Integrals of Functions} \label{Definite Integrals of Functions Section - Complex}
	
	\begin{definition}[Definite Integral of Complex-Valued Function]
		\label{Definite Integral of Complex-Valued Function Definition - Complex}
		Consider a complex-valued function \(w(t) = u(t) + iv(t)\), with \(u\) and \(v\) being real-valued functions. If \(u\) and \(v\) are piecewise continuous on interval \([a,b]\), then the definite integral of \(w(t)\) over interval \([a, b]\):
		\begin{align*}
			\int_{a}^{b} w(t) dt = \int_{a}^{b} u(t) dt + i \int_{a}^{b} v(t) dt
		\end{align*}
	\end{definition}
	The rules for integrals in \(\mathbb{R}\) and the Fundamental Theorem of Calculus still applies.
	
	Likewise with derivatives of complex-valued functions, the mean value theorem does not hold for complex-valued integrals.
	\begin{example}
		Let \(w(t) = e^{it}\) be a complex-valued function of \(t\). For \(w(t)\) to hold on \([a, b]\), this must hold for some \(a < c < b\):
		\begin{align*}
			\int_{a}^{b} w(t) dt = w(c)(b-a)
		\end{align*}
		Consider \(w(t)\) on \([0, 2\pi]\). Then
		\begin{align*}
			\int_{a}^{b} w(t) dt 
			&= \int_{0}^{2\pi} e^{it} dt = \eval{\frac{e^it}{i}}_{0}^{2\pi} = 0 &
			\abs{w(c)(b-a)} &= \abs{e^{ic}} 2 \pi = 2\pi
		\end{align*}
		 We can see there does not exist \(c\), \(0 < c < 2\pi\), such that both sides of the equations are equal. 
	\end{example}
	
	\section{Contours} \label{Contours Section - Complex}
	
	\begin{definition}[Arc]
		\label{Arc Definition - Complex}
		An arc is a set of points dependent on a parameter \(t \in \mathbb{R}\).
		\begin{align*}
			\{z = (x(t),y(t)) : t \in [a,b]\}
		\end{align*}
		Where \(x(t)\) and \(y(t)\) are continuous functions. 
	\end{definition}
	It is convenient in \(\mathbb{C}\) to use: 
	\begin{align*}
		 z = z(t) &= x(t) + iy(t)  & a \leq t \leq b
	\end{align*}
	
	\begin{definition}[Simple/Jordan Arc]
		\label{Simple/Jordan Arc Definition - Complex}
		An arc is simple if it does not cross itself. That is:
		\(t_1 \neq t_2 \implies z(t_1) \neq z(t_2)\)
	\end{definition}
	
	\begin{definition}[Simple Closed Curve / Jordan Curve]
		A simple curve, but endpoints gets mapped to equal values. That is, \(z(a) = z(b)\) for \(a \leq t \leq b\).
		It is positively oriented if it is counterclockwise. 
	\end{definition}

	The interval for which the arc is parameterized is not unique. Consider 
	\begin{align*}
		t &= \phi(\tau) & \alpha \leq \tau \leq \beta
	\end{align*}
	Then \(\phi(\alpha) = a\) and \(\phi(\beta)=b\). We have 
	\begin{align*}
		z(t) &= Z(\tau) = z[\phi(\tau)]& \alpha \leq \tau \leq \beta
	\end{align*}
	
	\begin{definition}[Differentiable Arc]
		\label{Differentiable Arc Definition - Complex}
		Suppose \(\dv*{t}z(t) = z'(t) = x'(t) + iy'(t)\) exists and is continuous. Then \(z'(t)\) is a differentiable arc.
	\end{definition}
	
	We can integrate over the differential arc in the interval \([a,b]\):
	\begin{align*}
		L &= \int_{a}^{b} \abs{z'(t)} dt	&	\abs{z'(t)} &= \sqrt{[x'(t)]^2 + [y'(t)]^2}
	\end{align*}
	Note: \(\abs{z'(t)}\) is a real-valued function. 
	
	Again, due to the curve being invariant under the representation for the arc:
	\begin{align*}
		L &= \int_{a}^{b} \abs{z'(t)} dt = \int_{\alpha}^{\beta} \abs{z'[\phi(t)]} \phi'(t) dt = \int_{\alpha}^{\beta} \abs{Z'(\tau)} d\tau 
			& Z'(\tau) &= z'[\phi(\tau)] \phi'(\tau)
	\end{align*} 

	If the differentiable arc \(z'(t) \neq 0\) in the interval \([a,b]\), then the unit tangent vector is defined in said interval: 
	\begin{align*}
		\vu{T} = \frac{z'(t)}{\abs{z'(t)}}
	\end{align*}
	Recall: The gradient of a function is perpendicular to the function, so \(\vu{T}\) is  normal to \(z(t)\), over the interval \([a,b]\).
	
	\begin{definition}[Smooth]
		\label{Smooth Definition - Complex}
		An arc \(z(t)\) is smooth in the interval \([a,b]\) if \(z'(t)\) is continuous \(\forall t \in [a,b]\) and non-zero \(\forall t \in (a,b)\).
	\end{definition}
	
	\begin{definition}[Contour]
		\label{Contour Definition - Complex}
		A piecewise smooth arc.
	\end{definition}

	\begin{definition}[Simple Closed Contour]
		\label{Simple Closed Contour Definition - Complex}
		A contour where only \(z(a) = z(b)\) in the interval \([a,b]\).
	\end{definition}
	
	\begin{theorem}[Jordan Curve Theorem]
		\label{Jordan Curve Theorem - Complex}
		All points on a simple close curve or simple closed contour \(z(t)\) are boundary points of two distinct domains. One is bounded and interior to \(z(t)\) and the other is unbounded and exterior to \(z(t)\).  That is, \(z(t)\) as a boundary line of two domains. 
	\end{theorem}
	\begin{proof}
		Good luck!
	\end{proof}
	i.e. If \(z(t)\) is a circle, then one domain is within the circle and contains the points on the parameter, and one is exterior to the circle.
	
	\section{Contour Integrals} \label{Contour Integrals Section - Complex}
	
	Evaluating an integral over a contour. 
	
	\begin{definition}[Contour Integral]
		\label{Contour Integral Definition - Complex}
		Let \(z(t)\) in \(C = [a, b]\) be a contour, and \(f[z(t)]\) be a piecewise continuous function on \(C\). Then the contour integral:
		\begin{align*}
			\int_{C} f(z) dz = \int_{a}^{b} f[(z(t))] z'(t) dt
		\end{align*}
		Note: \(C\) is contour \(\implies z'(t)\) is piecewise continuous on \(C \implies\) Existence of integral on \(C\)
	\end{definition}
	
	Notation: 
	Let \(f(z)\) be a function evaluated over the contour \(C\).
	\begin{align*}
		\int_{C} f(z) dz &&& \int_{z_1}^{z_2} f(z) dz
	\end{align*}
	\(\int_{z_1}^{z_2}\) is often used when the integral is independent of the path between end points. \(-C\) represents the same contour, but in reverse.
	
	Following from the contours, the integral is invariant under change in representation of the contour. 
	
	\begin{definition}[Sum (Contour)]
		\label{Sum (Contour) Definition - Complex}
		Let \(C_1\) be contour from \(z_1\) to \(z_2\), and \(C_2\) be from \(z_2\) to \(z_3\), then the sum is contour \(C\) from \(z_1\) to \(z_3\).
	\end{definition}

	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\begin{scope}[
				thick,
				decoration={
					markings,
					mark=at position 0.3 with {\arrow{>}},
					mark=at position 0.8 with {\arrow{>}}}
				] 
				
				\node[label=below:$z_1$] (A) at (0,0) {};
				\node[label=below:$z_3$] (B) at (5,1) {};
				\node[label=below:$z_2$] (C) at (2.5,0.5) {};
				\node[label=below:$C_1$] (D) at (1.25,0.66) {};
				\node[label=below:$C_2$] (E) at (3.75,0.33) {};
				
				\draw[thick,postaction={decorate}] plot [smooth] coordinates {(0,0) (1.25,0.66) (2.5,0.5) (3.75,0.33) (5,1)};
			\end{scope}
			
			\draw [fill=black] (A) circle (1pt);
			\draw [fill=black] (B) circle (1pt);
			\draw [fill=black] (C) circle (1pt);  
		\end{tikzpicture}
	\end{figure}
	
	Some properties (which follows from integrals):
	\begin{align*}
		\int_{C} = z_0 f(z) dz &= z_0 \int_{C} f(z) dz &
		\int_{C} f(z) + g(z) dz &= \int_{C} f(z) dz + \int_{C} g(z) dz 
	\end{align*}
	\begin{align*}
		\int_{-C} f(z) dz 
		&= \int_{-b}^{-a} f([z(-t)]) \dv{t} z(-t) dt = -\int_{-b}^{-a} f[z(-t)] z'(-t) dt \\
		&= -\int_{-b}^{-a} f[z(\tau)] z'(\tau) d\tau \\
		&= -\int_{C} f(z) dz
	\end{align*}
	\begin{align*}
		\int_{a}^{b} f[z(t)] z'(t) dt &= \int_{a}^{c} f[z(t)] z'(t) dt + \int_{c}^{b} f[z(t)] z'(t) dt\\
		\int_{C} f(z) dz &= \int_{C_1} f(z) dz + \int_{C_2} f(z) dz
	\end{align*}
	
	\subsection{Upper Bounds for the Moduli} \label{Upper Bounds for the Moduli Subsection - Complex}
	
	It is not analysis without inequality involving modulus.
	
	\begin{lemma}
		Let \(w(t)\) be a piecewise smooth function defined on \([a,b]\). Then
		\begin{align*}
			\abs{ \int_{a}^{b} w(t) dt} \leq \int_{a}^{b} \abs{w(t)} dt
		\end{align*}
	\end{lemma}
	\begin{proof}
		Assume \(\int_{a}^{b} w(t) dt\) is non-zero, otherwise the inequality is trivial. 
		\begin{align*}
			\int_{a}^{b} w(t) dt = r_0 e^{i\theta_0}
				&\implies r_0 = e^{-i\theta_0} \int_{a}^{b} w(t) dt\\
				&\implies r_0 = \Re{e^{-i\theta_0} \int_{a}^{b} w(t) dt} & r_0 &\in \mathbb{R} \\
				&\implies r_0 = \int_{a}^{b} \Re{e^{i\theta_0} w(t)} dt
		\end{align*}
		Now
		\begin{align*}
			\Re{e^{-i\theta_0} w(t)} 
			&\leq \abs{e^{-i\theta_0} w(t)} = \abs{e^{-i\theta_0}} \abs{w(t)} = \abs{w(t)} \\
			&\implies r_0 \leq \int_{a}^{b} \abs{w(t)} dt \\
			&\implies \abs{ \int_{a}^{b} w(t) dt} \leq \int_{a}^{b} \abs{w(t)} dt
				& r_0 = \abs{ \int_{a}^{b} w(t) dt}
		\end{align*}
	\end{proof}
	
	\begin{theorem}
		\label{Upper Bound for Moduli of Contour Integral Theorem - Complex}
		Let \(f(z)\) be piecewise continuous function on contour \(C\) with length \(L\).
		\begin{align*}
			\forall z \in C \ \exists M \in \mathbb{R} 
			\left[\abs{f(z)} \leq M \right] \implies \abs{\int_{C} f(z) dz} \leq ML 
		\end{align*}
		That is, if \(f(z)\) is bounded on the contour, then the value of it's integral is bounded. 
	\end{theorem}
	\begin{proof}
		Let \(z = z(t)\) in \([a, b]\) be a parametric representation of \(C\). Then
		\begin{align*}
			\abs{\int_{C} f(z) dz} = \abs{\int_{a}^{b} f[z(t)] z'(t) dt} 
			&\leq \int_{a}^{b} \abs{f[z(t)] z'(t)} dt = \int_{a}^{b}  \abs{f[z(t)]} \abs{z'(t)} dt  \\
			&\leq M \int_{a}^{b} z'(t) dt = M z(t) = ML
		\end{align*}
	\end{proof}

	Note: According to the Extreme Value Theorem, any continuous real-valued function on a closed interval is bounded, so such \(M \in \mathbb{R}\) will always exist. 
	
	\begin{observation}
		Let \(l\) be the length along the contour \(C\). Graphically, what this is telling us:
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}
				\begin{axis}[
					unit vector ratio=1 1 1,
					xlabel={$l$},
					ylabel={$f(z)$},
					xmin=0, xmax=10,
					ymin=-1, ymax=6,
					xtick=100,
					ytick=100,
					axis lines = middle,
					axis line style = Black,
					]
					\addplot[
						only marks,
						nodes near coords,
						point meta = explicit symbolic, 
						mark=none,
						]
						coordinates {
							(9, 2.5) [$M$]
							(4.5, 5) [$L$]
						};
					
					\draw[thick] plot [smooth] coordinates {
						(axis cs: 0,-1) 
						(axis cs: 1.25,3) 
						(axis cs: 2.5,1) 
						(axis cs: 3.75,2) 
						(axis cs: 5,5) 
						(axis cs: 7,0) 
						(axis cs: 9, -1)
					};
					
					\draw[dashed] plot [smooth] coordinates {
						(axis cs: 0,5) 
						(axis cs: 9,5) 
					};
				
					\draw[dashed] plot [smooth] coordinates {
						(axis cs: 9,5) 
						(axis cs: 9,0) 
					};
				
				\end{axis}
%				\begin{scope}[
%					thick
%					] 
%					\draw[thick] plot [smooth] coordinates {(0,-1) (1.25,0.66) (2.5,0.5) (3.75,0.33) (5,1) (7,0) (8, -1)};
%				\end{scope} 
			\end{tikzpicture}
		\end{figure}
	\end{observation}

	This is useful in evaluating the size of the integral, and if we are lucky:

	\begin{example}
		Let \(C_R\) be the semicircle:
		\begin{align*}
			z &= Re^{i\theta} & 0 \leq \theta \leq \pi, \ R>3
		\end{align*}
		Consider 
		\begin{align*}
			\lim_{r \rightarrow \infty} \int_{C_R} \frac{z+1}{(z^2 + 4)(z^2 + 9)} dz 
		\end{align*}
		We know that:
		\begin{align*}
			\abs{z+1} &\leq \abs{z} + 1 = R + 1\\
			\abs{z^2 + 4} &\geq \abs{\abs{z}^2 - 4} = R^2 - 4\\
			\abs{z^2 + 9} &\geq \abs{\abs{z^2} - 9} = R^2 - 9
		\end{align*}
		Then 
		\begin{align*}
			\abs{f(z)} 
			= \abs{\frac{z+1}{(z^2 + 4)(z^2 + 9)}} = \frac{\abs{z+1}}{\abs{z^2 + 4} \abs{z^2 + 9}} 
			\leq \frac{R+1}{(R^2 - 4) (R^2 - 9)}  = M_R
		\end{align*}
		Now we have
		\begin{align*}
			M_R &= \frac{R+1}{(R^2 - 4) (R^2 - 9)} & L = \pi R
		\end{align*}
		Since
		\begin{align*}
			\abs{\int_{C_R} \frac{z+1}{(z^2 + 4)(z^2 + 9)} dz} \leq M_R L 
		\end{align*}
		Hence
		\begin{align*}
			\lim_{R \rightarrow \infty} M_R L 
			= \lim_{R \rightarrow \infty} \frac{R^2+R}{(R^2 - 4) (R^2 - 9)} \pi
			= \lim_{R \rightarrow \infty} \frac{\frac{1}{R^2} + \frac{1}{R^3}}{\left(1 - \frac{4}{R^2}\right) \left(1-\frac{9}{R^2}\right)} \pi
			= 0
		\end{align*}
		Thus
		\begin{align*}
			\lim_{r \rightarrow \infty} \int_{C_R} \frac{z+1}{(z^2 + 4)(z^2 + 9)} dz = 0
		\end{align*} 
	\end{example}
	
	\section{Antiderivatives} \label{Antiderivatives Section - Complex}
	
	\begin{definition}[Antiderivative]
		\label{Antiderivative Definition - Complex}
		Let \(f(z)\) be a continuous function on domain \(D\), the antiderivative is a function \(F(z)\) such that 
		\begin{align*}
			F'(z) &= f(z) & \forall z \in D
		\end{align*}
	\end{definition}
	Note: By definition, \(F(z)\) is an analytic function, and an antiderivative is unique up to an additive constant. An indefinite integral is the family of functions that are the antiderivative of a particular function. 
	
	\begin{theorem}
		\label{Antiderivative and contour integrals theorem - Complex}
		Let \(f(z)\) be a continuous function on domain \(D\). TFAE:
		\begin{itemize}
			\item[(a)] There is a function \(F(z)\) such that 
			\[\forall z \in D [F'(z) = f(z)]\]
			(\(f(z)\) has an antiderivative throughout \(D\).)
			\item[(b)] All contours of \(f(z)\) in \(D\) from any point \(z_1\) to \(z_2\) all have the same value. That is 
			\begin{align*}
				\int_{z_1}^{z_2} f(z) dz = \eval{F(z)}_{z_1}^{z_2} = F(z_2) - F(z_1)
			\end{align*}
			\item[(c)] For all closed contours \(C\) lying in \(D\):
			\begin{align*}
				\oint_{C} f(z) dz = 0
			\end{align*}
		\end{itemize}
		Simply:
		\begin{align*}
			\forall z \in D [F'(z) = f(z)] 
			\iff \int_{z_1}^{z_2} f(z) dz = \eval{F(z)}_{z_1}^{z_2}
			\iff \oint_{C} f(z) dz = 0
		\end{align*}
	\end{theorem}
	\begin{proof}
		\underline{(a) \(\implies\) (b):} \newline
		Suppose \(F'(z)\) exists for \(f(z)\) for all \(z \in D\). We know:
		\begin{align*}
			\dv{t} F[z(t)] &= F'(z(t)) z'(t) = f[z(t)]z'(t) & t \in [a,b]
		\end{align*}
		Then 
		\begin{align*}
			\int_{C} f(z) dz = \int_{a}^{b} f[z(t)] z'(t) dt = \eval{F[z(t)]}_{a}^{b} = F[z(b)] - F[z(a)] = F(z_2) - F(z_1) 
		\end{align*}
		If \(C\) consists of a finite number of smooth arc \(C_k\), \(k \in \{1, 2, 3, \ldots, n\}\):
		\begin{align*}
			\int_{C} f(z) dz = \sum_{k=1}^{n} \int_{C} f(z) dz = \sum_{k=1}^{n} \int_{z_k}^{z_{k+1}} f(z) dz = \sum_{k=1}^{n}[F(z_{k+1}) - F(z_k)]
 		\end{align*}
		Then 
		\begin{align*}
			\int_{C} f(z) dz = F(z_{n+1}) - F(z_1)
		\end{align*}
		Thus 
		\begin{align*}
			\forall z \in D [F'(z) = f(z)] 
			\implies \int_{z_1}^{z_2} f(z) dz = \eval{F(z)}_{z_1}^{z_2}
		\end{align*}
		
		\underline{(b) \(\implies\) (c):} \newline
		Let \(C_1\) and \(C_2\) be contours with endpoints \(z_1\) and \(z_2\). Suppose that integration is path independent, then
		\begin{align*}
			\int_{C_1} f(z) dz = \int_{C_2} f(z) dz
			\implies \int_{C_1} f(z) dz + \int_{-C_2} f(z) dz = 0 
			\implies \int_{C = C_1 - C_2} f(z) dz = 0
		\end{align*}
		Thus 
		\begin{align*}
			\int_{z_1}^{z_2} f(z) dz = \eval{F(z)}_{z_1}^{z_2}
			\implies \oint_{C} f(z) dz = 0
		\end{align*}
		
		\underline{(c) \(\implies\) (a):} \newline
		Suppose integration around a closed contour \(C\) in \(D\) is zero. Since integration is path independent in \(D\), we can define the function:
		\begin{align*}
			F(z) = \int_{z_0}^{z} f(z) ds
		\end{align*}
		Let \(z + \Delta z\) be any point in the neighbourhood of \(z\) contained in \(D\), then
		\begin{align*}
			F(z + \Delta z ) - F(z) = \int_{z_0}^{z + \Delta z} f(z) ds - \int_{z_0}^{z} f(z) ds = \int_{z}^{z + \Delta z} f(s) ds
		\end{align*}
		Since
		\begin{align*}
			\int_{z}^{z + \Delta z} ds = \Delta z
			&\implies f(z) = \frac{1}{\Delta z} \int_{z}^{z + \Delta z} f(z) ds 
		\end{align*}
		Then 
		\begin{align*}
			\frac{F(z + \Delta z) - F(z)}{\Delta z} - f(z) 
			= \frac{1}{\Delta z} \int_{z}^{z + \Delta z} f(s) - f(z) ds
		\end{align*}
		As \(f\) is continuous at the point \(z\):
		\begin{align*}
			\forall \epsilon \exists \delta [\abs{s - z} < \delta \implies \abs{f(s) - f(z)} < \epsilon]
		\end{align*}
		If \(\abs{\Delta z}<\delta\):
		\begin{align*}
			\abs{\frac{F(z + \Delta z) - F(z)}{\Delta z} - f(z)} < \frac{1}{\abs{\Delta z}} \epsilon \abs{\Delta z} = \epsilon
		\end{align*}
		Then 
		\begin{align*}
			\lim_{\Delta z \rightarrow 0}
			\frac{F(z + \Delta z) - F(z)}{\Delta z} = f(z) \implies F'(z) = f(z)
		\end{align*}
		Thus
		\begin{align*}
			\oint_{C} f(z) dz = 0 \implies \forall z \in D [F'(z) = f(z)] 
		\end{align*}
	\end{proof}
	
	\begin{example}
		Let \(f(z) = z^{-2}\). We can see that \(f\) is continuous everywhere except at the origin, and has antiderivative \(F(z) = -z^{-1}\) in \(\abs{z}>0\). Thus around the unit circle:
		\begin{align*}
			\int_{C} z^{-2} dz &= 0 & z=e^{i\theta}, \ \theta \in [-\pi, \pi]
		\end{align*}
	\end{example}

	\begin{example}
		Consider \(f(z) = z^{-1}\). It has an antiderivative \(F(z) = \log(z)\), which is not differentiable or defined along its branch cut. To evaluate the integral along the unit circle, we can break it up into two domains to avoid this issue. First consider \(C_1\):
		\begin{align*}
			z &= e^{i\theta} & -\frac{\pi}{2} \leq \theta \leq \frac{\pi}{2}
		\end{align*}
		Then  
		\begin{align*}
			\int_{C_1} z^{-1} dz
			&= \int_{-i}^{i} z^{-1} dz = \eval{\operatorname{Log}(z)}_{-i}^{i} = \operatorname{Log}(i) - \operatorname{Log}(-i) \\
			&= \left(\ln(1) + i\frac{\pi}{2}\right) - \left(\ln(1) - i\frac{\pi}{2}\right) = \pi i
		\end{align*}
		Now consider \(C_2\):
		\begin{align*}
			z &= e^{i\theta} & \frac{\pi}{2} \leq \theta \leq \frac{3\pi}{2}
		\end{align*}
		Then 
		\begin{align*}
			\int_{C_2} = z^{-1} dz 
			&= \int_{i}^{-i} z^{-1} dz = \eval{\log(z)}_{i}^{-i} = \log(-i) - \log(i) \\
			&= \left(\ln(1) + i\frac{3\pi}{2}\right) - \left(\ln(1) + i\frac{\pi}{2}\right) = \pi i
		\end{align*}
		Thus around the circle \(C = C_1 + C_2\):
		\begin{align*}
			\int_{C} z^{-1} dz = \int_{C_1} z^{-1} dz + \int_{C_2} z^{-1} dz = \pi i + \pi i = 2 \pi i
		\end{align*}
	\end{example}
	
	\section{Cauchy-Goursat Theorem} \label{Cauchy-Goursat Theorem Section - Complex}
	
	Previously: A function \(f\) that has an antiderivative in any domain \(D\), then the integral of \(f\) around any closed contour in \(D\) is zero. (\Cref{Antiderivative and contour integrals theorem - Complex}) Now, it's for simple closed contours. 
	
	\begin{theorem}[Cauchy-Goursat Theorem]
		\label{Cauchy-Goursat Theorem - Complex}
		Let \(C\) be a simple closed contour. If a function \(f\) is analytic for all set of points \(z\) on and in \(C\), then 
		\begin{align*}
			\int_{C} f(z) dz = 0
		\end{align*}
	\end{theorem}
	\begin{proof}
		\underline{First, a lemma:} \newline
		\begin{lemma}
			Let \(C\) be a closed contour, \(R\) denote the region enclosed and on the contour, and \(f\) be a function analytic in \(R\). The region \(R\) can be covered by a finite number of squares or partial squares, indexed \(j = 1, 2, \ldots, n\), such that for some \(\epsilon > 0\): 
			\begin{align*}
				\abs{\frac{f(z) - f(z_j)}{z - z_j} - f'(z_j)} < \epsilon
			\end{align*}
			Holds for all points \(z\) other than a fixed point \(z_j\) in that square or partial square. We let a square denote a region with boundary points included with points interior to it. If a square has points not in \(R\), then we remove those points and it becomes a partial square.
		\end{lemma}
		\begin{proof}
			Suppose there does not exist a \(z_j\) where 
			\begin{align*}
				\abs{\frac{f(z) - f(z_j)}{z - z_j} - f'(z_j)} < \epsilon
			\end{align*}
			holds after subdividing a square a finite number of times for contradiction.
			Let \(\sigma_0\) denote the original square or the entire square of the partial square, \(\sigma_1\) denote the squares after subdividing \(\sigma_0\) into four equal smaller squares, and so on. After subdividing \(\sigma_0\), one of the \(\sigma_1\) must contain points of \(R\) but still no such \(z_j\) exists, so we continue to subdivide such \(\sigma_1\) since the inequality does not hold. We will then obtain an infinite sequence
			\[\sigma_0, \sigma_1, \sigma_2, \ldots, \sigma_{k-1}, \sigma_k, \ldots\]
			There is a point \(z_0\) that is common to each of these squares and each of these squares contain points of \(R\) other than \(z_0\). As the size of the squares are decreasing, there exists a neighbourhood \(\delta > \abs{z - z_0}\) containing the squares with diagonals less than \(\delta\), so each neighbourhood \(\delta\) contains points of \(R\) distinct from \(z_0\). Thus \(z_0\) is an accumulation point of \(R\) (\cref{Accumulation/Limit Point Definition - Complex}), and since \(R\) is a closed set, \(z_0 \in R\).
			
			Now, since \(f\) is analytic in \(R\) and \(z_0\), \(f'(z_0)\) exists and according to \cref{Derivative Definition - Complex}:
			\begin{align*}
				\forall \epsilon \exists \delta >0 
				\left[\abs{z - z_0}< \delta \implies \abs{\frac{f(z) - f(z_0)}{z - z_0} - f'(z_0)} < \epsilon \right]
			\end{align*}
			However, such neighbourhood \(\abs{z-z_0}<\delta\) contains \(\sigma_K\) for some sufficiently large \(K\), so \(z_0\) serves as \(z_j\) for a the subregion of \(\sigma_K\) or part of \(\sigma_K\), thus there is no need to subdivide \(\sigma_K\). We have reached a contradiction.
		\end{proof}
	
		\underline{Upper bound for modulus of an integral:} \newline \newline
		Given some \(\epsilon\) we cover region \(R\) with squares such that
		\begin{align*}
			\abs{\frac{f(z) - f(z_j)}{z - z_j} - f'(z_j)} < \epsilon
		\end{align*}
		We define a neighbourhood \(\delta_j(z)\) enclosing the \(j\)-th square or partial square by
		\begin{align*}
			\delta_j(z) = 
			\begin{cases}
				\frac{f(z) - f(z_j)}{z-z_j} - f'(z_j) & z \neq z_j \\
				0 & z = z_j
			\end{cases}
		\end{align*}
		Then 
		\begin{align*}
			\forall z \in \sigma \subset R [\abs{\delta_j (z)} < \epsilon]
		\end{align*}
		As \(f(z)\) is continuous throughout subregion \(\sigma\),  \(\delta_j(z)\) is continuous in \(\sigma\) and 
		\begin{align*}
			\lim_{z \rightarrow z_j} \delta_j(z) = f'(z_j) - f'(z_j) = 0
		\end{align*}
		Now, let \(C_j\) denote the positively oriented contours on the boundaries of the squares and partial squares covering \(R\). Then on any \(C_j\) be definition of \(\delta_j(z)\):
		\begin{align*}
			&f(z) = f(z_j) - z_j f'(z_j) + f'(z_j) z + (z - z_j) \delta_j(z) \\
			&\implies \int_{C_j} f(z) dz = [f(z_j) - z_j f'(z_j)] \int_{C_j} dz + f'(z_j) \int_{C_j} z dz + \int_{C_j} (z - z_j) \delta_j(z) dz
		\end{align*}
		However, according to \cref{Antiderivative and contour integrals theorem - Complex}:
		\begin{align*}
			\int_{C_j} dz &= 0 & \int_{C_j} z dz &= 0
		\end{align*}
		So
		\begin{align*}
			&\int_{C_j} f(z) dz = \int_{C_j} (z - z_j) \delta_j(z) dz & j = 1, 2, 3, \ldots, n \\
			&\implies \sum_{j=1}^{n} \int_{C_j} f(z) dz  = \sum_{j=1}^{n} \int_{C_j} (z - z_j) \delta_j (z) dz
		\end{align*}
		Now as boundaries of adjacent subregions cancel each other out, since they are taken along opposite senses to each other, only those on \(C\) remain, so
		\begin{align*}
			&\int_{C} f(z) dz = \sum_{j=1}^{n} \int_{C_j} (z - z_j) \delta_j (z) dz  
			\implies \abs{\int_{C} f(z) dz} \leq \sum_{j=1}^{n} \abs{\int_{C_j} (z - z_j) \delta_j (z) dz}
		\end{align*}
	
		\underline{Endgame:} \newline \newline
		Let \(s_j\) denote the length of the sides of the square or partial square \(\sigma_j\), since \(C_j\) is on the boundary or part of the boundary of the square.
		\begin{align*}
			\abs{z - z_j} &\leq \sqrt{2} s_j & \sqrt{2} s_j \text{ is diagonal of square}
		\end{align*}
		Then 
		\begin{align*}
			\abs{\delta_j(z)} < \epsilon 
			\implies \abs{(z - z_j) \delta_j(z)} = \abs{z - z_j} \abs{\delta_j(z)} < \sqrt{2} s_j \epsilon
		\end{align*}
		Let \(A_j\) be the area of the square. If \(C_j\) is the boundary of a square, then the length of \(C_j\) is \(4s_j\)  and we have
		\begin{align*}
			\abs{\int_{C_j} (z - z_j) \delta_j (z) dz} < \sqrt{2}s_j \epsilon 4 s_j = 4\sqrt{2} A_j \epsilon
		\end{align*}
		Now, if \(C_j\) is the boundary of a partial square, then the length of \(C_j\) is less than \(4s_j + L_j\), where \(L_j\) is the length of \(C_j\) that is a part of \(C\). Let \(S\) be the length of the sides of some square that entirely encloses \(C\), so sum of \(A_j\) is less then \(S^2\). Then we have: 
		\begin{align*}
			\abs{\int_{C_j} (z - z_j) \delta_j(z) dz} < \sqrt{2} s_j \epsilon (4s_j + L_j) < 4 \sqrt{2} A_j \epsilon + \sqrt{2} SL_j \epsilon
		\end{align*}
		If \(L\) is the length of \(C\), then
		\begin{align*}
			\abs{\int_{C} f(z) dz} 
			&\leq \sum_{j=1}^{n} \abs{\int_{C_j} (z - z_j) \delta_j (z) dz} 
			< \sum_{j=1}^{n} \left( 4 \sqrt{2} A_j \epsilon + \sqrt{2} SL_j \epsilon \right) \\
			&< \left(4 \sqrt{2} S^2 + \sqrt{2 SL} \right) \epsilon
		\end{align*}
		Since \(\epsilon\) is arbitrary, we can choose it to be as small as we like, so 
		\begin{align*}
			\forall \epsilon > 0
			\left[\abs{\int_{C} f(z) dz} < \left(4 \sqrt{2} S^2 + \sqrt{2 SL} \right) \epsilon \right] \implies \abs{\int_{C} f(z) dz} = 0
		\end{align*}
		Hence, if function \(f\) is analytic on all \(z \in C\) where \(C\) is a simple closed contour, then
		\begin{align*}
			\int_{C} f(z) dz = 0
		\end{align*}
	
		\underline{\textbf{TLDR:}} \newline 
		We found that the upper bound for \(f\) around the contour integral \(C\) is less than or equal to the sum of all the contours around the squares covering the region bounded by \(C\). Since \(f\) is analytic in \(R\), the sum of the contours of the squares in \(R\) is a function of the neighbourhood \(\delta_j (z)\) surrounding \(z_j\) in each square, which is chosen to be less than some \(\epsilon\), the error between the derivative of \(f\) and the finite difference of \(f\). 
		As \(\epsilon\) can be made arbitrary small and the inequality must hold for all values of \(\epsilon\), we find \(\int_{C} f(z) dz = 0\).
	\end{proof}

	\subsection{Simply Connected Domains} \label{Simply Connected Domains Subsection - Complex}
	
	\begin{definition}[Simply Connected Domain]
		\label{Simply Connected (Domain) Definition - Complex}
		A domain \(D\) which every simple closed contour that lies within it only encloses points in \(D\).
	\end{definition}

	\begin{theorem}
		\label{Contour integral in simply connected domain Theorem - Complex}
		Let \(f\) be a function that is analytic throughout a simply connected domain \(D\). Then for every closed contour \(C\) lying in \(D\):
		\begin{align*}
			\int_{C} f(z) dz = 0
		\end{align*}
		We will later learn in \cref{Continuity on closed contour implies and zero contour integral implies analycity Theorem - Complex} that this is \(\iff\), due to \cref{Antiderivative and contour integrals theorem - Complex}.
	\end{theorem}
	\begin{proof}
		Suppose \(C\) is simple and lies entirely in \(D\). The result follows from the Cauchy-Goursat theorem (\cref{Cauchy-Goursat Theorem - Complex}).
		
		Suppose that \(C\) is closed, but intersects itself a finite number of times, then it consists of a finite number of simple closed contours. Result again follows from the Cauchy-Goursat Theorem. 
		
		Note: There are subtleties for infinite number of self-intersection points. 
	\end{proof}

	\begin{corollary}
		If \(f\) is a function analytic throughout a simply connected domain \(D\), then it has antiderivatives everywhere in \(D\).
	\end{corollary}
	\begin{proof}
		If \(f\) is analytic in a simply connected domain \(D\) then it is continuous in \(D\). Then
		\begin{align*}
			\int_{C} f(z) dz = 0 &\iff \forall z \in D \ \exists F(z)[F'(z) = f(z)] 
				&\Cref{Antiderivative and contour integrals theorem - Complex}
		\end{align*}
	\end{proof}

	\begin{corollary}
		Entire functions have antideriviatives everywhere in their domain of definition.
	\end{corollary}
	\begin{proof}
		Consequence of previous corollary and that finite plane is simply connected.
	\end{proof}

	\subsection{Multiply Connected Domains} \label{Multiply Connected Domains Subsection - Complex}
	
	\begin{definition}[Multiply Connected Domains]
		A domain that is not simply connected.
	\end{definition}

	\begin{theorem}
		\label{Contour integral over Multiply Connected Domains Theorem - Complex}
		Let \(C\) be a simple closed contour in the positive direction, and \(C_k\) (\(k \in \{1,2,3,\ldots, n\}\)) be simple closed contours in \(C\) taken in the negative direction that hare disjoint with no common interior points. If a function \(f\) is analytic on \(C\) and \(C_k\) and throughout the multiply connected domain consisting of points inside \(C\) but exterior to all \(C_k\), then
		\begin{align*}
			\int_{C} f(z) dz + \sum_{k=1}^{n} \int_{C_k} f(z) dz = 0
		\end{align*}
	\end{theorem}
	\begin{proof}
		Let a polygonal path \(L_1\) connect \(C\) to the inner contour \(C_1\), \(L_2\) connecting \(C_1\) to \(C_2\), and continue in this manner. Finally, let \(L_{n+1}\) connect \(C_n\) to \(C\). Then we have two contours \(\Gamma_1\) and \(\Gamma_2\). \(\Gamma_1\) consisting of parts of the contours \(C\), \(C_k\), and \(L_k\). \(\Gamma_2\) consisting of the remaining parts of contours \(C\), \(C_k\), and \(-L_k\). If we apply the Cauchy-Goursat theorem to \(\Gamma_1\) and \(\Gamma_2\), then
		\begin{align*}
			\int_{\Gamma_1} f(z) dz + \int_{\Gamma_2} f(z) dz = 0
		\end{align*}
		Now, since the integrals along \(L_k\) cancel (due to being taken in the opposite direction), only integrals along \(C\) and \(C_k\) remain. Hence
		\begin{align*}
			&\int_{\Gamma_1} f(z) dz + \int_{\Gamma_2} f(z) dz = 0 \\
			&\implies \int_{C} f(z) dz + \sum_{k=1}^{n+1} \int_{L_k} f(z) dz - \sum_{k=1}^{n+1} \int_{L_k} f(z) dz + \sum_{k=1}^{n} \int_{C_k} f(z) dz = 0 \\
			&\implies \int_{C} f(z) dz + \sum_{k=1}^{n} \int_{C_k} f(z) dz = 0
		\end{align*}
	\end{proof}

	\begin{observation}
		Basically, imagine a slice of Swiss cheese. We took a knife and cut a single path though all of the holes in order. Another way of saying it is that we cut through each hole only once. This way we end up with two slices each consisting a part of the outer edge of the original slice, a part of the edge of the holes, and the edges introduced by our cut. Since we have cut through all of the holes, our two slices will not have holes so we can integrate along the outer edge of each of those slices. The Cauchy-Goursat theorem tells us that the value for the sum will be zero, since they are now simply connected domains. 
	\end{observation}
	
	\begin{question}
		We pretty much end up with two simply connected domains, right? If so then shouldn't it be:
		\begin{align*}
			\left( \int_{\Gamma_1} f(z) dz = 0 \right) \land \left( \int_{\Gamma_2} f(z) dz = 0 \right) \implies \int_{\Gamma_1} f(z) dz + \int_{\Gamma_2} f(z) dz = 0
		\end{align*}
		As apposed to just 
		\begin{align*}
			\int_{\Gamma_1} f(z) dz + \int_{\Gamma_2} f(z) dz = 0
		\end{align*}
		which does not imply
		\begin{align*}
			\left( \int_{\Gamma_1} f(z) dz = 0 \right) \land \left( \int_{\Gamma_2} f(z) dz = 0 \right)
		\end{align*}
	\end{question}

	\begin{corollary}[Principle of Deformation of Paths]
		\label{Principle of Deformation of Paths Corollary - Complex}
		Let \(C_1\) and \(C_2\) be positively oriented simple closed contours, with \(C_1\) interior to \(C_2\). If a function \(f\) is analytic on \(C_1\), \(C_2\), and the regions between them, then 
		\begin{align*}
			\int_{C_1} f(z) dz = \int_{C_2} f(z) dz
		\end{align*}
	\end{corollary}
	\begin{proof}
		It follows from \cref{Contour integral over Multiply Connected Domains Theorem - Complex} that 
		\begin{align*}
			\int_{C_1} f(z) dz - \int_{-C_2} f(z) dz = 0
			\implies \int_{C_1} f(z) dz = \int_{C_2} f(z) dz
		\end{align*}
	\end{proof}


	\section{Cauchy Integral Formula} \label{Cauchy Integral Formula Section - Complex}
	
	\begin{theorem}[Cauchy Integral Formula]
		\label{Cauchy Integral Formula}
		Let a function \(f\) be analytic on and inside a simple closed contour \(C\) oriented positively. Then for all \(z_0\) interior to \(C\):
		\begin{align*}
			f(z_0) = \frac{1}{2\pi i} \int_{C} \frac{f(z)}{z - z_0} dz
		\end{align*}
		That is, if \(f\) is analytic within and on a simple closed contour \(C\), then values of \(f\) interior to \(C\) is determined by values of \(f\) on \(C\).
	\end{theorem}
	\begin{proof}
		Let  \(C\) be a positively oriented contour and \(z_0\) be any point interior to \(C\). Let \(C_\rho\) be a positively oriented circular contour lying inside \(C\) centred at \(z_0\). That is, \(C_\rho\) lies on points \(\abs{z - z_0} = \rho\). Then from \cref{Principle of Deformation of Paths Corollary - Complex}, we can write: 
		\begin{align*}
			&\int_{C} \frac{f(z)}{z-z_0} dz = \int_{C_\rho} \frac{f(z)}{z-z_0} dz  \\
			&\implies \int_{C} \frac{f(z)}{z-z_0} dz - f(z_0) \int_{C_\rho} \frac{1}{z-z_0} dz = \int_{C} \frac{f(z) - f(z_0)}{z-z_0} dz 
		\end{align*}
		Now
		\begin{align*}
			\int_{C_\rho} \frac{1}{z-z_0} dz 
			&= \int_{C_\rho} \frac{1}{re^{i\theta}} ire^{i\theta} d\theta & z = z_0 + re^{i\theta} \\
			&= \int_{C_\rho} i d\theta = 2\pi i
		\end{align*}
		So
		\begin{align*}
			\int_{C} \frac{f(z)}{z - z_0}dz - 2\pi i f(z_0) = \int_{C_\rho} \frac{f(z) - f(z_0)}{z - z_0} dz
		\end{align*}
		Since \(f\) is analytic, thus continuous:
		\begin{align*}
			\forall \epsilon>0, \exists \delta>0 [\abs{z-z_0}<\delta \implies \abs{f(z) - f(z_0)}<\epsilon]
		\end{align*}
		Now, \(\abs{z - z_0} = \rho < \delta\) for all \(z\) on \(C_\rho\), so according to \cref{Upper Bound for Moduli of Contour Integral Theorem - Complex}:
		\begin{align*}
			\abs{\int_{C_\rho} \frac{f(z) - f(z_0)}{z - z_0} dz} &< \frac{\epsilon}{\rho} (2 \pi \rho) = 2 \pi \epsilon & \abs{\frac{f(z) - f(z_0)}{z - z_0}} < \frac{\epsilon}{\delta}
		\end{align*}
		Then
		\begin{align*}
			\abs{\int_{C} \frac{f(z)}{z - z_0}dz - 2\pi i f(z_0)} < 2 \pi \epsilon
		\end{align*}
		This inequality must hold for all values of \(\epsilon > 0\), so
		\begin{align*}
			\int_{C} \frac{f(z)}{z - z_0}dz - 2\pi i f(z_0) = 0 \implies \int_{C} \frac{f(z)}{z - z_0} dz = 2 \pi i f(z_0)
		\end{align*}
	\end{proof}
	
%	\subsection{Extension} \label{Cauchy Integral Formula (Extension) Subsection - Complex}
	
	The Cauchy Integral Theorem links \(f(z_0)\) to a contour integral. The extension of the Cauchy Integral Formula links the \(n\)-th derivative of \(f\), \(f^{(n)}(z_0)\), to the contour integral of \(f\) at \(z_0\).
	\begin{theorem}[Cauchy Integral Formula (Extension)]
		\label{Cauchy Integral Formula (Extension) - Complex}
		Let a function \(f\) be analytic on and inside a simple closed contour \(C\) oriented positively. Then for all \(z_0\) interior to \(C\):
		\begin{align*}
			f^{(n)}(z_0) &= \frac{n!}{2\pi i} \int_{C} \frac{f(z)}{(z-z_0)^{n+1}} dz & n \in \mathbb{N} \cup \{0\}
		\end{align*}
	\end{theorem}
	\begin{proof}
		Proof that is not a proof, but a verification in Brown and Churchill \cite{Brown.J;Churchill.R-Complex-Variables-2014}. \newline
		Taking the original Cauchy Integral formula:
		\begin{align*}
			f(z) = \frac{1}{2\pi i} \int_{C} \frac{f(s)}{s-z} ds
			&\implies f'(z) = \frac{1}{2\pi i} \int_{C} f(z) \pdv{z}(s-z)^{-1} ds = \frac{1}{2\pi i} \int_{C} \frac{f(s)}{(s-z)^2} ds
		\end{align*}
		Continued differentiation under the integral sign yields the desired result...or does it? Verification is needed. 
		
		\underline{\textbf{Verification}} \newline
		Let \(z\) be any point interior to a simple closed contour \(C\), and \(d\) denote the smallest distance from \(z\) to points \(s\) on \(C\). Assume \(0 < \abs{\Delta z} < d\), then 
		\begin{align*}
			\frac{f(z + \Delta z) - f(s)}{\Delta z} 
			&= \frac{1}{2 \pi i} \int_{C} \left(\frac{1}{s - z - \Delta z} - \frac{1}{s - z}\right) \frac{f(s)}{\Delta z} ds \\
			&= \frac{1}{2\pi i} \int_{C} \frac{f(s)}{(s - z - \Delta z)(s-z)} ds
		\end{align*}
		Now
		\begin{align*}
			\frac{1}{(s-z-\Delta z)(s-z)} = \frac{1}{(s-z)^2} + \frac{\Delta z}{(s-z-\Delta z)(s-z)^2}
		\end{align*}
		Hence
		\begin{align*}
			\frac{f(z + \Delta z) - f(s)}{\Delta z} - \frac{1}{2\pi i} \int_{C} \frac{f(s)}{(s-z)^2} ds = \frac{1}{2\pi i} \int_{C} \frac{f(s)\Delta z}{(s-z-\Delta z)(s-z)^2} ds
		\end{align*}
		Let \(M = \max{\abs{f(s)}}\) on \(C\), since \(\abs{s-z} \geq d\) and \(\abs{\Delta z}<d\):
		\begin{align*}
			\abs{s-z-\Delta z} = \abs{(s-z) - \Delta z} \geq \abs{\abs{s-z} - \abs{\Delta z}} \geq d - \abs{\Delta z} > 0
		\end{align*}
		Then letting \(L\) be the length of \(C\) and using \cref{Upper Bound for Moduli of Contour Integral Theorem - Complex}:
		\begin{align*}
			\abs{\int_{C} \frac{f(s) \Delta z}{(s-z-\Delta z)(s-z)^2}} 
			\leq \frac{\abs{\Delta z} M}{(d - \abs{\Delta z}) d^2} L
		\end{align*}
		Taking the limit \(\Delta z \rightarrow 0\):
		\begin{align*}
			\lim_{\Delta z \rightarrow 0} \abs{\int_{C} \frac{f(s) \Delta z}{(s-z-\Delta z)(s-z)^2}} 
			\leq  \lim_{\Delta z \rightarrow 0} \frac{\abs{\Delta z} M}{(d - \abs{\Delta z}) d^2} L = 0
		\end{align*}
		Hence
		\begin{align*}
			\lim_{\Delta z \rightarrow 0} \frac{f(z + \Delta z) - f(s)}{\Delta z} - \frac{1}{2\pi i} \int_{C} \frac{f(s)}{(s-z)^2} ds 
			= \lim_{\Delta z \rightarrow 0} \frac{1}{2\pi i} \int_{C} \frac{f(s)\Delta z}{(s-z-\Delta z)(s-z)^2} ds = 0
		\end{align*}
		Thus 
		\begin{align*}
			\lim_{\Delta z \rightarrow 0} \frac{f(z + \Delta z) 
			- f(s)}{\Delta z} = \frac{1}{2\pi i} \int_{C} \frac{f(s)}{(s-z)^2} ds 
		\end{align*}
		By induction, we get: 
		\begin{align*}
			f^{(n)}(z) &= \frac{n!}{2 \pi i} \int_{C} \frac{f(s)}{(s-z)^{n+1}} ds 
				& n \in \mathbb{N} \cup \{0\}
		\end{align*}
	\end{proof}
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\begin{axis}[
				width=10cm,
				unit vector ratio=1 1 1,
				xlabel={$x$},
				ylabel={$y$},
				xmin=0, xmax=25,
				ymin=0, ymax=16,
				xtick=100,
				ytick=100,
				axis lines = center,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(14, 8) [$z$]
					(16.05, 10.64) []
					(18, 2.75)  [$s$]
					(11.5, 8) [$z + \Delta z$]
				};
				\draw[draw=Black] plot [smooth cycle] coordinates {(axis cs: 2,7) (axis cs: 2,3) (axis cs: 20,3) (axis cs: 20,7) (axis cs: 10,14.2)};
				\draw[-, draw=Black]  (axis cs: 14,8)--(axis cs: 16.05,10.64)
				node[midway,sloped,above] {$d$};
				\draw[-, draw=Black]  (axis cs: 14,8)--(axis cs: 18,2.75)
				node[midway,sloped,above] {$\abs{s-z}$};
				\draw[-, draw=Black]  (axis cs: 14,8)--(axis cs: 11.5,8)
				node[midway,sloped,below] {$\abs{\Delta z}$};
				\draw[-{>[scale=2.0]}]  (axis cs: 3.8, 9.3)--(axis cs: 3.6, 9.05)
				node[midway,sloped,above] {$C$};
			\end{axis}
		\end{tikzpicture}
	\end{figure}
	
	\begin{example}
		We can then rewrite the Legendre Polynomials:
		\begin{align*}
			P_n(z) &= \frac{1}{n! 2^n} \dv[n]{z} (z^2 - 1)^2 = \frac{1}{2^{n+1} \pi i} \int_{C} \frac{(s^2 - 1)^n}{(s-z)^{n+1}} ds & n \in \mathbb{N} \cup \{0\}
		\end{align*}
	\end{example}
	
	\subsection{Consequences} \label{Cauchy Integral Theorem Consequences Subsection - Complex}
	
	\begin{theorem}
		\label{Analytic implies existance of high order derivatives theorem - Complex}
		Let a function \(f\) be analytic at a point \(z_0\), then \(f^{(n)}\) exists at \(z_0\) for all \(n \in \mathbb{N}\). That is, the derivative of \(f\) of all orders are analytic at \(z_0\).
	\end{theorem}
	\begin{proof}
		Suppose a function \(f\) is analytic at point \(z_0\), then there exists a neighbourhood \(\epsilon > \abs{z - z_0}\) where \(f\) is analytic. By extension, there is a positively oriented circular contour \(C_0\) centred at \(z_0\) with radius \(\epsilon / 2\) where \(f\) is analytic on and inside \(C_0\).
		Then by \cref{Cauchy Integral Formula (Extension) - Complex}:
		\begin{align*}
			f''(z) &= \frac{1}{\pi i} \int_{C_0} \frac{f(s)}{(s-z)^3} ds & \forall z \text{ interior to } C_0
		\end{align*}
		The existence of of \(f''(z)\) in \(\abs{z-z_0}<\epsilon\) \(\implies\) \(f'\) is analytic at \(z_0\). The same argument on \(f'\) implies \(f''\) is also analytic.
	\end{proof}
	
	Note: Suppose \(f(z) = u(x,y) + iv(x,y)\) is analytic at \(z = (x,y)\). Then it is also continuous: 
	\begin{align*}
		f(z) = u(x,y) + iv(x,y) 
		&\implies [f'(z) = u_x + iv_x = v_y - iu_y] \land [f' \text{ is continuous }] \\
		&\implies [f''(z) = u_{xx} + iv_{xx} = v_{xy} - iu_{yx}] \land [f'' \text{ is continuous}]
 	\end{align*}
	
	\begin{corollary}
		Let a function \(f(z) = u(x,y) + iv(x,y)\) be analytic at a point \(z_0\). Then \(u\) and \(v\) have continuous partial derivatives of all orders at \(z_0\). 
	\end{corollary}

	\begin{theorem}
		\label{Continuity on closed contour implies and zero contour integral implies analycity Theorem - Complex}
		Let a function \(f\) be continuous on domain \(D\), and \(C\) be any closed contour lying in \(D\).
		\begin{align*}
			\forall C \left[\int_{C} f(z) dz = 0 \right]
			\implies f \text{ is analytic throughout } D
		\end{align*}
		If \(D\) is simply connected, this is the converse of \cref{Contour integral in simply connected domain Theorem - Complex}.
	\end{theorem}
	\begin{proof}
		\begin{align*}
			f \text{ is continous in } D
			&\implies \forall z \in D, \exists F(z) [F'(z) = f(z)] 
				& \Cref{Antiderivative and contour integrals theorem - Complex}\\
			&\implies f \text{ is analytic in }  D
				& \Cref{Analytic implies existance of high order derivatives theorem - Complex}
		\end{align*}
	\end{proof}

	\begin{theorem}[Cauchy's Inequality]
		\label{Cauchy's Inequality Theorem - Complex}
		Let a function \(f\) be analytic on and inside a positively oriented circular contour \(C_R\) centered at \(z_0\) with radius \(R\). 
		\begin{align*}
			M_R = \eval{\max\abs{f(z)}}_{C_R} &\implies \abs{f^{(n)}(z_0)} \leq \frac{n! M_R}{R^n}
				& n \in \mathbb{N}
		\end{align*}
	\end{theorem}
	\begin{proof}
		From \cref{Upper Bound for Moduli of Contour Integral Theorem - Complex}:
		\begin{align*}
			&f^{(n)}(z_0) = \frac{n!}{2\pi i} \int_{C_R} \frac{f(z)}{(z-z_0)^{n+1}} & n \in \mathbb{N}\\
			&\implies \abs{f^{(n)}(z_0)} \leq \frac{n!}{2\pi} \cdot \frac{M_R}{R^{n+1}} \cdot 2 \pi R = \frac{n! M_R}{R^n} & \abs{z-z_0}\leq R, \  n \in \mathbb{N}
		\end{align*}
	\end{proof}
	
	\section{Liouville's Theorem and the Fundamental Theorem of Algebra} \label{Liouville's Theorem and the Fundamental Theorem of Algebra Section - Complex}
	
	\begin{theorem}[Liouville's Theorem]
		\label{Liouville's Theorem - Complex}
		Let \(f\) be a function in the complex plane
		\begin{align*}
			f \text{ is entire and bounded in } \mathbb{C} 
			\implies f(z) \text{ is constant in } \mathbb{C}
		\end{align*}
	\end{theorem}
	\begin{proof}
		\(f\) is entire so \(\forall z \in \mathbb{C}\) \(f'(z)\) exists. Then From \cref{Cauchy's Inequality Theorem - Complex}, and \(f\) being bounded:
		\begin{align*}
			\abs{f'(z_0)} &\leq \frac{M_R}{R} = \frac{M}{R}&  n=1, \forall z \in C,\exists M[\abs{f(z)} \leq M]
		\end{align*}
		This inequality must hold for all values of \(R\) (\(R\) can be arbitrarily large), so we find: 
		\begin{align*}
			\abs{f'(z_0)} = 0 \implies f \text{ is constant}
		\end{align*}
	\end{proof}
	
	\begin{observation}
		Liouville's Theorem implies that any non-constant function in \(\mathbb{C}\) is either not entire or unbounded. (See \cref{Maximum Modulus Principle Theorem - Complex})
	\end{observation}

	\begin{question}
		Shouldn't Liouville's theorem be \(\iff\) since a constant function is also entire and bounded?
	\end{question}
	
	\begin{theorem}[Fundamental Theorem of Algebra]
		\label{Fundamental Theorem of Algebra - Complex}
		Let \(P(z) = \sum_{i=0}^{n} a_i z^i\) be any polynomial, then
		\begin{align*}
			\forall n \in \mathbb{N}, \exists z_0 \in \mathbb{C}[P(z_0) = 0]
		\end{align*}
	\end{theorem}
	\begin{proof}
		Suppose for contradiction \(\nexists z_0 \in \mathbb{C}\) such that \(P(z_0) = 0\)
		Recall from \cref{Complex Poly Reciprocal Bounded Corollary - Complex} that \(\exists R \in \mathbb{R}\) such that 
		\begin{align*}
			\abs{\frac{1}{P(z)}} &< \frac{2}{\abs{a_n} R^n} & \forall z \in \mathbb{C} [\abs{z} > R]
		\end{align*}
		\(1/P(z)\) is bounded on for \(\abs{z}>R\), but \(P(z)\) is continuous on \(\abs{z}\leq R\), which implies that \(1/P(z)\) is bounded for \(\abs{z}\leq R\). Thus \(P(z)\) is bounded in the entire complex plane. (\Cref{Function continuous in closed and bounded region implies function is bounded Theorem - Complex})
		It follows from \cref{Liouville's Theorem - Complex} that \(1/P(z)\) is constant \(\implies P(z)\) is constant, but \(P(z)\) is not constant. Contradiction!
	\end{proof}

	\Cref{Fundamental Theorem of Algebra - Complex} tells us that any polynomial of degree \(n\geq 1\) can be expressed as a product of linear factors:
	\begin{align*}
		P(z) = c \prod_{i = 1}^{i=n} (z-z_i)
	\end{align*}
	Since the existence of a zero \(z_1\) implies 
	\begin{align*}
		P(z) &= (z-z_1)Q_1(z) & \deg (Q_1) = n-1
	\end{align*}
	Result follows from induction.
	
	\section{Maximum Modulus Principle} \label{Maximum Modulus Principle Section - Complex}
	
	\begin{theorem}[Gauss's Mean Value Theorem]
		\label{Gauss's Mean Value Theorem - Complex}
		Let \(f\) be a function analytic in and on a circular contour \(C_\rho\) centred at \(z_0\), then \(f(z_0)\) is the arithmetic mean of the values on the circle:
		\begin{align*}
			f(z_0) &= \frac{1}{2\pi i} \int_{0}^{2\pi} f(z_0 + \rho e^{i\theta}) d\theta 
			& z=z_0 + \rho e^{i\theta}, \ 0\leq \theta \leq 2\pi
		\end{align*}
		That is, the value of \(f(z_0)\) is the average of the values of \(f(z)\) in some neighbourhood with radius \(\rho\) around \(z_0\).
	\end{theorem}
	\begin{proof}
		Let \(C_\rho\) be a circular contour centred at \(z_0\), and \(\abs{z-z_0} = \rho\). Then by the Cauchy Integral formula (\cref{Cauchy Integral Formula}):
		\begin{align*}
			f(z_0) &= \frac{1}{2\pi i} \int_{C_\rho} \frac{f(z)}{z-z_0} dz = \frac{1}{2\pi i} \int_{0}^{2\pi} f(z_0 + \rho e^{i\theta}) d\theta 
			& z=z_0 + \rho e^{i\theta}, \ 0\leq \theta \leq 2\pi
		\end{align*}
	\end{proof}
	
	\begin{lemma}
		\label{Analytic function is bounded in neighbourhood implies constant value thoughout neighbourhood Lemma - Complex}
		Let \(f\) be a function analytic in some neighbourhood \(\abs{z-z_0}<\epsilon\):
		\begin{align*}
			\forall z \in \{z : \abs{z-z_0}<\epsilon\}
			[\abs{f(z)} \leq \abs{f(z_0)}] \implies \forall z \in \{\abs{z-z_0}<\epsilon\} [f(z) = f(z_0)]
		\end{align*}
		That is, if \(f\) is bounded by its value at \(z_0\) in the neighbourhood of \(z_0\), then it is constant throughout the neighbourhood with value \(f(z_0)\).
	\end{lemma}
	\begin{proof}
		Following \cref{Gauss's Mean Value Theorem - Complex}, we have 
		\begin{align*}
			f(z_0) = \frac{1}{2\pi i} \int_{0}^{2\pi} f(z_0 + \rho e^{i\theta}) d\theta 
			\implies \abs{f(z_0)} \leq \frac{1}{2\pi} \int_{0}^{2\pi} \abs{f(z_0 + \rho e^{i\theta})} d\theta
		\end{align*}
		However, since we have condition \(\abs{f(z)} =  \abs{f(z_0 + \rho e^{i\theta})} \leq \abs{f(z_0)}\):
		\begin{align*}
			&\int_{0}^{2\pi} \abs{f(z_0 + \rho e^{i\theta })} d \theta 
			 \leq \int_{0}^{2\pi} \abs{f(z_0)} d\theta = 2\pi \abs{f(z_0)} 
			&  \ 0 \leq \theta \leq 2 \pi \\
			&\implies \abs{f(z_0)} \geq \frac{1}{2\pi} \int_{0}^{2\pi} \abs{f(z_0 + \rho e^{i\theta })} d \theta 
		\end{align*}
		The inequalities tells us:
		\begin{align*}
			\abs{f(z_0)} = \frac{1}{2\pi} \int_{0}^{2\pi} \abs{f(z_0 + \rho e^{i\theta})} d\theta 
			\implies \int_{0}^{2\pi} \abs{f(z_0)} - \abs{f(z_0 + \rho e^{i\theta})} d\theta = 0
		\end{align*}
		Our condition \(\abs{f(z)} =  \abs{f(z_0 + \rho e^{i\theta})} \leq \abs{f(z_0)}\) tells us that 
		\begin{align*}
			\int_{0}^{2\pi} \abs{f(z_0 + \rho e^{i\theta})} d\theta \leq \int_{0}^{2\pi} \abs{f(z_0)} d\theta 
			\implies \int_{0}^{2\pi} \abs{f(z_0)} - \abs{f(z_0 + \rho e^{i\theta})} d\theta \geq 0
		\end{align*}
		So for \(\int_{0}^{2\pi} \abs{f(z_0)} - \abs{f(z_0 + \rho e^{i\theta})} d\theta = 0\), the integrand must be zero:
		\begin{align*}
			&\abs{f(z_0 + \rho e^{i\theta})} - \abs{f(z_0)} = 0 & 0\leq \theta \leq 2\pi \\
			&\implies \forall z \in \{z : \abs{z - z_0} = \rho\}[\abs{f(z)} = \abs{f(z_0)}]
		\end{align*}
		Since \(0 < \abs{z - z_0} < \epsilon\) and \(\abs{f(z)} = \abs{f(z_0)}\) for all \(0<\rho<\epsilon\), we have \(\abs{f(z)} = \abs{f(z_0)}\) for \(\abs{z-z_0} < \epsilon\). We know that if a function is analytic in a domain and its modulus is constant in the domain, then the function is constant (\cref{Abs of analytic function is constant in domain implies that function is constant Example - Complex}), thus 
		\begin{align*}
			\forall z \in \{z : \abs{z-z_0}<\epsilon\} [f(z) = f(z_0)]
		\end{align*}
		That is \(f(z)\) is constant in the neighbourhood \(\abs{z-z_0}<\epsilon\) with value \(f(z_0)\).
	\end{proof}
	
	\begin{theorem}[Maximum Modulus Principle]
		\label{Maximum Modulus Principle Theorem - Complex}
		Let \(f\) be an analytic function in a domain \(D\).
		\begin{align*}
			f \text{ not constant in } D \implies \forall z \in D, \nexists z_0 \in D [\abs{f(z)} \leq \abs{f(z_0)} = M]
		\end{align*}
		That is, if an analytic function is not constant in a domain \(D\), then it is not bound.
	\end{theorem}
	\begin{proof}
		Suppose for contradiction \(f\) is bounded in domain \(D\). That is \(\forall z \in D [\abs{f(z)} \leq M]\). Let \(L\) be a polygonal line lying in \(D\) extending from \(z_0\) to any arbitrary \(z_n = P\) in \(D\), and \(d\) be the shortest distance from points on \(L\) to the boundary of \(D\). Then for each point \(z_k\) (\(k \in [0, n]\)), we have \(\abs{z_k - z_{k-1}} < d\) and neighbourhoods \(N_k\).
		
		Each neighbourhood \(N_k\) has radius \(d\) and the center of each neighbourhood \(N_k\) lies in the neighbourhood of \(N_{k-1}\).
		
		Since \(\max \abs{f(z)} = \abs{f(z_0)}\), by \cref{Analytic function is bounded in neighbourhood implies constant value thoughout neighbourhood Lemma - Complex}, all points in \(N_0\) has value \(f(z_0)\). The neighbourhoods overlap, so by extension \(\forall k \in [0, n]\), \(f(z_k) = f(z_0)\), and we have \(f(z)\) is constant in \(D\) with value \(f(z_0)\). \(f\) is then bounded in \(D\) and we have a contradiction! Thus, if an analytic function \(f\) is not constant in domain \(D\), then it is not bounded.
	\end{proof}

	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[remember picture]
			\begin{axis}[
				height=12cm,
				rotate around={-10:(current axis.origin)},
				clip=false,
				unit vector ratio=1 1 1,
				xlabel={$x$},
				ylabel={$y$},
				xmin=0, xmax=100,
				ymin=0, ymax=50,
				xtick=100,
				ytick=100,
				axis lines = none,
				axis line style = Black,
				]
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=*,
				]
				coordinates {
					(10, 10) [$z_0$]
					(15, 15) [$z_1$]
					(20, 20) [$z_2$]
					(80, 40) [$z_n = P$]
				};
				\draw[draw=Black]  (axis cs: 10,10)--(axis cs: 30,30)
				node[midway,sloped,above] {};
				\draw[draw=Black]  (axis cs: 30,30)--(axis cs: 40,20)
				node[midway,above] {$L$};
				\draw[-,draw=Black]  (axis cs: 40,20) -- (axis cs: 80,40);
				\draw[dashed, draw=Grey] (axis cs: 10, 10) circle (10);
				\draw[dashed, draw=Grey] (axis cs: 15, 15) circle (10);
				\draw[dashed, draw=Grey] (axis cs: 20, 20) circle (10);
				\draw[dashed, draw=Grey] (axis cs: 80, 40) circle (10);
				
				\addplot[
				only marks,
				nodes near coords,
				point meta = explicit symbolic, 
				color=Black,
				mark=none,
				]
				coordinates {
					(10, 0) [$N_0$]
					(15, 5) [$N_1$]
					(20, 10) [$N_2$]
					(80, 30) [$N_n$]
				};
			\end{axis}
		\end{tikzpicture}
	\end{figure}
	
	For a closed bounded region \(R\), the Maximum Modulus Principle may seem to contradict \cref{Function continuous in closed and bounded region implies function is bounded Theorem - Complex}. It is important to realize that we are working with a domain and the differences between a domain (\cref{Domain Definition - Complex}) and region (\cref{Region Definition - Complex}).
	
	\begin{corollary}
		\label{Maximum of moduli of analytic function in closed and bounded region is only reached at the boundaries of the region Corollary - Complex}
		Let \(f\) be an analytic function on a closed bounded region \(R\) that is not constant in the interior of \(R\). Then \(\max\abs{f(z)}\) in \(R\) is always reached and only reached at some boundary of \(R\), never in the interior of \(R\).
	\end{corollary}
	\begin{proof}
		Consider 
		\begin{align*}
			f(x,y) = u(x,y) + iv(x,y)
		\end{align*}
		Then as \(f\) as analytic in \(R\), \(u\) is harmonic in \(R\) and can not assume maximum value in the interior of \(R\). The same logic applies to \(v\). (See Maximum Principle.)
		
		More precisely, consider \(g(z) = e^{f(z)}\), then \(g\) is analytic, continuous, and non-constant in the interior of \(R\). Hence \(\abs{g(z)} = \abs{e^{u(x,y)}}\) must assume its maximum value at the boundary of \(R\), so \(f(z)\) must also obtain its maximum at the boundary of \(R\).
	\end{proof}

	Note: \Cref{Maximum of moduli of analytic function in closed and bounded region is only reached at the boundaries of the region Corollary - Complex} follows from the properties of \(f(z) = u(x,y) + iv(x,y)\) being able to be expressed in terms of real valued functions and that harmonic real valued functions only have maximum and minimum values occurring at the boundaries of a closed and bounded region. We will soon see this in \cref{Minimum of moduli of analytic function in closed and bounded region is only reached at the boundaries of the region Example - Complex} and \cref{Max and Min of Harmonic Real-Valued function only occurs at the boundary of closed bounded region Example - Complex}.
	
	Note: There is a difference between complex-valued functions and real-valued functions. For complex valued functions \(f(z)\), the maximums and minimums of the modulus \(\abs{f(z)}\) occur at some boundary of \(R\), while for some real-valued function \(u(x,y)\) (no modulus) the maximum and minimum occurs only at some boundary of \(R\).
	
	Note: This can be seen a result of Gauss's Mean Value Theorem (\cref{Gauss's Mean Value Theorem - Complex})

	\subsection{Examples} \label{Maximum Modulus Principle Examples Subsection - Complex}
	
	\begin{example}
		Suppose that \(f(z)\) is entire and that the function \(u(x,y) = \Re{f(z)}\) is harmonic and has upper bound \(u_0 \geq u(x,y)\) for all \((x,y) \in \mathbb{R}^2\). Then \(u(x,y)\) is constant in \(\mathbb{R}^2\).
		\begin{proof}
			{\color{Grey}
				Consider \(g(z) = e^{f(z)} = e^{u(x,y)}  e^{iv(x,y)}\). \(e^{iv(x,y)}\) is a phase, so we ignore it and focus on \(e^{u(x,y)}\):
				\begin{align*}
					&e^{u(x,y)} \text{ is entire} \\
					&\implies \exists(x_0, y_0) \in \mathbb{R}^2
					\left[\abs{e^{u(x,y)}} \leq u_0 = u(x_0, y_0) \right] 
					& \text{Cauchy's Inequalitey } (\Cref{Cauchy's Inequality Theorem - Complex}) \\
					&\implies  e^{u(x,y)} \text{ is constant}
					& \text{Liouville's Theorem } (\Cref{Liouville's Theorem - Complex})
				\end{align*}
			}
		\end{proof}
	\end{example}

	\begin{example}
		\label{Minimum of moduli of analytic function in closed and bounded region is only reached at the boundaries of the region Example - Complex}
		Let a function \(f\) be continuous on a closed bounded region \(R\), and be analytic and not constant throughout the interior of \(R\). Also, let \(f(z) \neq 0\) for all \(z \in \mathbb{R}\). Prove \(\min \abs{f(z)}\) only occurs at the boundaries and never in the interiors of \(R\).
		\begin{proof}
			{\color{Grey}
				Consider \(g(z) = 1/f(z)\).
				\begin{align*}
					&f \text{ is continuous, analytic, non-constant, and }\forall z /in R[f(z) \neq 0] \\
					&\implies g \text{ is continuous, analytic, non-constant}\\
					&\implies \max\abs{g(z)} \text{ only occurs at boundary of } R 
					& \Cref{Maximum of moduli of analytic function in closed and bounded region is only reached at the boundaries of the region Corollary - Complex} \\
					&\implies \min\abs{f(z)} \text{ occurs only at some boundary of } R
				\end{align*}
			}
			As for why \(f(z) \neq 0\) for all \(z \in R\) is required: \newline
			{\color{Grey}
				Suppose \(f(z_0) = 0\) for some \(z_0\) in the interior of \(R\).
				\begin{align*}
					&\implies g(z) = 1/f(z) \text{ is not countinuous in } R 
					& \Cref{Maximum of moduli of analytic function in closed and bounded region is only reached at the boundaries of the region Corollary - Complex} \\
					&\implies \max\abs{g(z)} \text{ does not occur only some boundary of } R\\
					&\implies \min \abs{f(z)} \text{ exists in the interior of } R
				\end{align*}
			}
		\end{proof}
	\end{example}
	\begin{question}
		If \(f(z_0) = 0\) for some \(z_0\) in the interior of R, then does that mean \(\min\abs{f(z_0)} = 0\) for all \(z_0 \in \{z : f(z) = 0\}\)?
	\end{question}

	\begin{example}
		\label{Max and Min of Harmonic Real-Valued function only occurs at the boundary of closed bounded region Example - Complex}
		Let \(f(z) = u(x,y) + iv(x,y)\) be a function continuous on a closed bounded region \(R\) be analytic and not constant in the interior of \(R\). Prove \(\min u(x,y)\) occurs only at some boundary of \(R\).
		\begin{proof}
			{\color{Grey}
			Let \(f(z) = u_1(x,y) + iv_1(x,y)\) and \(g(z) = e^{1/f(z)} = e^{u_2(x,y) + iv_2(x,y)}\), which is continuous, analytic, and not constant in the interior of \(R\). Then \(\abs{g(z)} = e^{u_2(x,y)}\) is continuous in \(R\) must have a maximum at the some boundary of \(R\). Hence, \(f(z)\) has a minimum at some boundary of \(R\).
			}
		\end{proof}
	\end{example}

	\chapter{Series} \label{Series Chapter - Complex}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\chapter{Conformal Mapping} \label{Conformal Mapping Chapter - Complex}
	
	
	
	
	\part{Ordinary Differential Equations} \label{Ordinary Differential Equations Part}
	
	\part{Nonlinear Dynamics} \label{Nonlinear Dynamics Part}
	
	
	\part{Partial Differential Equations} \label{Partial Differential Equations Part}
	
	\paragraph{Calculus of Variations} \label{Calculus of Variations Part}
	
	\part{Integral Equations} \label{Integral Equations Part}
	
	
	\part{Linear Algebra} \label{Linear Algebra Part}
	
	\chapter{Markov Chains} \label{Markov Chains Chapter - Linear Algebra}
	
	
	\part{Tensors} \label{Tensors Part}
	
	
	\part{Riemann Geometry} \label{Reimann Geometry Part}
	
	
	\part{Abstract Algebra} \label{Abstract Algebra Part}
	
	\chapter{Groups} \label{Groups Chapter - Abstract Algebra}
	
	
	\chapter{Rings} \label{Rings Chapter - Abstract Algebra}
	
	\section{Ideals} \label{Ideals Section - Abstract Algebra}
	
	\chapter{Integral Domains} \label{Integral Domains Chapter - Abstract Algebra}
	
	\chapter{GCD Domains} \label{GCD Domains Chapter - Abstract Algebra}
	
	\chapter{Unique Factorization Domains} \label{Unique Factorization Domains Chapter - Abstract Algebra}
	
	\chapter{Principal Ideal Domains} \label{Principal Ideal Domains Chapter - Abstract Algebra}
	
	\chapter{Fields} \label{Fields Chapter - Abstract Algebra}
	
	
	\part{Galois Theory} \label{Galois Theory Part}
	
	\part{Lie Theory} \label{Lie Algebra Part}
	
	\chapter{Lie Groups}
	
	\chapter{Lie Algebra}
	
	\part{C-Star Algebra} \label{C-Star Algebra Part}
	
	\part{Set Theory} \label{Set Theory Part}
	
	\part{Model Theory} \label{Model Theory Part}
	
	\part{Statistics} \label{Statistics Part}
	\part{Tips and Tricks} \label{Tips and Tricks Part}
	
	\chapter{Integration Techniques} \label{Integration Techniques Chapter - Tips and Tricks}
	
	\section{DI Method (Integration Table)} \label{DI Method Section - Tips and Tricks}
	
	\section{Feynman Integration} \label{Feynman Integration Section - Tips and Tricks}
	
	\backmatter
	\part{Index} \label{Index Part}
	
	\part{Bibliography}
	\bibliographystyle{unsrt}
	\typeout{}
	\bibliography{Bibliography}
	

\end{document}
